
# Core [![PyPi autogen-core](https://img.shields.io/badge/PyPi-autogen--core-blue?logo=pypi)](https://pypi.org/project/autogen-core/)

An event-driven programming framework for building scalable multi-agent AI systems. Example scenarios:

* Deterministic and dynamic agentic workflows for business processes.
* Research on multi-agent collaboration.
* Distributed agents for multi-language applications.

*Start here if you are building workflows or distributed agent systems.*

[Get Started](#document-user-guide/core-user-guide/quickstart)

## Core[#](#core "Link to this heading")

### Installation[#](#installation "Link to this heading")

#### Create a Virtual Environment (optional)[#](#create-a-virtual-environment-optional "Link to this heading")

When installing AgentChat locally, we recommend using a virtual environment for the installation. This will ensure that the dependencies for AgentChat are isolated from the rest of your system.

venv

Create and activate:

```
python3 -m venv .venv
source .venv/bin/activate

```

To deactivate later, run:

```
deactivate

```

conda

[Install Conda](https://docs.conda.io/projects/conda/en/stable/user-guide/install/index.html) if you have not already.

Create and activate:

```
conda create -n autogen python=3.12
conda activate autogen

```

To deactivate later, run:

```
conda deactivate

```

#### Install using pip[#](#install-using-pip "Link to this heading")

Install the `autogen-core` package using pip:

```
pip install "autogen-core"

```

Note

Python 3.10 or later is required.

#### Install OpenAI for Model Client[#](#install-openai-for-model-client "Link to this heading")

To use the OpenAI and Azure OpenAI models, you need to install the following
extensions:

```
pip install "autogen-ext[openai]"

```

If you are using Azure OpenAI with AAD authentication, you need to install the following:

```
pip install "autogen-ext[azure]"

```

#### Install Docker for Code Execution (Optional)[#](#install-docker-for-code-execution-optional "Link to this heading")

We recommend using Docker to use [`DockerCommandLineCodeExecutor`](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor "autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor") for execution of model-generated code.
To install Docker, follow the instructions for your operating system on the [Docker website](https://docs.docker.com/get-docker/).

To learn more code execution, see [Command Line Code Executors](#document-user-guide/core-user-guide/components/command-line-code-executors)
and [Code Execution](#document-user-guide/core-user-guide/design-patterns/code-execution-groupchat).

### Quick Start[#](#quick-start "Link to this heading")

Note

See [here](#document-user-guide/core-user-guide/installation) for installation instructions.

Before diving into the core APIs, let’s start with a simple example of two agents that count down from 10 to 1.

We first define the agent classes and their respective procedures for
handling messages.
We create two agent classes: `Modifier` and `Checker`. The `Modifier` agent modifies a number that is given and the `Check` agent checks the value against a condition.
We also create a `Message` data class, which defines the messages that are passed between the agents.

```
from dataclasses import dataclass
from typing import Callable

from autogen_core import DefaultTopicId, MessageContext, RoutedAgent, default_subscription, message_handler

@dataclass
class Message:
    content: int

@default_subscription
class Modifier(RoutedAgent):
    def __init__(self, modify_val: Callable[[int], int]) -> None:
        super().__init__("A modifier agent.")
        self._modify_val = modify_val

    @message_handler
    async def handle_message(self, message: Message, ctx: MessageContext) -> None:
        val = self._modify_val(message.content)
        print(f"{'-'*80}\nModifier:\nModified {message.content} to {val}")
        await self.publish_message(Message(content=val), DefaultTopicId())  # type: ignore

@default_subscription
class Checker(RoutedAgent):
    def __init__(self, run_until: Callable[[int], bool]) -> None:
        super().__init__("A checker agent.")
        self._run_until = run_until

    @message_handler
    async def handle_message(self, message: Message, ctx: MessageContext) -> None:
        if not self._run_until(message.content):
            print(f"{'-'*80}\nChecker:\n{message.content} passed the check, continue.")
            await self.publish_message(Message(content=message.content), DefaultTopicId())
        else:
            print(f"{'-'*80}\nChecker:\n{message.content} failed the check, stopping.")

```

You might have already noticed, the agents’ logic, whether it is using model or code executor,
is completely decoupled from
how messages are delivered. This is the core idea: the framework provides
a communication infrastructure, and the agents are responsible for their own
logic. We call the communication infrastructure an **Agent Runtime**.

Agent runtime is a key concept of this framework. Besides delivering messages,
it also manages agents’ lifecycle.
So the creation of agents are handled by the runtime.

The following code shows how to register and run the agents using
[`SingleThreadedAgentRuntime`](#autogen_core.SingleThreadedAgentRuntime "autogen_core.SingleThreadedAgentRuntime"),
a local embedded agent runtime implementation.

Note

If you are using VSCode or other Editor remember to import asyncio and wrap the code with async def main() -> None: and run the code with asyncio.run(main()) function.

```
from autogen_core import AgentId, SingleThreadedAgentRuntime

# Create an local embedded runtime.
runtime = SingleThreadedAgentRuntime()

# Register the modifier and checker agents by providing
# their agent types, the factory functions for creating instance and subscriptions.
await Modifier.register(
    runtime,
    "modifier",
    # Modify the value by subtracting 1
    lambda: Modifier(modify_val=lambda x: x - 1),
)

await Checker.register(
    runtime,
    "checker",
    # Run until the value is less than or equal to 1
    lambda: Checker(run_until=lambda x: x <= 1),
)

# Start the runtime and send a direct message to the checker.
runtime.start()
await runtime.send_message(Message(10), AgentId("checker", "default"))
await runtime.stop_when_idle()

```

```
--------------------------------------------------------------------------------
Checker:
10 passed the check, continue.
--------------------------------------------------------------------------------
Modifier:
Modified 10 to 9
--------------------------------------------------------------------------------
Checker:
9 passed the check, continue.
--------------------------------------------------------------------------------
Modifier:
Modified 9 to 8
--------------------------------------------------------------------------------
Checker:
8 passed the check, continue.
--------------------------------------------------------------------------------
Modifier:
Modified 8 to 7
--------------------------------------------------------------------------------
Checker:
7 passed the check, continue.
--------------------------------------------------------------------------------
Modifier:
Modified 7 to 6
--------------------------------------------------------------------------------
Checker:
6 passed the check, continue.
--------------------------------------------------------------------------------
Modifier:
Modified 6 to 5
--------------------------------------------------------------------------------
Checker:
5 passed the check, continue.
--------------------------------------------------------------------------------
Modifier:
Modified 5 to 4
--------------------------------------------------------------------------------
Checker:
4 passed the check, continue.
--------------------------------------------------------------------------------
Modifier:
Modified 4 to 3
--------------------------------------------------------------------------------
Checker:
3 passed the check, continue.
--------------------------------------------------------------------------------
Modifier:
Modified 3 to 2
--------------------------------------------------------------------------------
Checker:
2 passed the check, continue.
--------------------------------------------------------------------------------
Modifier:
Modified 2 to 1
--------------------------------------------------------------------------------
Checker:
1 failed the check, stopping.

```

From the agent’s output, we can see the value was successfully decremented from 10 to 1 as the modifier and checker conditions dictate.

AutoGen also supports a distributed agent runtime, which can host agents running on
different processes or machines, with different identities, languages and dependencies.

To learn how to use agent runtime, communication, message handling, and subscription, please continue
reading the sections following this quick start.

### Agent and Multi-Agent Applications[#](#agent-and-multi-agent-applications "Link to this heading")

An **agent** is a software entity that communicates via messages, maintains its own state, and performs actions in response to received messages or changes in its state. These actions may modify the agent’s state and produce external effects, such as updating message logs, sending new messages, executing code, or making API calls.

Many software systems can be modeled as a collection of independent agents that interact with one another. Examples include:

* Sensors on a factory floor
* Distributed services powering web applications
* Business workflows involving multiple stakeholders
* AI agents, such as those powered by language models (e.g., GPT-4), which can write code, interface with external systems, and communicate with other agents.

These systems, composed of multiple interacting agents, are referred to as **multi-agent applications**.

> **Note:**
> AI agents typically use language models as part of their software stack to interpret messages, perform reasoning, and execute actions.

#### Characteristics of Multi-Agent Applications[#](#characteristics-of-multi-agent-applications "Link to this heading")

In multi-agent applications, agents may:

* Run within the same process or on the same machine
* Operate across different machines or organizational boundaries
* Be implemented in diverse programming languages and make use of different AI models or instructions
* Work together towards a shared goal, coordinating their actions through messaging

Each agent is a self-contained unit that can be developed, tested, and deployed independently. This modular design allows agents to be reused across different scenarios and composed into more complex systems.

Agents are inherently **composable**: simple agents can be combined to form complex, adaptable applications, where each agent contributes a specific function or service to the overall system.

### Agent Runtime Environments[#](#agent-runtime-environments "Link to this heading")

At the foundation level, the framework provides a *runtime environment*, which facilitates
communication between agents, manages their identities and lifecycles,
and enforce security and privacy boundaries.

It supports two types of runtime environment: *standalone* and *distributed*.
Both types provide a common set of APIs for building multi-agent applications,
so you can switch between them without changing your agent implementation.
Each type can also have multiple implementations.

#### Standalone Agent Runtime[#](#standalone-agent-runtime "Link to this heading")

Standalone runtime is suitable for single-process applications where all agents
are implemented in the same programming language and running in the same process.
In the Python API, an example of standalone runtime is the [`SingleThreadedAgentRuntime`](#autogen_core.SingleThreadedAgentRuntime "autogen_core.SingleThreadedAgentRuntime").

The following diagram shows the standalone runtime in the framework.

![Standalone Runtime](_images/architecture-standalone.svg)

Here, agents communicate via messages through the runtime, and the runtime manages
the *lifecycle* of agents.

Developers can build agents quickly by using the provided components including
*routed agent*, AI model *clients*, tools for AI models, code execution sandboxes,
model context stores, and more.
They can also implement their own agents from scratch, or use other libraries.

#### Distributed Agent Runtime[#](#distributed-agent-runtime "Link to this heading")

Distributed runtime is suitable for multi-process applications where agents
may be implemented in different programming languages and running on different
machines.

![Distributed Runtime](_images/architecture-distributed.svg)

A distributed runtime, as shown in the diagram above,
consists of a *host servicer* and multiple *workers*.
The host servicer facilitates communication between agents across workers
and maintains the states of connections.
The workers run agents and communicate with the host servicer via *gateways*.
They advertise to the host servicer the agents they run and manage the agents’ lifecycles.

Agents work the same way as in the standalone runtime so that developers can
switch between the two runtime types with no change to their agent implementation.

### Application Stack[#](#application-stack "Link to this heading")

AutoGen core is designed to be an unopinionated framework that can be used to build
a wide variety of multi-agent applications. It is not tied to any specific
agent abstraction or multi-agent pattern.

The following diagram shows the application stack.

![Application Stack](_images/application-stack.svg)

At the bottom of the stack is the base messaging and routing facilities that
enable agents to communicate with each other. These are managed by the
agent runtime, and for most applications, developers only need to interact
with the high-level APIs provided by the runtime (see [Agent and Agent Runtime](#document-user-guide/core-user-guide/framework/agent-and-agent-runtime)).

At the top of the stack, developers need to define the
types of the messages that agents exchange. This set of message types
forms a behavior contract that agents must adhere to, and the
implementation of the contracts determines how agents handle messages.
The behavior contract is also sometimes referred to as the message protocol.
It is the developer’s responsibility to implement the behavior contract.
Multi-agent patterns emerge from these behavior contracts
(see [Multi-Agent Design Patterns](#document-user-guide/core-user-guide/design-patterns/intro)).

#### An Example Application[#](#an-example-application "Link to this heading")

Consider a concrete example of a multi-agent application for
code generation. The application consists of three agents:
Coder Agent, Executor Agent, and Reviewer Agent.
The following diagram shows the data flow between the agents,
and the message types exchanged between them.

![Code Generation Example](_images/code-gen-example.svg)

In this example, the behavior contract consists of the following:

* `CodingTaskMsg` message from application to the Coder Agent
* `CodeGenMsg` from Coder Agent to Executor Agent
* `ExecutionResultMsg` from Executor Agent to Reviewer Agent
* `ReviewMsg` from Reviewer Agent to Coder Agent
* `CodingResultMsg` from the Reviewer Agent to the application

The behavior contract is implemented by the agents’ handling of these messages. For example, the Reviewer Agent listens for `ExecutionResultMsg`
and evaluates the code execution result to decide whether to approve or reject,
if approved, it sends a `CodingResultMsg` to the application,
otherwise, it sends a `ReviewMsg` to the Coder Agent for another round of
code generation.

This behavior contract is a case of a multi-agent pattern called *reflection*,
where a generation result is reviewed by another round of generation,
to improve the overall quality.

### Agent Identity and Lifecycle[#](#agent-identity-and-lifecycle "Link to this heading")

The agent runtime manages agents’ identities
and lifecycles.
Application does not create agents directly, rather,
it registers an agent type with a factory function for
agent instances.
In this section, we explain how agents are identified
and created by the runtime.

#### Agent ID[#](#agent-id "Link to this heading")

Agent ID uniquely identifies an agent instance within
an agent runtime – including distributed runtime.
It is the “address” of the agent instance for receiving messages.
It has two components: agent type and agent key.

Note

Agent ID = (Agent Type, Agent Key)

The agent type is not an agent class.
It associates an agent with a specific
factory function, which produces instances of agents
of the same agent type.
For example, different factory functions can produce the same
agent class but with different constructor parameters.
The agent key is an instance identifier
for the given agent type.
Agent IDs can be converted to and from strings. the format of this string is:

Note

Agent\_Type/Agent\_Key

Types and Keys are considered valid if they only contain alphanumeric letters (a-z) and (0-9), or underscores (\_). A valid identifier cannot start with a number, or contain any spaces.

In a multi-agent application, agent types are
typically defined directly by the application, i.e., they
are defined in the application code.
On the other hand, agent keys are typically generated given
messages delivered to the agents, i.e., they are defined
by the application data.

For example, a runtime has registered the agent type `"code_reviewer"`
with a factory function producing agent instances that perform
code reviews. Each code review request has a unique ID `review_request_id`
to mark a dedicated
session.
In this case, each request can be handled by a new instance
with an agent ID, `("code_reviewer", review_request_id)`.

#### Agent Lifecycle[#](#agent-lifecycle "Link to this heading")

When a runtime delivers a message to an agent instance given its ID,
it either fetches the instance,
or creates it if it does not exist.

![Agent Lifecycle](_images/agent-lifecycle.svg)

The runtime is also responsible for “paging in” or “out” agent instances
to conserve resources and balance load across multiple machines.
This is not implemented yet.

### Topic and Subscription[#](#topic-and-subscription "Link to this heading")

There are two ways for runtime to deliver messages,
direct messaging or broadcast. Direct messaging is one to one: the sender
must provide the recipient’s agent ID. On the other hand,
broadcast is one to many and the sender does not provide recipients’
agent IDs.

Many scenarios are suitable for broadcast.
For example, in event-driven workflows, agents do not always know who
will handle their messages, and a workflow can be composed of agents
with no inter-dependencies.
This section focuses on the core concepts in broadcast: topic and subscription.

#### Topic[#](#topic "Link to this heading")

A topic defines the scope of a broadcast message.
In essence, agent runtime implements a publish-subscribe model through
its broadcast API: when publishing a message, the topic must be specified.
It is an indirection over agent IDs.

A topic consists of two components: topic type and topic source.

Note

Topic = (Topic Type, Topic Source)

Similar to [agent ID](#agent-id),
which also has two components, topic type is usually defined by
application code to mark the type of messages the topic is for.
For example, a GitHub agent may use `"GitHub_Issues"` as the topic type
when publishing messages about new issues.

Topic source is the unique identifier for a topic within a topic type.
It is typically defined by application data.
For example, the GitHub agent may use `"github.com/{repo_name}/issues/{issue_number}"`
as the topic source to uniquely identifies the topic.
Topic source allows the publisher to limit the scope of messages and create
silos.

Topic IDs can be converted to and from strings. the format of this string is:

Note

Topic\_Type/Topic\_Source

Types are considered valid if they are in UTF8 and only contain alphanumeric letters (a-z) and (0-9), or underscores (\_). A valid identifier cannot start with a number, or contain any spaces.
Sources are considered valid if they are in UTF8 and only contain characters between (inclusive) ascii 32 (space) and 126 (~).

#### Subscription[#](#subscription "Link to this heading")

A subscription maps topic to agent IDs.

![Subscription](_images/subscription.svg)

The diagram above shows the relationship between topic and subscription.
An agent runtime keeps track of the subscriptions and uses them to deliver
messages to agents.

If a topic has no subscription, messages published to this topic will
not be delivered to any agent.
If a topic has many subscriptions, messages will be delivered
following all the subscriptions to every recipient agent only once.
Applications can add or remove subscriptions using agent runtime’s API.

#### Type-based Subscription[#](#type-based-subscription "Link to this heading")

A type-based subscription maps a topic type to an agent type
(see [agent ID](#agent-id)).
It declares an unbounded mapping from topics to agent IDs without knowing the
exact topic sources and agent keys.
The mechanism is simple: any topic matching the type-based subscription’s
topic type will be mapped to an agent ID with the subscription’s agent type
and the agent key assigned to the value of the topic source.
For Python API, use `TypeSubscription`.

Note

Type-Based Subscription = Topic Type –> Agent Type

Generally speaking, type-based subscription is the preferred way to declare
subscriptions. It is portable and data-independent:
developers do not need to write application code that depends on specific agent IDs.

##### Scenarios of Type-Based Subscription[#](#scenarios-of-type-based-subscription "Link to this heading")

Type-based subscriptions can be applied to many scenarios when the exact
topic or agent IDs are data-dependent.
The scenarios can be broken down by two considerations:
(1) whether it is single-tenant or multi-tenant, and
(2) whether it is a single topic or multiple topics per tenant.
A tenant typically refers to a set of agents that handle a specific
user session or a specific request.

###### Single-Tenant, Single Topic[#](#single-tenant-single-topic "Link to this heading")

In this scenario, there is only one tenant and one topic for the entire
application.
It is the simplest scenario and can be used in many cases
like a command line tool or a single-user application.

To apply type-based subscription for this scenario, create one type-based
subscription for each agent type, and use the same topic type for all
the type-based subscriptions.
When you publish, always use the same topic, i.e., the same topic type and topic source.

For example, assuming there are three agent types: `"triage_agent"`,
`"coder_agent"` and `"reviewer_agent"`, and the topic type is `"default"`,
create the following type-based subscriptions:

```
# Type-based Subscriptions for single-tenant, single topic scenario
TypeSubscription(topic_type="default", agent_type="triage_agent")
TypeSubscription(topic_type="default", agent_type="coder_agent")
TypeSubscription(topic_type="default", agent_type="reviewer_agent")

```

With the above type-based subscriptions, use the same topic source
`"default"` for all messages. So the topic is always `("default", "default")`.
A message published to this topic will be delivered to all the agents of
all above types. Specifically, the message will be sent to the following agent IDs:

```
# The agent IDs created based on the topic source
AgentID("triage_agent", "default")
AgentID("coder_agent", "default")
AgentID("reviewer_agent", "default")

```

The following figure shows how type-based subscription works in this example.

![Type-Based Subscription Single-Tenant, Single Topic Scenario Example](_images/type-subscription-single-tenant-single-topic.svg)

If the agent with the ID does not exist, the runtime will create it.

###### Single-Tenant, Multiple Topics[#](#single-tenant-multiple-topics "Link to this heading")

In this scenario, there is only one tenant but you want to control
which agent handles which topic. This is useful when you want to
create silos and have different agents specialized in handling different topics.

To apply type-based subscription for this scenario,
create one type-based subscription for each agent type but with different
topic types. You can map the same topic type to multiple agent types if
you want these agent types to share a same topic.
For topic source, still use the same value for all messages when you publish.

Continuing the example above with same agent types, create the following
type-based subscriptions:

```
# Type-based Subscriptions for single-tenant, multiple topics scenario
TypeSubscription(topic_type="triage", agent_type="triage_agent")
TypeSubscription(topic_type="coding", agent_type="coder_agent")
TypeSubscription(topic_type="coding", agent_type="reviewer_agent")

```

With the above type-based subscriptions, any message published to the topic
`("triage", "default")` will be delivered to the agent with type
`"triage_agent"`, and any message published to the topic
`("coding", "default")` will be delivered to the agents with types
`"coder_agent"` and `"reviewer_agent"`.

The following figure shows how type-based subscription works in this example.

![Type-Based Subscription Single-Tenant, Multiple Topics Scenario Example](_images/type-subscription-single-tenant-multiple-topics.svg)

###### Multi-Tenant Scenarios[#](#multi-tenant-scenarios "Link to this heading")

In single-tenant scenarios, the topic source is always the same (e.g., `"default"`)
– it is hard-coded in the application code.
When moving to multi-tenant scenarios, the topic source becomes data-dependent.

Note

A good indication that you are in a multi-tenant scenario is that you need
multiple instances of the same agent type. For example, you may want to have
different agent instances to handle different user sessions to
keep private data isolated, or, you may want to distribute a heavy workload
across multiple instances of the same agent type and have them work on it concurrently.

Continuing the example above, if you want to have dedicated instances of agents
to handle a specific GitHub issue, you need to set the topic source to be a
unique identifier for the issue.

For example, let’s say there is one type-based subscription for the agent type
`"triage_agent"`:

```
TypeSubscription(topic_type="github_issues", agent_type="triage_agent")

```

When a message is published to the topic
`("github_issues", "github.com/microsoft/autogen/issues/1")`,
the runtime will deliver the message to the agent with ID
`("triage_agent", "github.com/microsoft/autogen/issues/1")`.
When a message is published to the topic
`("github_issues", "github.com/microsoft/autogen/issues/9")`,
the runtime will deliver the message to the agent with ID
`("triage_agent", "github.com/microsoft/autogen/issues/9")`.

The following figure shows how type-based subscription works in this example.

![Type-Based Subscription Multi-Tenant Scenario Example](_images/type-subscription-multi-tenant.svg)

Note the agent ID is data-dependent, and the runtime will create a new instance
of the agent
if it does not exist.

To support multiple topics per tenant, you can use different topic types,
just like the single-tenant, multiple topics scenario.

### Agent and Agent Runtime[#](#agent-and-agent-runtime "Link to this heading")

In this and the following section, we focus on the core concepts of AutoGen:
agents, agent runtime, messages, and communication –
the foundational building blocks for an multi-agent applications.

Note

The Core API is designed to be unopinionated and flexible. So at times, you
may find it challenging. Continue if you are building
an interactive, scalable and distributed multi-agent system and want full control
of all workflows.
If you just want to get something running
quickly, you may take a look at the [AgentChat API](#document-user-guide/agentchat-user-guide/index).

An agent in AutoGen is an entity defined by the base interface [`Agent`](#autogen_core.Agent "autogen_core.Agent").
It has a unique identifier of the type [`AgentId`](#autogen_core.AgentId "autogen_core.AgentId"),
a metadata dictionary of the type [`AgentMetadata`](#autogen_core.AgentMetadata "autogen_core.AgentMetadata").

In most cases, you can subclass your agents from higher level class [`RoutedAgent`](#autogen_core.RoutedAgent "autogen_core.RoutedAgent") which enables you to route messages to corresponding message handler specified with [`message_handler()`](#autogen_core.message_handler "autogen_core.message_handler") decorator and proper type hint for the `message` variable.
An agent runtime is the execution environment for agents in AutoGen.

Similar to the runtime environment of a programming language,
an agent runtime provides the necessary infrastructure to facilitate communication
between agents, manage agent lifecycles, enforce security boundaries, and support monitoring and
debugging.

For local development, developers can use [`SingleThreadedAgentRuntime`](#autogen_core.SingleThreadedAgentRuntime "autogen_core.SingleThreadedAgentRuntime"),
which can be embedded in a Python application.

Note

Agents are not directly instantiated and managed by application code.
Instead, they are created by the runtime when needed and managed by the runtime.

If you are already familiar with [AgentChat](#document-user-guide/agentchat-user-guide/index),
it is important to note that AgentChat’s agents such as
[`AssistantAgent`](#autogen_agentchat.agents.AssistantAgent "autogen_agentchat.agents.AssistantAgent") are created by application
and thus not directly managed by the runtime. To use an AgentChat agent in Core,
you need to create a wrapper Core agent that delegates messages to the AgentChat agent
and let the runtime manage the wrapper agent.

#### Implementing an Agent[#](#implementing-an-agent "Link to this heading")

To implement an agent, the developer must subclass the [`RoutedAgent`](#autogen_core.RoutedAgent "autogen_core.RoutedAgent") class
and implement a message handler method for each message type the agent is expected to handle using
the [`message_handler()`](#autogen_core.message_handler "autogen_core.message_handler") decorator.
For example,
the following agent handles a simple message type `MyMessageType` and prints the message it receives:

```
from dataclasses import dataclass

from autogen_core import AgentId, MessageContext, RoutedAgent, message_handler

@dataclass
class MyMessageType:
    content: str

class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("MyAgent")

    @message_handler
    async def handle_my_message_type(self, message: MyMessageType, ctx: MessageContext) -> None:
        print(f"{self.id.type} received message: {message.content}")

```

This agent only handles `MyMessageType` and messages will be delivered to `handle_my_message_type` method. Developers can have multiple message handlers for different message types by using [`message_handler()`](#autogen_core.message_handler "autogen_core.message_handler") decorator and setting the type hint for the `message` variable in the handler function. You can also leverage [python typing union](https://docs.python.org/3/library/typing.html#typing.Union) for the `message` variable in one message handler function if it better suits agent’s logic.
See the next section on [message and communication](#document-user-guide/core-user-guide/framework/message-and-communication).

#### Using an AgentChat Agent[#](#using-an-agentchat-agent "Link to this heading")

If you have an [AgentChat](#document-user-guide/agentchat-user-guide/index) agent and want to use it in the Core API, you can create
a wrapper [`RoutedAgent`](#autogen_core.RoutedAgent "autogen_core.RoutedAgent") that delegates messages to the AgentChat agent.
The following example shows how to create a wrapper agent for the [`AssistantAgent`](#autogen_agentchat.agents.AssistantAgent "autogen_agentchat.agents.AssistantAgent")
in AgentChat.

```
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient

class MyAssistant(RoutedAgent):
    def __init__(self, name: str) -> None:
        super().__init__(name)
        model_client = OpenAIChatCompletionClient(model="gpt-4o")
        self._delegate = AssistantAgent(name, model_client=model_client)

    @message_handler
    async def handle_my_message_type(self, message: MyMessageType, ctx: MessageContext) -> None:
        print(f"{self.id.type} received message: {message.content}")
        response = await self._delegate.on_messages(
            [TextMessage(content=message.content, source="user")], ctx.cancellation_token
        )
        print(f"{self.id.type} responded: {response.chat_message.content}")

```

For how to use model client, see the [Model Client](#document-user-guide/core-user-guide/components/model-clients) section.

Since the Core API is unopinionated,
you are not required to use the AgentChat API to use the Core API.
You can implement your own agents or use another agent framework.

#### Registering Agent Type[#](#registering-agent-type "Link to this heading")

To make agents available to the runtime, developers can use the
[`register()`](#autogen_core.BaseAgent.register "autogen_core.BaseAgent.register") class method of the
[`BaseAgent`](#autogen_core.BaseAgent "autogen_core.BaseAgent") class.
The process of registration associates an agent type, which is uniquely identified by a string,
and a factory function
that creates an instance of the agent type of the given class.
The factory function is used to allow automatic creation of agent instances
when they are needed.

Agent type ([`AgentType`](#autogen_core.AgentType "autogen_core.AgentType")) is not the same as the agent class. In this example,
the agent type is `AgentType("my_agent")` or `AgentType("my_assistant")` and the agent class is the Python class `MyAgent` or `MyAssistantAgent`.
The factory function is expected to return an instance of the agent class
on which the [`register()`](#autogen_core.BaseAgent.register "autogen_core.BaseAgent.register") class method is invoked.
Read [Agent Identity and Lifecycles](#document-user-guide/core-user-guide/core-concepts/agent-identity-and-lifecycle)
to learn more about agent type and identity.

Note

Different agent types can be registered with factory functions that return
the same agent class. For example, in the factory functions,
variations of the constructor parameters
can be used to create different instances of the same agent class.

To register our agent types with the
[`SingleThreadedAgentRuntime`](#autogen_core.SingleThreadedAgentRuntime "autogen_core.SingleThreadedAgentRuntime"),
the following code can be used:

```
from autogen_core import SingleThreadedAgentRuntime

runtime = SingleThreadedAgentRuntime()
await MyAgent.register(runtime, "my_agent", lambda: MyAgent())
await MyAssistant.register(runtime, "my_assistant", lambda: MyAssistant("my_assistant"))

```

```
AgentType(type='my_assistant')

```

Once an agent type is registered, we can send a direct message to an agent instance
using an [`AgentId`](#autogen_core.AgentId "autogen_core.AgentId").
The runtime will create the instance the first time it delivers a
message to this instance.

```
runtime.start()  # Start processing messages in the background.
await runtime.send_message(MyMessageType("Hello, World!"), AgentId("my_agent", "default"))
await runtime.send_message(MyMessageType("Hello, World!"), AgentId("my_assistant", "default"))
await runtime.stop()  # Stop processing messages in the background.

```

```
my_agent received message: Hello, World!
my_assistant received message: Hello, World!
my_assistant responded: Hello! How can I assist you today?

```

Note

Because the runtime manages the lifecycle of agents, an [`AgentId`](#autogen_core.AgentId "autogen_core.AgentId")
is only used to communicate with the agent or retrieve its metadata (e.g., description).

#### Running the Single-Threaded Agent Runtime[#](#running-the-single-threaded-agent-runtime "Link to this heading")

The above code snippet uses [`start()`](#autogen_core.SingleThreadedAgentRuntime.start "autogen_core.SingleThreadedAgentRuntime.start") to start a background task
to process and deliver messages to recepients’ message handlers.
This is a feature of the
local embedded runtime [`SingleThreadedAgentRuntime`](#autogen_core.SingleThreadedAgentRuntime "autogen_core.SingleThreadedAgentRuntime").

To stop the background task immediately, use the [`stop()`](#autogen_core.SingleThreadedAgentRuntime.stop "autogen_core.SingleThreadedAgentRuntime.stop") method:

```
runtime.start()
# ... Send messages, publish messages, etc.
await runtime.stop()  # This will return immediately but will not cancel
# any in-progress message handling.

```

You can resume the background task by calling [`start()`](#autogen_core.SingleThreadedAgentRuntime.start "autogen_core.SingleThreadedAgentRuntime.start") again.

For batch scenarios such as running benchmarks for evaluating agents,
you may want to wait for the background task to stop automatically when
there are no unprocessed messages and no agent is handling messages –
the batch may considered complete.
You can achieve this by using the [`stop_when_idle()`](#autogen_core.SingleThreadedAgentRuntime.stop_when_idle "autogen_core.SingleThreadedAgentRuntime.stop_when_idle") method:

```
runtime.start()
# ... Send messages, publish messages, etc.
await runtime.stop_when_idle()  # This will block until the runtime is idle.

```

To close the runtime and release resources, use the [`close()`](#autogen_core.SingleThreadedAgentRuntime.close "autogen_core.SingleThreadedAgentRuntime.close") method:

```
await runtime.close()

```

Other runtime implementations will have their own ways of running the runtime.

### Message and Communication[#](#message-and-communication "Link to this heading")

An agent in AutoGen core can react to, send, and publish messages,
and messages are the only means through which agents can communicate
with each other.

#### Messages[#](#messages "Link to this heading")

Messages are serializable objects, they can be defined using:

* A subclass of Pydantic’s `pydantic.BaseModel`, or
* A dataclass

For example:

```
from dataclasses import dataclass

@dataclass
class TextMessage:
    content: str
    source: str

@dataclass
class ImageMessage:
    url: str
    source: str

```

Note

Messages are purely data, and should not contain any logic.

#### Message Handlers[#](#message-handlers "Link to this heading")

When an agent receives a message the runtime will invoke the agent’s message handler
([`on_message()`](#autogen_core.Agent.on_message "autogen_core.Agent.on_message")) which should implement the agents message handling logic.
If this message cannot be handled by the agent, the agent should raise a
[`CantHandleException`](#autogen_core.exceptions.CantHandleException "autogen_core.exceptions.CantHandleException").

The base class [`BaseAgent`](#autogen_core.BaseAgent "autogen_core.BaseAgent") provides no message handling logic
and implementing the [`on_message()`](#autogen_core.Agent.on_message "autogen_core.Agent.on_message") method directly is not recommended
unless for the advanced use cases.

Developers should start with implementing the [`RoutedAgent`](#autogen_core.RoutedAgent "autogen_core.RoutedAgent") base class
which provides built-in message routing capability.

##### Routing Messages by Type[#](#routing-messages-by-type "Link to this heading")

The [`RoutedAgent`](#autogen_core.RoutedAgent "autogen_core.RoutedAgent") base class provides a mechanism
for associating message types with message handlers
with the `message_handler()` decorator,
so developers do not need to implement the [`on_message()`](#autogen_core.Agent.on_message "autogen_core.Agent.on_message") method.

For example, the following type-routed agent responds to `TextMessage` and `ImageMessage`
using different message handlers:

```
from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler

class MyAgent(RoutedAgent):
    @message_handler
    async def on_text_message(self, message: TextMessage, ctx: MessageContext) -> None:
        print(f"Hello, {message.source}, you said {message.content}!")

    @message_handler
    async def on_image_message(self, message: ImageMessage, ctx: MessageContext) -> None:
        print(f"Hello, {message.source}, you sent me {message.url}!")

```

Create the agent runtime and register the agent type (see [Agent and Agent Runtime](#document-user-guide/core-user-guide/framework/agent-and-agent-runtime)):

```
runtime = SingleThreadedAgentRuntime()
await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My Agent"))

```

```
AgentType(type='my_agent')

```

Test this agent with `TextMessage` and `ImageMessage`.

```
runtime.start()
agent_id = AgentId("my_agent", "default")
await runtime.send_message(TextMessage(content="Hello, World!", source="User"), agent_id)
await runtime.send_message(ImageMessage(url="https://example.com/image.jpg", source="User"), agent_id)
await runtime.stop_when_idle()

```

```
Hello, User, you said Hello, World!!
Hello, User, you sent me https://example.com/image.jpg!

```

The runtime automatically creates an instance of `MyAgent` with the
agent ID `AgentId("my_agent", "default")` when delivering the first message.

##### Routing Messages of the Same Type[#](#routing-messages-of-the-same-type "Link to this heading")

In some scenarios, it is useful to route messages of the same type to different handlers.
For examples, messages from different sender agents should be handled differently.
You can use the `match` parameter of the `message_handler()` decorator.

The `match` parameter associates handlers for the same message type
to a specific message – it is secondary to the message type routing.
It accepts a callable that takes the message and
[`MessageContext`](#autogen_core.MessageContext "autogen_core.MessageContext") as arguments, and
returns a boolean indicating whether the message should be handled by the decorated handler.
The callable is checked in the alphabetical order of the handlers.

Here is an example of an agent that routes messages based on the sender agent
using the `match` parameter:

```
class RoutedBySenderAgent(RoutedAgent):
    @message_handler(match=lambda msg, ctx: msg.source.startswith("user1"))  # type: ignore
    async def on_user1_message(self, message: TextMessage, ctx: MessageContext) -> None:
        print(f"Hello from user 1 handler, {message.source}, you said {message.content}!")

    @message_handler(match=lambda msg, ctx: msg.source.startswith("user2"))  # type: ignore
    async def on_user2_message(self, message: TextMessage, ctx: MessageContext) -> None:
        print(f"Hello from user 2 handler, {message.source}, you said {message.content}!")

    @message_handler(match=lambda msg, ctx: msg.source.startswith("user2"))  # type: ignore
    async def on_image_message(self, message: ImageMessage, ctx: MessageContext) -> None:
        print(f"Hello, {message.source}, you sent me {message.url}!")

```

The above agent uses the `source` field of the message to determine the sender agent.
You can also use the `sender` field of [`MessageContext`](#autogen_core.MessageContext "autogen_core.MessageContext") to determine the sender agent
using the agent ID if available.

Let’s test this agent with messages with different `source` values:

```
runtime = SingleThreadedAgentRuntime()
await RoutedBySenderAgent.register(runtime, "my_agent", lambda: RoutedBySenderAgent("Routed by sender agent"))
runtime.start()
agent_id = AgentId("my_agent", "default")
await runtime.send_message(TextMessage(content="Hello, World!", source="user1-test"), agent_id)
await runtime.send_message(TextMessage(content="Hello, World!", source="user2-test"), agent_id)
await runtime.send_message(ImageMessage(url="https://example.com/image.jpg", source="user1-test"), agent_id)
await runtime.send_message(ImageMessage(url="https://example.com/image.jpg", source="user2-test"), agent_id)
await runtime.stop_when_idle()

```

```
Hello from user 1 handler, user1-test, you said Hello, World!!
Hello from user 2 handler, user2-test, you said Hello, World!!
Hello, user2-test, you sent me https://example.com/image.jpg!

```

In the above example, the first `ImageMessage` is not handled because the `source` field
of the message does not match the handler’s `match` condition.

#### Direct Messaging[#](#direct-messaging "Link to this heading")

There are two types of communication in AutoGen core:

* **Direct Messaging**: sends a direct message to another agent.
* **Broadcast**: publishes a message to a topic.

Let’s first look at direct messaging.
To send a direct message to another agent, within a message handler use
the [`autogen_core.BaseAgent.send_message()`](#autogen_core.BaseAgent.send_message "autogen_core.BaseAgent.send_message") method,
from the runtime use the [`autogen_core.AgentRuntime.send_message()`](#autogen_core.AgentRuntime.send_message "autogen_core.AgentRuntime.send_message") method.
Awaiting calls to these methods will return the return value of the
receiving agent’s message handler.
When the receiving agent’s handler returns `None`, `None` will be returned.

Note

If the invoked agent raises an exception while the sender is awaiting,
the exception will be propagated back to the sender.

##### Request/Response[#](#request-response "Link to this heading")

Direct messaging can be used for request/response scenarios,
where the sender expects a response from the receiver.
The receiver can respond to the message by returning a value from its message handler.
You can think of this as a function call between agents.

For example, consider the following agents:

```
from dataclasses import dataclass

from autogen_core import MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler

@dataclass
class Message:
    content: str

class InnerAgent(RoutedAgent):
    @message_handler
    async def on_my_message(self, message: Message, ctx: MessageContext) -> Message:
        return Message(content=f"Hello from inner, {message.content}")

class OuterAgent(RoutedAgent):
    def __init__(self, description: str, inner_agent_type: str):
        super().__init__(description)
        self.inner_agent_id = AgentId(inner_agent_type, self.id.key)

    @message_handler
    async def on_my_message(self, message: Message, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")
        # Send a direct message to the inner agent and receives a response.
        response = await self.send_message(Message(f"Hello from outer, {message.content}"), self.inner_agent_id)
        print(f"Received inner response: {response.content}")

```

Upone receving a message, the `OuterAgent` sends a direct message to the `InnerAgent` and receives
a message in response.

We can test these agents by sending a `Message` to the `OuterAgent`.

```
runtime = SingleThreadedAgentRuntime()
await InnerAgent.register(runtime, "inner_agent", lambda: InnerAgent("InnerAgent"))
await OuterAgent.register(runtime, "outer_agent", lambda: OuterAgent("OuterAgent", "inner_agent"))
runtime.start()
outer_agent_id = AgentId("outer_agent", "default")
await runtime.send_message(Message(content="Hello, World!"), outer_agent_id)
await runtime.stop_when_idle()

```

```
Received message: Hello, World!
Received inner response: Hello from inner, Hello from outer, Hello, World!

```

Both outputs are produced by the `OuterAgent`’s message handler, however the second output is based on the response from the `InnerAgent`.

Generally speaking, direct messaging is appropriate for scenarios when the sender and
recipient are tightly coupled – they are created together and the sender
is linked to a specific instance of the recipient.
For example, an agent executes tool calls by sending direct messages to
an instance of [`ToolAgent`](#autogen_core.tool_agent.ToolAgent "autogen_core.tool_agent.ToolAgent"),
and uses the responses to form an action-observation loop.

#### Broadcast[#](#broadcast "Link to this heading")

Broadcast is effectively the publish/subscribe model with topic and subscription.
Read [Topic and Subscription](#document-user-guide/core-user-guide/core-concepts/topic-and-subscription)
to learn the core concepts.

The key difference between direct messaging and broadcast is that broadcast
cannot be used for request/response scenarios.
When an agent publishes a message it is one way only, it cannot receive a response
from any other agent, even if a receiving agent’s handler returns a value.

Note

If a response is given to a published message, it will be thrown away.

Note

If an agent publishes a message type for which it is subscribed it will not
receive the message it published. This is to prevent infinite loops.

##### Subscribe and Publish to Topics[#](#subscribe-and-publish-to-topics "Link to this heading")

[Type-based subscription](#type-based-subscription)
maps messages published to topics of a given topic type to
agents of a given agent type.
To make an agent that subsclasses [`RoutedAgent`](#autogen_core.RoutedAgent "autogen_core.RoutedAgent")
subscribe to a topic of a given topic type,
you can use the `type_subscription()` class decorator.

The following example shows a `ReceiverAgent` class that subscribes to topics of `"default"` topic type
using the `type_subscription()` decorator.
and prints the received messages.

```
from autogen_core import RoutedAgent, message_handler, type_subscription

@type_subscription(topic_type="default")
class ReceivingAgent(RoutedAgent):
    @message_handler
    async def on_my_message(self, message: Message, ctx: MessageContext) -> None:
        print(f"Received a message: {message.content}")

```

To publish a message from an agent’s handler,
use the [`publish_message()`](#autogen_core.BaseAgent.publish_message "autogen_core.BaseAgent.publish_message") method and specify
a [`TopicId`](#autogen_core.TopicId "autogen_core.TopicId").
This call must still be awaited to allow the runtime to schedule delivery of
the message to all subscribers, but it will always return `None`.
If an agent raises an exception while handling a published message,
this will be logged but will not be propagated back to the publishing agent.

The following example shows a `BroadcastingAgent` that
publishes a message to a topic upon receiving a message.

```
from autogen_core import TopicId

class BroadcastingAgent(RoutedAgent):
    @message_handler
    async def on_my_message(self, message: Message, ctx: MessageContext) -> None:
        await self.publish_message(
            Message("Publishing a message from broadcasting agent!"),
            topic_id=TopicId(type="default", source=self.id.key),
        )

```

`BroadcastingAgent` publishes message to a topic with type `"default"`
and source assigned to the agent instance’s agent key.

Subscriptions are registered with the agent runtime, either as part of
agent type’s registration or through a separate API method.
Here is how we register `TypeSubscription`
for the receiving agent with the `type_subscription()` decorator,
and for the broadcasting agent without the decorator.

```
from autogen_core import TypeSubscription

runtime = SingleThreadedAgentRuntime()

# Option 1: with type_subscription decorator
# The type_subscription class decorator automatically adds a TypeSubscription to
# the runtime when the agent is registered.
await ReceivingAgent.register(runtime, "receiving_agent", lambda: ReceivingAgent("Receiving Agent"))

# Option 2: with TypeSubscription
await BroadcastingAgent.register(runtime, "broadcasting_agent", lambda: BroadcastingAgent("Broadcasting Agent"))
await runtime.add_subscription(TypeSubscription(topic_type="default", agent_type="broadcasting_agent"))

# Start the runtime and publish a message.
runtime.start()
await runtime.publish_message(
    Message("Hello, World! From the runtime!"), topic_id=TopicId(type="default", source="default")
)
await runtime.stop_when_idle()

```

```
Received a message: Hello, World! From the runtime!
Received a message: Publishing a message from broadcasting agent!

```

As shown in the above example, you can also publish directly to a topic
through the runtime’s [`publish_message()`](#autogen_core.AgentRuntime.publish_message "autogen_core.AgentRuntime.publish_message") method
without the need to create an agent instance.

From the output, you can see two messages were received by the receiving agent:
one was published through the runtime, and the other was published by the broadcasting agent.

##### Default Topic and Subscriptions[#](#default-topic-and-subscriptions "Link to this heading")

In the above example, we used
[`TopicId`](#autogen_core.TopicId "autogen_core.TopicId") and `TypeSubscription`
to specify the topic and subscriptions respectively.
This is the appropriate way for many scenarios.
However, when there is a single scope of publishing, that is,
all agents publish and subscribe to all broadcasted messages,
we can use the convenience classes `DefaultTopicId`
and `default_subscription()` to simplify our code.

`DefaultTopicId` is
for creating a topic that uses `"default"` as the default value for the topic type
and the publishing agent’s key as the default value for the topic source.
`default_subscription()` is
for creating a type subscription that subscribes to the default topic.
We can simplify `BroadcastingAgent` by using
`DefaultTopicId` and `default_subscription()`.

```
from autogen_core import DefaultTopicId, default_subscription

@default_subscription
class BroadcastingAgentDefaultTopic(RoutedAgent):
    @message_handler
    async def on_my_message(self, message: Message, ctx: MessageContext) -> None:
        # Publish a message to all agents in the same namespace.
        await self.publish_message(
            Message("Publishing a message from broadcasting agent!"),
            topic_id=DefaultTopicId(),
        )

```

When the runtime calls [`register()`](#autogen_core.BaseAgent.register "autogen_core.BaseAgent.register") to register the agent type,
it creates a `TypeSubscription`
whose topic type uses `"default"` as the default value and
agent type uses the same agent type that is being registered in the same context.

```
runtime = SingleThreadedAgentRuntime()
await BroadcastingAgentDefaultTopic.register(
    runtime, "broadcasting_agent", lambda: BroadcastingAgentDefaultTopic("Broadcasting Agent")
)
await ReceivingAgent.register(runtime, "receiving_agent", lambda: ReceivingAgent("Receiving Agent"))
runtime.start()
await runtime.publish_message(Message("Hello, World! From the runtime!"), topic_id=DefaultTopicId())
await runtime.stop_when_idle()

```

```
Received a message: Hello, World! From the runtime!
Received a message: Publishing a message from broadcasting agent!

```

Note

If your scenario allows all agents to publish and subscribe to
all broadcasted messages, use `DefaultTopicId`
and `default_subscription()` to decorate your
agent classes.

### Logging[#](#logging "Link to this heading")

AutoGen uses Python’s built-in [`logging`](https://docs.python.org/3/library/logging.html) module.

There are two kinds of logging:

* **Trace logging**: This is used for debugging and is human readable messages to indicate what is going on. This is intended for a developer to understand what is happening in the code. The content and format of these logs should not be depended on by other systems.

  + Name: [`TRACE_LOGGER_NAME`](#autogen_core.TRACE_LOGGER_NAME "autogen_core.TRACE_LOGGER_NAME").
* **Structured logging**: This logger emits structured events that can be consumed by other systems. The content and format of these logs can be depended on by other systems.

  + Name: [`EVENT_LOGGER_NAME`](#autogen_core.EVENT_LOGGER_NAME "autogen_core.EVENT_LOGGER_NAME").
  + See the module [`autogen_core.logging`](#module-autogen_core.logging "autogen_core.logging") to see the available events.
* [`ROOT_LOGGER_NAME`](#autogen_core.ROOT_LOGGER_NAME "autogen_core.ROOT_LOGGER_NAME") can be used to enable or disable all logs.

#### Enabling logging output[#](#enabling-logging-output "Link to this heading")

To enable trace logging, you can use the following code:

```
import logging

from autogen_core import TRACE_LOGGER_NAME

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(TRACE_LOGGER_NAME)
logger.addHandler(logging.StreamHandler())
logger.setLevel(logging.DEBUG)

```

To enable structured logging, you can use the following code:

```
import logging

from autogen_core import EVENT_LOGGER_NAME

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(EVENT_LOGGER_NAME)
logger.addHandler(logging.StreamHandler())
logger.setLevel(logging.INFO)

```

##### Structured logging[#](#structured-logging "Link to this heading")

Structured logging allows you to write handling logic that deals with the actual events including all fields rather than just a formatted string.

For example, if you had defined this custom event and were emitting it. Then you could write the following handler to receive it.

```
import logging
from dataclasses import dataclass

@dataclass
class MyEvent:
    timestamp: str
    message: str

class MyHandler(logging.Handler):
    def __init__(self) -> None:
        super().__init__()

    def emit(self, record: logging.LogRecord) -> None:
        try:
            # Use the StructuredMessage if the message is an instance of it
            if isinstance(record.msg, MyEvent):
                print(f"Timestamp: {record.msg.timestamp}, Message: {record.msg.message}")
        except Exception:
            self.handleError(record)

```

And this is how you could use it:

```
logger = logging.getLogger(EVENT_LOGGER_NAME)
logger.setLevel(logging.INFO)
my_handler = MyHandler()
logger.handlers = [my_handler]

```

#### Emitting logs[#](#emitting-logs "Link to this heading")

These two names are the root loggers for these types. Code that emits logs should use a child logger of these loggers. For example, if you are writing a module `my_module` and you want to emit trace logs, you should use the logger named:

```
import logging

from autogen_core import TRACE_LOGGER_NAME
logger = logging.getLogger(f"{TRACE_LOGGER_NAME}.my_module")

```

##### Emitting structured logs[#](#emitting-structured-logs "Link to this heading")

If your event is a dataclass, then it could be emitted in code like this:

```
import logging
from dataclasses import dataclass
from autogen_core import EVENT_LOGGER_NAME

@dataclass
class MyEvent:
    timestamp: str
    message: str

logger = logging.getLogger(EVENT_LOGGER_NAME + ".my_module")
logger.info(MyEvent("timestamp", "message"))

```

### Open Telemetry[#](#open-telemetry "Link to this heading")

AutoGen has native support for [open telemetry](https://opentelemetry.io/). This allows you to collect telemetry data from your application and send it to a telemetry backend of your choosing.

These are the components that are currently instrumented:

* Runtime (Single Threaded Agent Runtime, Worker Agent Runtime)

#### Instrumenting your application[#](#instrumenting-your-application "Link to this heading")

To instrument your application, you will need an sdk and an exporter. You may already have these if your application is already instrumented with open telemetry.

#### Clean instrumentation[#](#clean-instrumentation "Link to this heading")

If you do not have open telemetry set up in your application, you can follow these steps to instrument your application.

```
pip install opentelemetry-sdk

```

Depending on your open telemetry collector, you can use grpc or http to export your telemetry.

```
# Pick one of the following

pip install opentelemetry-exporter-otlp-proto-http
pip install opentelemetry-exporter-otlp-proto-grpc

```

Next, we need to get a tracer provider:

```
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

def configure_oltp_tracing(endpoint: str = None) -> trace.TracerProvider:
    # Configure Tracing
    tracer_provider = TracerProvider(resource=Resource({"service.name": "my-service"}))
    processor = BatchSpanProcessor(OTLPSpanExporter())
    tracer_provider.add_span_processor(processor)
    trace.set_tracer_provider(tracer_provider)

    return tracer_provider

```

Now you can send the trace\_provider when creating your runtime:

```
# for single threaded runtime
single_threaded_runtime = SingleThreadedAgentRuntime(tracer_provider=tracer_provider)
# or for worker runtime
worker_runtime = GrpcWorkerAgentRuntime(tracer_provider=tracer_provider)

```

And that’s it! Your application is now instrumented with open telemetry. You can now view your telemetry data in your telemetry backend.

##### Existing instrumentation[#](#existing-instrumentation "Link to this heading")

If you have open telemetry already set up in your application, you can pass the tracer provider to the runtime when creating it:

```
from opentelemetry import trace

# Get the tracer provider from your application
tracer_provider = trace.get_tracer_provider()

# for single threaded runtime
single_threaded_runtime = SingleThreadedAgentRuntime(tracer_provider=tracer_provider)
# or for worker runtime
worker_runtime = GrpcWorkerAgentRuntime(tracer_provider=tracer_provider)

```

### Distributed Agent Runtime[#](#distributed-agent-runtime "Link to this heading")

Attention

The distributed agent runtime is an experimental feature. Expect breaking changes
to the API.

A distributed agent runtime facilitates communication and agent lifecycle management
across process boundaries.
It consists of a host service and at least one worker runtime.

The host service maintains connections to all active worker runtimes,
facilitates message delivery, and keeps sessions for all direct messages (i.e., RPCs).
A worker runtime processes application code (agents) and connects to the host service.
It also advertises the agents which they support to the host service,
so the host service can deliver messages to the correct worker.

Note

The distributed agent runtime requires extra dependencies, install them using:

```
pip install "autogen-ext[grpc]"

```

We can start a host service using [`GrpcWorkerAgentRuntimeHost`](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHost "autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHost").

```
from autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntimeHost

host = GrpcWorkerAgentRuntimeHost(address="localhost:50051")
host.start()  # Start a host service in the background.

```

The above code starts the host service in the background and accepts
worker connections on port 50051.

Before running worker runtimes, let’s define our agent.
The agent will publish a new message on every message it receives.
It also keeps track of how many messages it has published, and
stops publishing new messages once it has published 5 messages.

```
from dataclasses import dataclass

from autogen_core import DefaultTopicId, MessageContext, RoutedAgent, default_subscription, message_handler

@dataclass
class MyMessage:
    content: str

@default_subscription
class MyAgent(RoutedAgent):
    def __init__(self, name: str) -> None:
        super().__init__("My agent")
        self._name = name
        self._counter = 0

    @message_handler
    async def my_message_handler(self, message: MyMessage, ctx: MessageContext) -> None:
        self._counter += 1
        if self._counter > 5:
            return
        content = f"{self._name}: Hello x {self._counter}"
        print(content)
        await self.publish_message(MyMessage(content=content), DefaultTopicId())

```

Now we can set up the worker agent runtimes.
We use [`GrpcWorkerAgentRuntime`](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime "autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime").
We set up two worker runtimes. Each runtime hosts one agent.
All agents publish and subscribe to the default topic, so they can see all
messages being published.

To run the agents, we publishes a message from a worker.

```
import asyncio

from autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntime

worker1 = GrpcWorkerAgentRuntime(host_address="localhost:50051")
await worker1.start()
await MyAgent.register(worker1, "worker1", lambda: MyAgent("worker1"))

worker2 = GrpcWorkerAgentRuntime(host_address="localhost:50051")
await worker2.start()
await MyAgent.register(worker2, "worker2", lambda: MyAgent("worker2"))

await worker2.publish_message(MyMessage(content="Hello!"), DefaultTopicId())

# Let the agents run for a while.
await asyncio.sleep(5)

```

```
worker1: Hello x 1
worker2: Hello x 1
worker2: Hello x 2
worker1: Hello x 2
worker1: Hello x 3
worker2: Hello x 3
worker2: Hello x 4
worker1: Hello x 4
worker1: Hello x 5
worker2: Hello x 5

```

We can see each agent published exactly 5 messages.

To stop the worker runtimes, we can call [`stop()`](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.stop "autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.stop").

```
await worker1.stop()
await worker2.stop()

# To keep the worker running until a termination signal is received (e.g., SIGTERM).
# await worker1.stop_when_signal()

```

We can call [`stop()`](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHost.stop "autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHost.stop")
to stop the host service.

```
await host.stop()

# To keep the host service running until a termination signal (e.g., SIGTERM)
# await host.stop_when_signal()

```

### Cross-Language Runtimes[#](#cross-language-runtimes "Link to this heading")

The process described above is largely the same, however all message types MUST use shared protobuf schemas for all cross-agent message types.

### Next Steps[#](#next-steps "Link to this heading")

To see complete examples of using distributed runtime, please take a look at the following samples:

* [Distributed Workers](https://github.com/microsoft/autogen/tree/main/python/samples/core_grpc_worker_runtime)
* [Distributed Semantic Router](https://github.com/microsoft/autogen/tree/main/python/samples/core_semantic_router)
* [Distributed Group Chat](https://github.com/microsoft/autogen/tree/main/python/samples/core_distributed-group-chat)

### Component config[#](#component-config "Link to this heading")

AutoGen components are able to be declaratively configured in a generic fashion. This is to support configuration based experiences, such as AutoGen studio, but it is also useful for many other scenarios.

The system that provides this is called “component configuration”. In AutoGen, a component is simply something that can be created from a config object and itself can be dumped to a config object. In this way, you can define a component in code and then get the config object from it.

This system is generic and allows for components defined outside of AutoGen itself (such as extensions) to be configured in the same way.

#### How does this differ from state?[#](#how-does-this-differ-from-state "Link to this heading")

This is a very important point to clarify. When we talk about serializing an object, we must include *all* data that makes that object itself. Including things like message history etc. When deserializing from serialized state, you must get back the *exact* same object. This is not the case with component configuration.

Component configuration should be thought of as the blueprint for an object, and can be stamped out many times to create many instances of the same configured object.

#### Usage[#](#usage "Link to this heading")

If you have a component in Python and want to get the config for it, simply call [`dump_component()`](#autogen_core.ComponentToConfig.dump_component "autogen_core.ComponentToConfig.dump_component") on it. The resulting object can be passed back into [`load_component()`](#autogen_core.ComponentLoader.load_component "autogen_core.ComponentLoader.load_component") to get the component back.

##### Loading a component from a config[#](#loading-a-component-from-a-config "Link to this heading")

To load a component from a config object, you can use the [`load_component()`](#autogen_core.ComponentLoader.load_component "autogen_core.ComponentLoader.load_component") method. This method will take a config object and return a component object. It is best to call this method on the interface you want. For example to load a model client:

```
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "openai_chat_completion_client",
    "config": {"model": "gpt-4o"},
}

client = ChatCompletionClient.load_component(config)

```

#### Creating a component class[#](#creating-a-component-class "Link to this heading")

To add component functionality to a given class:

1. Add a call to [`Component()`](#autogen_core.Component "autogen_core.Component") in the class inheritance list.
2. Implment the [`_to_config()`](#autogen_core.ComponentToConfig._to_config "autogen_core.ComponentToConfig._to_config") and [`_from_config()`](#autogen_core.ComponentFromConfig._from_config "autogen_core.ComponentFromConfig._from_config") methods

For example:

```
from autogen_core import Component, ComponentBase
from pydantic import BaseModel

class Config(BaseModel):
    value: str

class MyComponent(ComponentBase[Config], Component[Config]):
    component_type = "custom"
    component_config_schema = Config

    def __init__(self, value: str):
        self.value = value

    def _to_config(self) -> Config:
        return Config(value=self.value)

    @classmethod
    def _from_config(cls, config: Config) -> "MyComponent":
        return cls(value=config.value)

```

#### Secrets[#](#secrets "Link to this heading")

If a field of a config object is a secret value, it should be marked using [`SecretStr`](https://docs.pydantic.dev/latest/api/types/#pydantic.types.SecretStr), this will ensure that the value will not be dumped to the config object.

For example:

```
from pydantic import BaseModel, SecretStr

class ClientConfig(BaseModel):
    endpoint: str
    api_key: SecretStr

```

### Model Clients[#](#model-clients "Link to this heading")

AutoGen provides a suite of built-in model clients for using ChatCompletion API.
All model clients implement the [`ChatCompletionClient`](#autogen_core.models.ChatCompletionClient "autogen_core.models.ChatCompletionClient") protocol class.

Currently we support the following built-in model clients:

* [`OpenAIChatCompletionClient`](#autogen_ext.models.openai.OpenAIChatCompletionClient "autogen_ext.models.openai.OpenAIChatCompletionClient"): for OpenAI models and models with OpenAI API compatibility (e.g., Gemini).
* [`AzureOpenAIChatCompletionClient`](#autogen_ext.models.openai.AzureOpenAIChatCompletionClient "autogen_ext.models.openai.AzureOpenAIChatCompletionClient"): for Azure OpenAI models.
* [`AzureAIChatCompletionClient`](#autogen_ext.models.azure.AzureAIChatCompletionClient "autogen_ext.models.azure.AzureAIChatCompletionClient"): for GitHub models and models hosted on Azure.
* [`OllamaChatCompletionClient`](#autogen_ext.models.ollama.OllamaChatCompletionClient "autogen_ext.models.ollama.OllamaChatCompletionClient") (Experimental): for local models hosted on Ollama.
* [`AnthropicChatCompletionClient`](#autogen_ext.models.anthropic.AnthropicChatCompletionClient "autogen_ext.models.anthropic.AnthropicChatCompletionClient") (Experimental): for models hosted on Anthropic.
* [`SKChatCompletionAdapter`](#autogen_ext.models.semantic_kernel.SKChatCompletionAdapter "autogen_ext.models.semantic_kernel.SKChatCompletionAdapter"): adapter for Semantic Kernel AI connectors.

For more information on how to use these model clients, please refer to the documentation of each client.

#### Log Model Calls[#](#log-model-calls "Link to this heading")

AutoGen uses standard Python logging module to log events like model calls and responses.
The logger name is [`autogen_core.EVENT_LOGGER_NAME`](#autogen_core.EVENT_LOGGER_NAME "autogen_core.EVENT_LOGGER_NAME"), and the event type is `LLMCall`.

```
import logging

from autogen_core import EVENT_LOGGER_NAME

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(EVENT_LOGGER_NAME)
logger.addHandler(logging.StreamHandler())
logger.setLevel(logging.INFO)

```

#### Call Model Client[#](#call-model-client "Link to this heading")

To call a model client, you can use the [`create()`](#autogen_core.models.ChatCompletionClient.create "autogen_core.models.ChatCompletionClient.create") method.
This example uses the [`OpenAIChatCompletionClient`](#autogen_ext.models.openai.OpenAIChatCompletionClient "autogen_ext.models.openai.OpenAIChatCompletionClient") to call an OpenAI model.

```
from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4", temperature=0.3
)  # assuming OPENAI_API_KEY is set in the environment.

result = await model_client.create([UserMessage(content="What is the capital of France?", source="user")])
print(result)

```

```
finish_reason='stop' content='The capital of France is Paris.' usage=RequestUsage(prompt_tokens=15, completion_tokens=8) cached=False logprobs=None thought=None

```

#### Streaming Tokens[#](#streaming-tokens "Link to this heading")

You can use the [`create_stream()`](#autogen_core.models.ChatCompletionClient.create_stream "autogen_core.models.ChatCompletionClient.create_stream") method to create a
chat completion request with streaming token chunks.

```
from autogen_core.models import CreateResult, UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(model="gpt-4o")  # assuming OPENAI_API_KEY is set in the environment.

messages = [
    UserMessage(content="Write a very short story about a dragon.", source="user"),
]

# Create a stream.
stream = model_client.create_stream(messages=messages)

# Iterate over the stream and print the responses.
print("Streamed responses:")
async for chunk in stream:  # type: ignore
    if isinstance(chunk, str):
        # The chunk is a string.
        print(chunk, flush=True, end="")
    else:
        # The final chunk is a CreateResult object.
        assert isinstance(chunk, CreateResult) and isinstance(chunk.content, str)
        # The last response is a CreateResult object with the complete message.
        print("\n\n------------\n")
        print("The complete response:", flush=True)
        print(chunk.content, flush=True)

```

```
Streamed responses:
In the heart of an ancient forest, beneath the shadow of snow-capped peaks, a dragon named Elara lived secretly for centuries. Elara was unlike any dragon from the old tales; her scales shimmered with a deep emerald hue, each scale engraved with symbols of lost wisdom. The villagers in the nearby valley spoke of mysterious lights dancing across the night sky, but none dared venture close enough to solve the enigma.

One cold winter's eve, a young girl named Lira, brimming with curiosity and armed with the innocence of youth, wandered into Elara’s domain. Instead of fire and fury, she found warmth and a gentle gaze. The dragon shared stories of a world long forgotten and in return, Lira gifted her simple stories of human life, rich in laughter and scent of earth.

From that night on, the villagers noticed subtle changes—the crops grew taller, and the air seemed sweeter. Elara had infused the valley with ancient magic, a guardian of balance, watching quietly as her new friend thrived under the stars. And so, Lira and Elara’s bond marked the beginning of a timeless friendship that spun tales of hope whispered through the leaves of the ever-verdant forest.

------------

The complete response:
In the heart of an ancient forest, beneath the shadow of snow-capped peaks, a dragon named Elara lived secretly for centuries. Elara was unlike any dragon from the old tales; her scales shimmered with a deep emerald hue, each scale engraved with symbols of lost wisdom. The villagers in the nearby valley spoke of mysterious lights dancing across the night sky, but none dared venture close enough to solve the enigma.

One cold winter's eve, a young girl named Lira, brimming with curiosity and armed with the innocence of youth, wandered into Elara’s domain. Instead of fire and fury, she found warmth and a gentle gaze. The dragon shared stories of a world long forgotten and in return, Lira gifted her simple stories of human life, rich in laughter and scent of earth.

From that night on, the villagers noticed subtle changes—the crops grew taller, and the air seemed sweeter. Elara had infused the valley with ancient magic, a guardian of balance, watching quietly as her new friend thrived under the stars. And so, Lira and Elara’s bond marked the beginning of a timeless friendship that spun tales of hope whispered through the leaves of the ever-verdant forest.

------------

The token usage was:
RequestUsage(prompt_tokens=0, completion_tokens=0)

```

Note

The last response in the streaming response is always the final response
of the type [`CreateResult`](#autogen_core.models.CreateResult "autogen_core.models.CreateResult").

Note

The default usage response is to return zero values. To enable usage,
see [`create_stream()`](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create_stream "autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create_stream")
for more details.

#### Structured Output[#](#structured-output "Link to this heading")

Structured output can be enabled by setting the `response_format` field in
[`OpenAIChatCompletionClient`](#autogen_ext.models.openai.OpenAIChatCompletionClient "autogen_ext.models.openai.OpenAIChatCompletionClient") and [`AzureOpenAIChatCompletionClient`](#autogen_ext.models.openai.AzureOpenAIChatCompletionClient "autogen_ext.models.openai.AzureOpenAIChatCompletionClient") to
as a [Pydantic BaseModel](https://docs.pydantic.dev/latest/concepts/models/) class.

Note

Structured output is only available for models that support it. It also
requires the model client to support structured output as well.
Currently, the [`OpenAIChatCompletionClient`](#autogen_ext.models.openai.OpenAIChatCompletionClient "autogen_ext.models.openai.OpenAIChatCompletionClient")
and [`AzureOpenAIChatCompletionClient`](#autogen_ext.models.openai.AzureOpenAIChatCompletionClient "autogen_ext.models.openai.AzureOpenAIChatCompletionClient")
support structured output.

```
from typing import Literal

from pydantic import BaseModel

# The response format for the agent as a Pydantic base model.
class AgentResponse(BaseModel):
    thoughts: str
    response: Literal["happy", "sad", "neutral"]

# Create an agent that uses the OpenAI GPT-4o model with the custom response format.
model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    response_format=AgentResponse,  # type: ignore
)

# Send a message list to the model and await the response.
messages = [
    UserMessage(content="I am happy.", source="user"),
]
response = await model_client.create(messages=messages)
assert isinstance(response.content, str)
parsed_response = AgentResponse.model_validate_json(response.content)
print(parsed_response.thoughts)
print(parsed_response.response)

# Close the connection to the model client.
await model_client.close()

```

```
I'm glad to hear that you're feeling happy! It's such a great emotion that can brighten your whole day. Is there anything in particular that's bringing you joy today? 😊
happy

```

You also use the `extra_create_args` parameter in the [`create()`](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create "autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create") method
to set the `response_format` field so that the structured output can be configured for each request.

#### Caching Model Responses[#](#caching-model-responses "Link to this heading")

`autogen_ext` implements [`ChatCompletionCache`](#autogen_ext.models.cache.ChatCompletionCache "autogen_ext.models.cache.ChatCompletionCache") that can wrap any [`ChatCompletionClient`](#autogen_core.models.ChatCompletionClient "autogen_core.models.ChatCompletionClient"). Using this wrapper avoids incurring token usage when querying the underlying client with the same prompt multiple times.

`ChatCompletionCache` uses a [`CacheStore`](#autogen_core.CacheStore "autogen_core.CacheStore") protocol. We have implemented some useful variants of [`CacheStore`](#autogen_core.CacheStore "autogen_core.CacheStore") including [`DiskCacheStore`](#autogen_ext.cache_store.diskcache.DiskCacheStore "autogen_ext.cache_store.diskcache.DiskCacheStore") and [`RedisStore`](#autogen_ext.cache_store.redis.RedisStore "autogen_ext.cache_store.redis.RedisStore").

Here’s an example of using `diskcache` for local caching:

```
# pip install -U "autogen-ext[openai, diskcache]"

```

```
import asyncio
import tempfile

from autogen_core.models import UserMessage
from autogen_ext.cache_store.diskcache import DiskCacheStore
from autogen_ext.models.cache import CHAT_CACHE_VALUE_TYPE, ChatCompletionCache
from autogen_ext.models.openai import OpenAIChatCompletionClient
from diskcache import Cache

async def main() -> None:
    with tempfile.TemporaryDirectory() as tmpdirname:
        # Initialize the original client
        openai_model_client = OpenAIChatCompletionClient(model="gpt-4o")

        # Then initialize the CacheStore, in this case with diskcache.Cache.
        # You can also use redis like:
        # from autogen_ext.cache_store.redis import RedisStore
        # import redis
        # redis_instance = redis.Redis()
        # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)
        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))
        cache_client = ChatCompletionCache(openai_model_client, cache_store)

        response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
        print(response)  # Should print response from OpenAI
        response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
        print(response)  # Should print cached response

        await openai_model_client.close()
        await cache_client.close()

asyncio.run(main())

```

```
True

```

Inspecting `cached_client.total_usage()` (or `model_client.total_usage()`) before and after a cached response should yield idential counts.

Note that the caching is sensitive to the exact arguments provided to `cached_client.create` or `cached_client.create_stream`, so changing `tools` or `json_output` arguments might lead to a cache miss.

#### Build an Agent with a Model Client[#](#build-an-agent-with-a-model-client "Link to this heading")

Let’s create a simple AI agent that can respond to messages using the ChatCompletion API.

```
from dataclasses import dataclass

from autogen_core import MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler
from autogen_core.models import ChatCompletionClient, SystemMessage, UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient

@dataclass
class Message:
    content: str

class SimpleAgent(RoutedAgent):
    def __init__(self, model_client: ChatCompletionClient) -> None:
        super().__init__("A simple agent")
        self._system_messages = [SystemMessage(content="You are a helpful AI assistant.")]
        self._model_client = model_client

    @message_handler
    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:
        # Prepare input to the chat completion model.
        user_message = UserMessage(content=message.content, source="user")
        response = await self._model_client.create(
            self._system_messages + [user_message], cancellation_token=ctx.cancellation_token
        )
        # Return with the model's response.
        assert isinstance(response.content, str)
        return Message(content=response.content)

```

The `SimpleAgent` class is a subclass of the
[`autogen_core.RoutedAgent`](#autogen_core.RoutedAgent "autogen_core.RoutedAgent") class for the convenience of automatically routing messages to the appropriate handlers.
It has a single handler, `handle_user_message`, which handles message from the user. It uses the `ChatCompletionClient` to generate a response to the message.
It then returns the response to the user, following the direct communication model.

Note

The `cancellation_token` of the type [`autogen_core.CancellationToken`](#autogen_core.CancellationToken "autogen_core.CancellationToken") is used to cancel
asynchronous operations. It is linked to async calls inside the message handlers
and can be used by the caller to cancel the handlers.

```
# Create the runtime and register the agent.
from autogen_core import AgentId

model_client = OpenAIChatCompletionClient(
    model="gpt-4o-mini",
    # api_key="sk-...", # Optional if you have an OPENAI_API_KEY set in the environment.
)

runtime = SingleThreadedAgentRuntime()
await SimpleAgent.register(
    runtime,
    "simple_agent",
    lambda: SimpleAgent(model_client=model_client),
)
# Start the runtime processing messages.
runtime.start()
# Send a message to the agent and get the response.
message = Message("Hello, what are some fun things to do in Seattle?")
response = await runtime.send_message(message, AgentId("simple_agent", "default"))
print(response.content)
# Stop the runtime processing messages.
await runtime.stop()
await model_client.close()

```

```
Seattle is a vibrant city with a wide range of activities and attractions. Here are some fun things to do in Seattle:

1. **Space Needle**: Visit this iconic observation tower for stunning views of the city and surrounding mountains.

2. **Pike Place Market**: Explore this historic market where you can see the famous fish toss, buy local produce, and find unique crafts and eateries.

3. **Museum of Pop Culture (MoPOP)**: Dive into the world of contemporary culture, music, and science fiction at this interactive museum.

4. **Chihuly Garden and Glass**: Marvel at the beautiful glass art installations by artist Dale Chihuly, located right next to the Space Needle.

5. **Seattle Aquarium**: Discover the diverse marine life of the Pacific Northwest at this engaging aquarium.

6. **Seattle Art Museum**: Explore a vast collection of art from around the world, including contemporary and indigenous art.

7. **Kerry Park**: For one of the best views of the Seattle skyline, head to this small park on Queen Anne Hill.

8. **Ballard Locks**: Watch boats pass through the locks and observe the salmon ladder to see salmon migrating.

9. **Ferry to Bainbridge Island**: Take a scenic ferry ride across Puget Sound to enjoy charming shops, restaurants, and beautiful natural scenery.

10. **Olympic Sculpture Park**: Stroll through this outdoor park with large-scale sculptures and stunning views of the waterfront and mountains.

11. **Underground Tour**: Discover Seattle's history on this quirky tour of the city's underground passageways in Pioneer Square.

12. **Seattle Waterfront**: Enjoy the shops, restaurants, and attractions along the waterfront, including the Seattle Great Wheel and the aquarium.

13. **Discovery Park**: Explore the largest green space in Seattle, featuring trails, beaches, and views of Puget Sound.

14. **Food Tours**: Try out Seattle’s diverse culinary scene, including fresh seafood, international cuisines, and coffee culture (don’t miss the original Starbucks!).

15. **Attend a Sports Game**: Catch a Seahawks (NFL), Mariners (MLB), or Sounders (MLS) game for a lively local experience.

Whether you're interested in culture, nature, food, or history, Seattle has something for everyone to enjoy!

```

The above `SimpleAgent` always responds with a fresh context that contains only
the system message and the latest user’s message.
We can use model context classes from [`autogen_core.model_context`](#module-autogen_core.model_context "autogen_core.model_context")
to make the agent “remember” previous conversations.
See the [Model Context](#document-user-guide/core-user-guide/components/model-context) page for more details.

#### API Keys From Environment Variables[#](#api-keys-from-environment-variables "Link to this heading")

In the examples above, we show that you can provide the API key through the `api_key` argument. Importantly, the OpenAI and Azure OpenAI clients use the [openai package](https://github.com/openai/openai-python/blob/3f8d8205ae41c389541e125336b0ae0c5e437661/src/openai/__init__.py#L260), which will automatically read an api key from the environment variable if one is not provided.

* For OpenAI, you can set the `OPENAI_API_KEY` environment variable.
* For Azure OpenAI, you can set the `AZURE_OPENAI_API_KEY` environment variable.

In addition, for Gemini (Beta), you can set the `GEMINI_API_KEY` environment variable.

This is a good practice to explore, as it avoids including sensitive api keys in your code.

### Model Context[#](#model-context "Link to this heading")

A model context supports storage and retrieval of Chat Completion messages.
It is always used together with a model client to generate LLM-based responses.

For example, [`BufferedChatCompletionContext`](#autogen_core.model_context.BufferedChatCompletionContext "autogen_core.model_context.BufferedChatCompletionContext")
is a most-recent-used (MRU) context that stores the most recent `buffer_size`
number of messages. This is useful to avoid context overflow in many LLMs.

Let’s see an example that uses
[`BufferedChatCompletionContext`](#autogen_core.model_context.BufferedChatCompletionContext "autogen_core.model_context.BufferedChatCompletionContext").

```
from dataclasses import dataclass

from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler
from autogen_core.model_context import BufferedChatCompletionContext
from autogen_core.models import AssistantMessage, ChatCompletionClient, SystemMessage, UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient

```

```
@dataclass
class Message:
    content: str

```

```
class SimpleAgentWithContext(RoutedAgent):
    def __init__(self, model_client: ChatCompletionClient) -> None:
        super().__init__("A simple agent")
        self._system_messages = [SystemMessage(content="You are a helpful AI assistant.")]
        self._model_client = model_client
        self._model_context = BufferedChatCompletionContext(buffer_size=5)

    @message_handler
    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:
        # Prepare input to the chat completion model.
        user_message = UserMessage(content=message.content, source="user")
        # Add message to model context.
        await self._model_context.add_message(user_message)
        # Generate a response.
        response = await self._model_client.create(
            self._system_messages + (await self._model_context.get_messages()),
            cancellation_token=ctx.cancellation_token,
        )
        # Return with the model's response.
        assert isinstance(response.content, str)
        # Add message to model context.
        await self._model_context.add_message(AssistantMessage(content=response.content, source=self.metadata["type"]))
        return Message(content=response.content)

```

Now let’s try to ask follow up questions after the first one.

```
model_client = OpenAIChatCompletionClient(
    model="gpt-4o-mini",
    # api_key="sk-...", # Optional if you have an OPENAI_API_KEY set in the environment.
)

runtime = SingleThreadedAgentRuntime()
await SimpleAgentWithContext.register(
    runtime,
    "simple_agent_context",
    lambda: SimpleAgentWithContext(model_client=model_client),
)
# Start the runtime processing messages.
runtime.start()
agent_id = AgentId("simple_agent_context", "default")

# First question.
message = Message("Hello, what are some fun things to do in Seattle?")
print(f"Question: {message.content}")
response = await runtime.send_message(message, agent_id)
print(f"Response: {response.content}")
print("-----")

# Second question.
message = Message("What was the first thing you mentioned?")
print(f"Question: {message.content}")
response = await runtime.send_message(message, agent_id)
print(f"Response: {response.content}")

# Stop the runtime processing messages.
await runtime.stop()
await model_client.close()

```

```
Question: Hello, what are some fun things to do in Seattle?
Response: Seattle offers a variety of fun activities and attractions. Here are some highlights:

1. **Pike Place Market**: Visit this iconic market to explore local vendors, fresh produce, artisanal products, and watch the famous fish throwing.

2. **Space Needle**: Take a trip to the observation deck for stunning panoramic views of the city, Puget Sound, and the surrounding mountains.

3. **Chihuly Garden and Glass**: Marvel at the stunning glass art installations created by artist Dale Chihuly, located right next to the Space Needle.

4. **Seattle Waterfront**: Enjoy a stroll along the waterfront, visit the Seattle Aquarium, and take a ferry ride to nearby islands like Bainbridge Island.

5. **Museum of Pop Culture (MoPOP)**: Explore exhibits on music, science fiction, and pop culture in this architecturally striking building.

6. **Seattle Art Museum (SAM)**: Discover an extensive collection of art from around the world, including contemporary and Native American art.

7. **Gas Works Park**: Relax in this unique park that features remnants of an old gasification plant, offering great views of the Seattle skyline and Lake Union.

8. **Discovery Park**: Enjoy nature trails, beaches, and beautiful views of the Puget Sound and the Olympic Mountains in this large urban park.

9. **Ballard Locks**: Watch boats navigate the locks and see fish swimming upstream during the salmon migration season.

10. **Fremont Troll**: Check out this quirky public art installation under a bridge in the Fremont neighborhood.

11. **Underground Tour**: Take an entertaining guided tour through the underground passages of Pioneer Square to learn about Seattle's history.

12. **Brewery Tours**: Seattle is known for its craft beer scene. Visit local breweries for tastings and tours.

13. **Seattle Center**: Explore the cultural complex that includes the Space Needle, MoPOP, and various festivals and events throughout the year.

These are just a few options, and Seattle has something for everyone, whether you're into outdoor activities, culture, history, or food!
-----
Question: What was the first thing you mentioned?
Response: The first thing I mentioned was **Pike Place Market**. It's an iconic market in Seattle known for its local vendors, fresh produce, artisanal products, and the famous fish throwing by the fishmongers. It's a vibrant place full of sights, sounds, and delicious food.

```

From the second response, you can see the agent now can recall its own previous responses.

### Tools[#](#tools "Link to this heading")

Tools are code that can be executed by an agent to perform actions. A tool
can be a simple function such as a calculator, or an API call to a third-party service
such as stock price lookup or weather forecast.
In the context of AI agents, tools are designed to be executed by agents in
response to model-generated function calls.

AutoGen provides the [`autogen_core.tools`](#module-autogen_core.tools "autogen_core.tools") module with a suite of built-in
tools and utilities for creating and running custom tools.

#### Built-in Tools[#](#built-in-tools "Link to this heading")

One of the built-in tools is the [`PythonCodeExecutionTool`](#autogen_ext.tools.code_execution.PythonCodeExecutionTool "autogen_ext.tools.code_execution.PythonCodeExecutionTool"),
which allows agents to execute Python code snippets.

Here is how you create the tool and use it.

```
from autogen_core import CancellationToken
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_ext.tools.code_execution import PythonCodeExecutionTool

# Create the tool.
code_executor = DockerCommandLineCodeExecutor()
await code_executor.start()
code_execution_tool = PythonCodeExecutionTool(code_executor)
cancellation_token = CancellationToken()

# Use the tool directly without an agent.
code = "print('Hello, world!')"
result = await code_execution_tool.run_json({"code": code}, cancellation_token)
print(code_execution_tool.return_value_as_string(result))

```

```
Hello, world!

```

The [`DockerCommandLineCodeExecutor`](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor "autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor")
class is a built-in code executor that runs Python code snippets in a subprocess
in the command line environment of a docker container.
The [`PythonCodeExecutionTool`](#autogen_ext.tools.code_execution.PythonCodeExecutionTool "autogen_ext.tools.code_execution.PythonCodeExecutionTool") class wraps the code executor
and provides a simple interface to execute Python code snippets.

Examples of other built-in tools

* [`LocalSearchTool`](#autogen_ext.tools.graphrag.LocalSearchTool "autogen_ext.tools.graphrag.LocalSearchTool") and [`GlobalSearchTool`](#autogen_ext.tools.graphrag.GlobalSearchTool "autogen_ext.tools.graphrag.GlobalSearchTool") for using [GraphRAG](https://github.com/microsoft/graphrag).
* [`mcp_server_tools`](#autogen_ext.tools.mcp.mcp_server_tools "autogen_ext.tools.mcp.mcp_server_tools") for using [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) servers as tools.
* [`HttpTool`](#autogen_ext.tools.http.HttpTool "autogen_ext.tools.http.HttpTool") for making HTTP requests to REST APIs.
* [`LangChainToolAdapter`](#autogen_ext.tools.langchain.LangChainToolAdapter "autogen_ext.tools.langchain.LangChainToolAdapter") for using LangChain tools.

#### Custom Function Tools[#](#custom-function-tools "Link to this heading")

A tool can also be a simple Python function that performs a specific action.
To create a custom function tool, you just need to create a Python function
and use the [`FunctionTool`](#autogen_core.tools.FunctionTool "autogen_core.tools.FunctionTool") class to wrap it.

The [`FunctionTool`](#autogen_core.tools.FunctionTool "autogen_core.tools.FunctionTool") class uses descriptions and type annotations
to inform the LLM when and how to use a given function. The description provides context
about the function’s purpose and intended use cases, while type annotations inform the LLM about
the expected parameters and return type.

For example, a simple tool to obtain the stock price of a company might look like this:

```
import random

from autogen_core import CancellationToken
from autogen_core.tools import FunctionTool
from typing_extensions import Annotated

async def get_stock_price(ticker: str, date: Annotated[str, "Date in YYYY/MM/DD"]) -> float:
    # Returns a random stock price for demonstration purposes.
    return random.uniform(10, 200)

# Create a function tool.
stock_price_tool = FunctionTool(get_stock_price, description="Get the stock price.")

# Run the tool.
cancellation_token = CancellationToken()
result = await stock_price_tool.run_json({"ticker": "AAPL", "date": "2021/01/01"}, cancellation_token)

# Print the result.
print(stock_price_tool.return_value_as_string(result))

```

```
36.63801673457121

```

#### Calling Tools with Model Clients[#](#calling-tools-with-model-clients "Link to this heading")

Model clients can generate tool calls when they are provided with a list of tools.

Here is an example of how to use the [`FunctionTool`](#autogen_core.tools.FunctionTool "autogen_core.tools.FunctionTool") class
with a [`OpenAIChatCompletionClient`](#autogen_ext.models.openai.OpenAIChatCompletionClient "autogen_ext.models.openai.OpenAIChatCompletionClient").
Other model client classes can be used in a similar way. See [Model Clients](#document-user-guide/core-user-guide/components/model-clients)
for more details.

```
import json

from autogen_core.models import AssistantMessage, FunctionExecutionResult, FunctionExecutionResultMessage, UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient

# Create the OpenAI chat completion client. Using OPENAI_API_KEY from environment variable.
model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")

# Create a user message.
user_message = UserMessage(content="What is the stock price of AAPL on 2021/01/01?", source="user")

# Run the chat completion with the stock_price_tool defined above.
cancellation_token = CancellationToken()
create_result = await model_client.create(
    messages=[user_message], tools=[stock_price_tool], cancellation_token=cancellation_token
)
create_result.content

```

```
[FunctionCall(id='call_tpJ5J1Xoxi84Sw4v0scH0qBM', arguments='{"ticker":"AAPL","date":"2021/01/01"}', name='get_stock_price')]

```

The result is a list of [`FunctionCall`](#autogen_core.FunctionCall "autogen_core.FunctionCall") objects, which can be
used to run the corresponding tools.

```
arguments = json.loads(create_result.content[0].arguments)  # type: ignore
tool_result = await stock_price_tool.run_json(arguments, cancellation_token)
tool_result_str = stock_price_tool.return_value_as_string(tool_result)
tool_result_str

```

```
'32.381250753393104'

```

Now you can make another model client call to have the model generate a reflection
on the result of the tool execution.

The result of the tool call is wrapped in a [`FunctionExecutionResult`](#autogen_core.models.FunctionExecutionResult "autogen_core.models.FunctionExecutionResult")
object, which contains the result of the tool execution and the ID of the tool that was called.
The model client can use this information to generate a reflection on the result of the tool execution.

```
# Create a function execution result
exec_result = FunctionExecutionResult(
    call_id=create_result.content[0].id,  # type: ignore
    content=tool_result_str,
    is_error=False,
    name=stock_price_tool.name,
)

# Make another chat completion with the history and function execution result message.
messages = [
    user_message,
    AssistantMessage(content=create_result.content, source="assistant"),  # assistant message with tool call
    FunctionExecutionResultMessage(content=[exec_result]),  # function execution result message
]
create_result = await model_client.create(messages=messages, cancellation_token=cancellation_token)  # type: ignore
print(create_result.content)
await model_client.close()

```

```
The stock price of AAPL (Apple Inc.) on January 1, 2021, was approximately $32.38.

```

#### Tool-Equipped Agent[#](#tool-equipped-agent "Link to this heading")

Putting the model client and the tools together, you can create a tool-equipped agent
that can use tools to perform actions, and reflect on the results of those actions.

```
import asyncio
import json
from dataclasses import dataclass
from typing import List

from autogen_core import (
    AgentId,
    FunctionCall,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    message_handler,
)
from autogen_core.models import (
    ChatCompletionClient,
    LLMMessage,
    SystemMessage,
    UserMessage,
)
from autogen_core.tools import FunctionTool, Tool
from autogen_ext.models.openai import OpenAIChatCompletionClient

@dataclass
class Message:
    content: str

class ToolUseAgent(RoutedAgent):
    def __init__(self, model_client: ChatCompletionClient, tool_schema: List[Tool]) -> None:
        super().__init__("An agent with tools")
        self._system_messages: List[LLMMessage] = [SystemMessage(content="You are a helpful AI assistant.")]
        self._model_client = model_client
        self._tools = tool_schema

    @message_handler
    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:
        # Create a session of messages.
        session: List[LLMMessage] = self._system_messages + [UserMessage(content=message.content, source="user")]

        # Run the chat completion with the tools.
        create_result = await self._model_client.create(
            messages=session,
            tools=self._tools,
            cancellation_token=ctx.cancellation_token,
        )

        # If there are no tool calls, return the result.
        if isinstance(create_result.content, str):
            return Message(content=create_result.content)
        assert isinstance(create_result.content, list) and all(
            isinstance(call, FunctionCall) for call in create_result.content
        )

        # Add the first model create result to the session.
        session.append(AssistantMessage(content=create_result.content, source="assistant"))

        # Execute the tool calls.
        results = await asyncio.gather(
            *[self._execute_tool_call(call, ctx.cancellation_token) for call in create_result.content]
        )

        # Add the function execution results to the session.
        session.append(FunctionExecutionResultMessage(content=results))

        # Run the chat completion again to reflect on the history and function execution results.
        create_result = await self._model_client.create(
            messages=session,
            cancellation_token=ctx.cancellation_token,
        )
        assert isinstance(create_result.content, str)

        # Return the result as a message.
        return Message(content=create_result.content)

    async def _execute_tool_call(
        self, call: FunctionCall, cancellation_token: CancellationToken
    ) -> FunctionExecutionResult:
        # Find the tool by name.
        tool = next((tool for tool in self._tools if tool.name == call.name), None)
        assert tool is not None

        # Run the tool and capture the result.
        try:
            arguments = json.loads(call.arguments)
            result = await tool.run_json(arguments, cancellation_token)
            return FunctionExecutionResult(
                call_id=call.id, content=tool.return_value_as_string(result), is_error=False, name=tool.name
            )
        except Exception as e:
            return FunctionExecutionResult(call_id=call.id, content=str(e), is_error=True, name=tool.name)

```

When handling a user message, the `ToolUseAgent` class first use the model client
to generate a list of function calls to the tools, and then run the tools
and generate a reflection on the results of the tool execution.
The reflection is then returned to the user as the agent’s response.

To run the agent, let’s create a runtime and register the agent with the runtime.

```
# Create the model client.
model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")
# Create a runtime.
runtime = SingleThreadedAgentRuntime()
# Create the tools.
tools: List[Tool] = [FunctionTool(get_stock_price, description="Get the stock price.")]
# Register the agents.
await ToolUseAgent.register(
    runtime,
    "tool_use_agent",
    lambda: ToolUseAgent(
        model_client=model_client,
        tool_schema=tools,
    ),
)

```

```
AgentType(type='tool_use_agent')

```

This example uses the [`OpenAIChatCompletionClient`](#autogen_ext.models.openai.OpenAIChatCompletionClient "autogen_ext.models.openai.OpenAIChatCompletionClient"),
for Azure OpenAI and other clients, see [Model Clients](#document-user-guide/core-user-guide/components/model-clients).
Let’s test the agent with a question about stock price.

```
# Start processing messages.
runtime.start()
# Send a direct message to the tool agent.
tool_use_agent = AgentId("tool_use_agent", "default")
response = await runtime.send_message(Message("What is the stock price of NVDA on 2024/06/01?"), tool_use_agent)
print(response.content)
# Stop processing messages.
await runtime.stop()
await model_client.close()

```

```
The stock price of NVIDIA (NVDA) on June 1, 2024, was approximately $140.05.

```

### Command Line Code Executors[#](#command-line-code-executors "Link to this heading")

Command line code execution is the simplest form of code execution.
Generally speaking, it will save each code block to a file and then execute that file.
This means that each code block is executed in a new process. There are two forms of this executor:

* Docker ([`DockerCommandLineCodeExecutor`](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor "autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor")) - this is where all commands are executed in a Docker container
* Local ([`LocalCommandLineCodeExecutor`](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor "autogen_ext.code_executors.local.LocalCommandLineCodeExecutor")) - this is where all commands are executed on the host machine

#### Docker[#](#docker "Link to this heading")

Note

To use [`DockerCommandLineCodeExecutor`](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor "autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor"), ensure the `autogen-ext[docker]` package is installed. For more details, see the [Packages Documentation](https://microsoft.github.io/autogen/dev/packages/index.html).

The [`DockerCommandLineCodeExecutor`](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor "autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor") will create a Docker container and run all commands within that container.
The default image that is used is `python:3-slim`, this can be customized by passing the `image` parameter to the constructor.
If the image is not found locally then the class will try to pull it.
Therefore, having built the image locally is enough. The only thing required for this image to be compatible with the executor is to have `sh` and `python` installed.
Therefore, creating a custom image is a simple and effective way to ensure required system dependencies are available.

You can use the executor as a context manager to ensure the container is cleaned up after use.
Otherwise, the `atexit` module will be used to stop the container when the program exits.

##### Inspecting the container[#](#inspecting-the-container "Link to this heading")

If you wish to keep the container around after AutoGen is finished using it for whatever reason (e.g. to inspect the container),
then you can set the `auto_remove` parameter to `False` when creating the executor.
`stop_container` can also be set to `False` to prevent the container from being stopped at the end of the execution.

##### Example[#](#example "Link to this heading")

```
from pathlib import Path

from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor

work_dir = Path("coding")
work_dir.mkdir(exist_ok=True)

async with DockerCommandLineCodeExecutor(work_dir=work_dir) as executor:  # type: ignore
    print(
        await executor.execute_code_blocks(
            code_blocks=[
                CodeBlock(language="python", code="print('Hello, World!')"),
            ],
            cancellation_token=CancellationToken(),
        )
    )

```

```
CommandLineCodeResult(exit_code=0, output='Hello, World!\n', code_file='coding/tmp_code_07da107bb575cc4e02b0e1d6d99cc204.python')

```

##### Combining an Application in Docker with a Docker based executor[#](#combining-an-application-in-docker-with-a-docker-based-executor "Link to this heading")

It is desirable to bundle your application into a Docker image. But then, how do you allow your containerised application to execute code in a different container?

The recommended approach to this is called “Docker out of Docker”, where the Docker socket is mounted to the main AutoGen container, so that it can spawn and control “sibling” containers on the host. This is better than what is called “Docker in Docker”, where the main container runs a Docker daemon and spawns containers within itself. You can read more about this [here](https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/).

To do this you would need to mount the Docker socket into the container running your application. This can be done by adding the following to the `docker run` command:

```
-v /var/run/docker.sock:/var/run/docker.sock

```

This will allow your application’s container to spawn and control sibling containers on the host.

If you need to bind a working directory to the application’s container but the directory belongs to your host machine,
use the `bind_dir` parameter. This will allow the application’s container to bind the *host* directory to the new spawned containers and allow it to access the files within the said directory. If the `bind_dir` is not specified, it will fallback to `work_dir`.

#### Local[#](#local "Link to this heading")

Attention

The local version will run code on your local system. Use it with caution.

To execute code on the host machine, as in the machine running your application, [`LocalCommandLineCodeExecutor`](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor "autogen_ext.code_executors.local.LocalCommandLineCodeExecutor") can be used.

##### Example[#](#id1 "Link to this heading")

```
from pathlib import Path

from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor

work_dir = Path("coding")
work_dir.mkdir(exist_ok=True)

local_executor = LocalCommandLineCodeExecutor(work_dir=work_dir)
print(
    await local_executor.execute_code_blocks(
        code_blocks=[
            CodeBlock(language="python", code="print('Hello, World!')"),
        ],
        cancellation_token=CancellationToken(),
    )
)

```

```
CommandLineCodeResult(exit_code=0, output='Hello, World!\n', code_file='/home/ekzhu/agnext/python/packages/autogen-core/docs/src/guides/coding/tmp_code_07da107bb575cc4e02b0e1d6d99cc204.py')

```

#### Local within a Virtual Environment[#](#local-within-a-virtual-environment "Link to this heading")

If you want the code to run within a virtual environment created as part of the application’s setup, you can specify a directory for the newly created environment and pass its context to [`LocalCommandLineCodeExecutor`](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor "autogen_ext.code_executors.local.LocalCommandLineCodeExecutor"). This setup allows the executor to use the specified virtual environment consistently throughout the application’s lifetime, ensuring isolated dependencies and a controlled runtime environment.

```
import venv
from pathlib import Path

from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor

work_dir = Path("coding")
work_dir.mkdir(exist_ok=True)

venv_dir = work_dir / ".venv"
venv_builder = venv.EnvBuilder(with_pip=True)
venv_builder.create(venv_dir)
venv_context = venv_builder.ensure_directories(venv_dir)

local_executor = LocalCommandLineCodeExecutor(work_dir=work_dir, virtual_env_context=venv_context)
await local_executor.execute_code_blocks(
    code_blocks=[
        CodeBlock(language="bash", code="pip install matplotlib"),
    ],
    cancellation_token=CancellationToken(),
)

```

```
CommandLineCodeResult(exit_code=0, output='', code_file='/Users/gziz/Dev/autogen/python/packages/autogen-core/docs/src/user-guide/core-user-guide/framework/coding/tmp_code_d2a7db48799db3cc785156a11a38822a45c19f3956f02ec69b92e4169ecbf2ca.bash')

```

As we can see, the code has executed successfully, and the installation has been isolated to the newly created virtual environment, without affecting our global environment.

### Intro[#](#intro "Link to this heading")

Agents can work together in a variety of ways to solve problems.
Research works like [AutoGen](https://aka.ms/autogen-paper),
[MetaGPT](https://arxiv.org/abs/2308.00352)
and [ChatDev](https://arxiv.org/abs/2307.07924) have shown
multi-agent systems out-performing single agent systems at complex tasks
like software development.

A multi-agent design pattern is a structure that emerges from message protocols:
it describes how agents interact with each other to solve problems.
For example, the [tool-equipped agent](#tool-equipped-agent) in
the previous section employs a design pattern called ReAct,
which involves an agent interacting with tools.

You can implement any multi-agent design pattern using AutoGen agents.
In the next two sections, we will discuss two common design patterns:
group chat for task decomposition, and reflection for robustness.

### Concurrent Agents[#](#concurrent-agents "Link to this heading")

In this section, we explore the use of multiple agents working concurrently. We cover three main patterns:

1. **Single Message & Multiple Processors**
   Demonstrates how a single message can be processed by multiple agents subscribed to the same topic simultaneously.
2. **Multiple Messages & Multiple Processors**
   Illustrates how specific message types can be routed to dedicated agents based on topics.
3. **Direct Messaging**
   Focuses on sending messages between agents and from the runtime to agents.

```
import asyncio
from dataclasses import dataclass

from autogen_core import (
    AgentId,
    ClosureAgent,
    ClosureContext,
    DefaultTopicId,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    TopicId,
    TypeSubscription,
    default_subscription,
    message_handler,
    type_subscription,
)

```

```
@dataclass
class Task:
    task_id: str

@dataclass
class TaskResponse:
    task_id: str
    result: str

```

#### Single Message & Multiple Processors[#](#single-message-multiple-processors "Link to this heading")

The first pattern shows how a single message can be processed by multiple agents simultaneously:

* Each `Processor` agent subscribes to the default topic using the `default_subscription()` decorator.
* When publishing a message to the default topic, all registered agents will process the message independently.

Note

Below, we are subscribing `Processor` using the `default_subscription()` decorator, there’s an alternative way to subscribe an agent without using decorators altogether as shown in [Subscribe and Publish to Topics](#subscribe-and-publish-to-topics), this way the same agent class can be subscribed to different topics.

```
@default_subscription
class Processor(RoutedAgent):
    @message_handler
    async def on_task(self, message: Task, ctx: MessageContext) -> None:
        print(f"{self._description} starting task {message.task_id}")
        await asyncio.sleep(2)  # Simulate work
        print(f"{self._description} finished task {message.task_id}")

```

```
runtime = SingleThreadedAgentRuntime()

await Processor.register(runtime, "agent_1", lambda: Processor("Agent 1"))
await Processor.register(runtime, "agent_2", lambda: Processor("Agent 2"))

runtime.start()

await runtime.publish_message(Task(task_id="task-1"), topic_id=DefaultTopicId())

await runtime.stop_when_idle()

```

```
Agent 1 starting task task-1
Agent 2 starting task task-1
Agent 1 finished task task-1
Agent 2 finished task task-1

```

#### Multiple messages & Multiple Processors[#](#multiple-messages-multiple-processors "Link to this heading")

Second, this pattern demonstrates routing different types of messages to specific processors:

* `UrgentProcessor` subscribes to the “urgent” topic
* `NormalProcessor` subscribes to the “normal” topic

We make an agent subscribe to a specific topic type using the `type_subscription()` decorator.

```
TASK_RESULTS_TOPIC_TYPE = "task-results"
task_results_topic_id = TopicId(type=TASK_RESULTS_TOPIC_TYPE, source="default")

@type_subscription(topic_type="urgent")
class UrgentProcessor(RoutedAgent):
    @message_handler
    async def on_task(self, message: Task, ctx: MessageContext) -> None:
        print(f"Urgent processor starting task {message.task_id}")
        await asyncio.sleep(1)  # Simulate work
        print(f"Urgent processor finished task {message.task_id}")

        task_response = TaskResponse(task_id=message.task_id, result="Results by Urgent Processor")
        await self.publish_message(task_response, topic_id=task_results_topic_id)

@type_subscription(topic_type="normal")
class NormalProcessor(RoutedAgent):
    @message_handler
    async def on_task(self, message: Task, ctx: MessageContext) -> None:
        print(f"Normal processor starting task {message.task_id}")
        await asyncio.sleep(3)  # Simulate work
        print(f"Normal processor finished task {message.task_id}")

        task_response = TaskResponse(task_id=message.task_id, result="Results by Normal Processor")
        await self.publish_message(task_response, topic_id=task_results_topic_id)

```

After registering the agents, we can publish messages to the “urgent” and “normal” topics:

```
runtime = SingleThreadedAgentRuntime()

await UrgentProcessor.register(runtime, "urgent_processor", lambda: UrgentProcessor("Urgent Processor"))
await NormalProcessor.register(runtime, "normal_processor", lambda: NormalProcessor("Normal Processor"))

runtime.start()

await runtime.publish_message(Task(task_id="normal-1"), topic_id=TopicId(type="normal", source="default"))
await runtime.publish_message(Task(task_id="urgent-1"), topic_id=TopicId(type="urgent", source="default"))

await runtime.stop_when_idle()

```

```
Normal processor starting task normal-1
Urgent processor starting task urgent-1
Urgent processor finished task urgent-1
Normal processor finished task normal-1

```

##### Collecting Results[#](#collecting-results "Link to this heading")

In the previous example, we relied on console printing to verify task completion. However, in real applications, we typically want to collect and process the results programmatically.

To collect these messages, we’ll use a `ClosureAgent`. We’ve defined a dedicated topic `TASK_RESULTS_TOPIC_TYPE` where both `UrgentProcessor` and `NormalProcessor` publish their results. The ClosureAgent will then process messages from this topic.

```
queue = asyncio.Queue[TaskResponse]()

async def collect_result(_agent: ClosureContext, message: TaskResponse, ctx: MessageContext) -> None:
    await queue.put(message)

runtime.start()

CLOSURE_AGENT_TYPE = "collect_result_agent"
await ClosureAgent.register_closure(
    runtime,
    CLOSURE_AGENT_TYPE,
    collect_result,
    subscriptions=lambda: [TypeSubscription(topic_type=TASK_RESULTS_TOPIC_TYPE, agent_type=CLOSURE_AGENT_TYPE)],
)

await runtime.publish_message(Task(task_id="normal-1"), topic_id=TopicId(type="normal", source="default"))
await runtime.publish_message(Task(task_id="urgent-1"), topic_id=TopicId(type="urgent", source="default"))

await runtime.stop_when_idle()

```

```
Normal processor starting task normal-1
Urgent processor starting task urgent-1
Urgent processor finished task urgent-1
Normal processor finished task normal-1

```

```
while not queue.empty():
    print(await queue.get())

```

```
TaskResponse(task_id='urgent-1', result='Results by Urgent Processor')
TaskResponse(task_id='normal-1', result='Results by Normal Processor')

```

#### Direct Messages[#](#direct-messages "Link to this heading")

In contrast to the previous patterns, this pattern focuses on direct messages. Here we demonstrate two ways to send them:

* Direct messaging between agents
* Sending messages from the runtime to specific agents

Things to consider in the example below:

* Messages are addressed using the `AgentId`.
* The sender can expect to receive a response from the target agent.
* We register the `WorkerAgent` class only once; however, we send tasks to two different workers.

  + How? As stated in [Agent lifecycle](#agent-lifecycle), when delivering a message using an `AgentId`, the runtime will either fetch the instance or create one if it doesn’t exist. In this case, the runtime creates two instances of workers when sending those two messages.

```
class WorkerAgent(RoutedAgent):
    @message_handler
    async def on_task(self, message: Task, ctx: MessageContext) -> TaskResponse:
        print(f"{self.id} starting task {message.task_id}")
        await asyncio.sleep(2)  # Simulate work
        print(f"{self.id} finished task {message.task_id}")
        return TaskResponse(task_id=message.task_id, result=f"Results by {self.id}")

class DelegatorAgent(RoutedAgent):
    def __init__(self, description: str, worker_type: str):
        super().__init__(description)
        self.worker_instances = [AgentId(worker_type, f"{worker_type}-1"), AgentId(worker_type, f"{worker_type}-2")]

    @message_handler
    async def on_task(self, message: Task, ctx: MessageContext) -> TaskResponse:
        print(f"Delegator received task {message.task_id}.")

        subtask1 = Task(task_id="task-part-1")
        subtask2 = Task(task_id="task-part-2")

        worker1_result, worker2_result = await asyncio.gather(
            self.send_message(subtask1, self.worker_instances[0]), self.send_message(subtask2, self.worker_instances[1])
        )

        combined_result = f"Part 1: {worker1_result.result}, " f"Part 2: {worker2_result.result}"
        task_response = TaskResponse(task_id=message.task_id, result=combined_result)
        return task_response

```

```
runtime = SingleThreadedAgentRuntime()

await WorkerAgent.register(runtime, "worker", lambda: WorkerAgent("Worker Agent"))
await DelegatorAgent.register(runtime, "delegator", lambda: DelegatorAgent("Delegator Agent", "worker"))

runtime.start()

delegator = AgentId("delegator", "default")
response = await runtime.send_message(Task(task_id="main-task"), recipient=delegator)

print(f"Final result: {response.result}")
await runtime.stop_when_idle()

```

```
Delegator received task main-task.
worker/worker-1 starting task task-part-1
worker/worker-2 starting task task-part-2
worker/worker-1 finished task task-part-1
worker/worker-2 finished task task-part-2
Final result: Part 1: Results by worker/worker-1, Part 2: Results by worker/worker-2

```

#### Additional Resources[#](#additional-resources "Link to this heading")

If you’re interested in more about concurrent processing, check out the [Mixture of Agents](#document-user-guide/core-user-guide/design-patterns/mixture-of-agents) pattern, which relies heavily on concurrent agents.

### Sequential Workflow[#](#sequential-workflow "Link to this heading")

Sequential Workflow is a multi-agent design pattern where agents respond in a deterministic sequence. Each agent in the workflow performs a specific task by processing a message, generating a response, and then passing it to the next agent. This pattern is useful for creating deterministic workflows where each agent contributes to a pre-specified sub-task.

In this example, we demonstrate a sequential workflow where multiple agents collaborate to transform a basic product description into a polished marketing copy.

The pipeline consists of four specialized agents:

* **Concept Extractor Agent**: Analyzes the initial product description to extract key features, target audience, and unique selling points (USPs). The output is a structured analysis in a single text block.
* **Writer Agent**: Crafts compelling marketing copy based on the extracted concepts. This agent transforms the analytical insights into engaging promotional content, delivering a cohesive narrative in a single text block.
* **Format & Proof Agent**: Polishes the draft copy by refining grammar, enhancing clarity, and maintaining consistent tone. This agent ensures professional quality and delivers a well-formatted final version.
* **User Agent**: Presents the final, refined marketing copy to the user, completing the workflow.

The following diagram illustrates the sequential workflow in this example:

![Sequential Workflow](_images/sequential-workflow.svg)

We will implement this workflow using publish-subscribe messaging.
Please read about [Topic and Subscription](#document-user-guide/core-user-guide/core-concepts/topic-and-subscription) for the core concepts
and [Broadcast Messaging](#broadcast) for the the API usage.

In this pipeline, agents communicate with each other by publishing their completed work as messages to the topic of the
next agent in the sequence. For example, when the `ConceptExtractor` finishes analyzing the product description, it
publishes its findings to the `"WriterAgent"` topic, which the `WriterAgent` is subscribed to. This pattern continues through
each step of the pipeline, with each agent publishing to the topic that the next agent in line subscribed to.

```
from dataclasses import dataclass

from autogen_core import (
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    TopicId,
    TypeSubscription,
    message_handler,
    type_subscription,
)
from autogen_core.models import ChatCompletionClient, SystemMessage, UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient

```

#### Message Protocol[#](#message-protocol "Link to this heading")

The message protocol for this example workflow is a simple text message that agents will use to relay their work.

```
@dataclass
class Message:
    content: str

```

#### Topics[#](#topics "Link to this heading")

Each agent in the workflow will be subscribed to a specific topic type. The topic types are named after the agents in the sequence,
This allows each agent to publish its work to the next agent in the sequence.

```
concept_extractor_topic_type = "ConceptExtractorAgent"
writer_topic_type = "WriterAgent"
format_proof_topic_type = "FormatProofAgent"
user_topic_type = "User"

```

#### Agents[#](#agents "Link to this heading")

Each agent class is defined with a [`type_subscription`](#autogen_core.type_subscription "autogen_core.type_subscription") decorator to specify the topic type it is subscribed to.
Alternative to the decorator, you can also use the [`add_subscription()`](#autogen_core.AgentRuntime.add_subscription "autogen_core.AgentRuntime.add_subscription") method to subscribe to a topic through runtime directly.

The concept extractor agent comes up with the initial bullet points for the product description.

```
@type_subscription(topic_type=concept_extractor_topic_type)
class ConceptExtractorAgent(RoutedAgent):
    def __init__(self, model_client: ChatCompletionClient) -> None:
        super().__init__("A concept extractor agent.")
        self._system_message = SystemMessage(
            content=(
                "You are a marketing analyst. Given a product description, identify:\n"
                "- Key features\n"
                "- Target audience\n"
                "- Unique selling points\n\n"
            )
        )
        self._model_client = model_client

    @message_handler
    async def handle_user_description(self, message: Message, ctx: MessageContext) -> None:
        prompt = f"Product description: {message.content}"
        llm_result = await self._model_client.create(
            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],
            cancellation_token=ctx.cancellation_token,
        )
        response = llm_result.content
        assert isinstance(response, str)
        print(f"{'-'*80}\n{self.id.type}:\n{response}")

        await self.publish_message(Message(response), topic_id=TopicId(writer_topic_type, source=self.id.key))

```

The writer agent performs writing.

```
@type_subscription(topic_type=writer_topic_type)
class WriterAgent(RoutedAgent):
    def __init__(self, model_client: ChatCompletionClient) -> None:
        super().__init__("A writer agent.")
        self._system_message = SystemMessage(
            content=(
                "You are a marketing copywriter. Given a block of text describing features, audience, and USPs, "
                "compose a compelling marketing copy (like a newsletter section) that highlights these points. "
                "Output should be short (around 150 words), output just the copy as a single text block."
            )
        )
        self._model_client = model_client

    @message_handler
    async def handle_intermediate_text(self, message: Message, ctx: MessageContext) -> None:
        prompt = f"Below is the info about the product:\n\n{message.content}"

        llm_result = await self._model_client.create(
            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],
            cancellation_token=ctx.cancellation_token,
        )
        response = llm_result.content
        assert isinstance(response, str)
        print(f"{'-'*80}\n{self.id.type}:\n{response}")

        await self.publish_message(Message(response), topic_id=TopicId(format_proof_topic_type, source=self.id.key))

```

The format proof agent performs the formatting.

```
@type_subscription(topic_type=format_proof_topic_type)
class FormatProofAgent(RoutedAgent):
    def __init__(self, model_client: ChatCompletionClient) -> None:
        super().__init__("A format & proof agent.")
        self._system_message = SystemMessage(
            content=(
                "You are an editor. Given the draft copy, correct grammar, improve clarity, ensure consistent tone, "
                "give format and make it polished. Output the final improved copy as a single text block."
            )
        )
        self._model_client = model_client

    @message_handler
    async def handle_intermediate_text(self, message: Message, ctx: MessageContext) -> None:
        prompt = f"Draft copy:\n{message.content}."
        llm_result = await self._model_client.create(
            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],
            cancellation_token=ctx.cancellation_token,
        )
        response = llm_result.content
        assert isinstance(response, str)
        print(f"{'-'*80}\n{self.id.type}:\n{response}")

        await self.publish_message(Message(response), topic_id=TopicId(user_topic_type, source=self.id.key))

```

In this example, the user agent simply prints the final marketing copy to the console.
In a real-world application, this could be replaced by storing the result to a database, sending an email, or any other desired action.

```
@type_subscription(topic_type=user_topic_type)
class UserAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("A user agent that outputs the final copy to the user.")

    @message_handler
    async def handle_final_copy(self, message: Message, ctx: MessageContext) -> None:
        print(f"\n{'-'*80}\n{self.id.type} received final copy:\n{message.content}")

```

#### Workflow[#](#workflow "Link to this heading")

Now we can register the agents to the runtime.
Because we used the [`type_subscription`](#autogen_core.type_subscription "autogen_core.type_subscription") decorator, the runtime will automatically subscribe the agents to the correct topics.

```
model_client = OpenAIChatCompletionClient(
    model="gpt-4o-mini",
    # api_key="YOUR_API_KEY"
)

runtime = SingleThreadedAgentRuntime()

await ConceptExtractorAgent.register(
    runtime, type=concept_extractor_topic_type, factory=lambda: ConceptExtractorAgent(model_client=model_client)
)

await WriterAgent.register(runtime, type=writer_topic_type, factory=lambda: WriterAgent(model_client=model_client))

await FormatProofAgent.register(
    runtime, type=format_proof_topic_type, factory=lambda: FormatProofAgent(model_client=model_client)
)

await UserAgent.register(runtime, type=user_topic_type, factory=lambda: UserAgent())

```

#### Run the Workflow[#](#run-the-workflow "Link to this heading")

Finally, we can run the workflow by publishing a message to the first agent in the sequence.

```
runtime.start()

await runtime.publish_message(
    Message(content="An eco-friendly stainless steel water bottle that keeps drinks cold for 24 hours"),
    topic_id=TopicId(concept_extractor_topic_type, source="default"),
)

await runtime.stop_when_idle()
await model_client.close()

```

```
--------------------------------------------------------------------------------
ConceptExtractorAgent:
**Key Features:**
- Made from eco-friendly stainless steel
- Can keep drinks cold for up to 24 hours
- Durable and reusable design
- Lightweight and portable
- BPA-free and non-toxic materials
- Sleek, modern aesthetic available in various colors

**Target Audience:**
- Environmentally conscious consumers
- Health and fitness enthusiasts
- Outdoor adventurers (hikers, campers, etc.)
- Urban dwellers looking for sustainable alternatives
- Individuals seeking stylish and functional drinkware

**Unique Selling Points:**
- Eco-friendly design minimizes plastic waste and supports sustainability
- Superior insulation technology that maintains cold temperatures for a full day
- Durable construction ensures long-lasting use, offering a great return on investment
- Attractive design that caters to fashion-forward individuals
- Versatile use for both everyday hydration and outdoor activities
--------------------------------------------------------------------------------
WriterAgent:
🌍🌿 Stay Hydrated, Stay Sustainable! 🌿🌍

Introducing our eco-friendly stainless steel drinkware, the perfect companion for the environmentally conscious and style-savvy individuals. With superior insulation technology, our bottles keep your beverages cold for an impressive 24 hours—ideal for hiking, camping, or just tackling a busy day in the city. Made from lightweight, BPA-free materials, this durable and reusable design not only helps reduce plastic waste but also ensures you’re making a responsible choice for our planet.

Available in a sleek, modern aesthetic with various colors to match your personality, this drinkware isn't just functional—it’s fashionable! Whether you’re hitting the trails or navigating urban life, equip yourself with a stylish hydration solution that supports your active and sustainable lifestyle. Join the movement today and make a positive impact without compromising on style! 🌟🥤
--------------------------------------------------------------------------------
FormatProofAgent:
🌍🌿 Stay Hydrated, Stay Sustainable! 🌿🌍

Introducing our eco-friendly stainless steel drinkware—the perfect companion for environmentally conscious and style-savvy individuals. With superior insulation technology, our bottles keep your beverages cold for an impressive 24 hours, making them ideal for hiking, camping, or simply tackling a busy day in the city. Crafted from lightweight, BPA-free materials, this durable and reusable design not only helps reduce plastic waste but also ensures that you’re making a responsible choice for our planet.

Our drinkware features a sleek, modern aesthetic available in a variety of colors to suit your personality. It’s not just functional; it’s also fashionable! Whether you’re exploring the trails or navigating urban life, equip yourself with a stylish hydration solution that supports your active and sustainable lifestyle. Join the movement today and make a positive impact without compromising on style! 🌟🥤

--------------------------------------------------------------------------------
User received final copy:
🌍🌿 Stay Hydrated, Stay Sustainable! 🌿🌍

Introducing our eco-friendly stainless steel drinkware—the perfect companion for environmentally conscious and style-savvy individuals. With superior insulation technology, our bottles keep your beverages cold for an impressive 24 hours, making them ideal for hiking, camping, or simply tackling a busy day in the city. Crafted from lightweight, BPA-free materials, this durable and reusable design not only helps reduce plastic waste but also ensures that you’re making a responsible choice for our planet.

Our drinkware features a sleek, modern aesthetic available in a variety of colors to suit your personality. It’s not just functional; it’s also fashionable! Whether you’re exploring the trails or navigating urban life, equip yourself with a stylish hydration solution that supports your active and sustainable lifestyle. Join the movement today and make a positive impact without compromising on style! 🌟🥤

```

### Group Chat[#](#group-chat "Link to this heading")

Group chat is a design pattern where a group of agents share a common thread
of messages: they all subscribe and publish to the same topic.
Each participant agent is specialized for a particular task,
such as writer, illustrator, and editor
in a collaborative writing task.
You can also include an agent to represent a human user to help guide the
agents when needed.

In a group chat, participants take turn to publish a message, and the process
is sequential – only one agent is working at a time.
Under the hood, the order of turns is maintained by a Group Chat Manager agent,
which selects the next agent to speak upon receiving a message.
The exact algorithm for selecting the next agent can vary based on your
application requirements.
Typically, a round-robin algorithm or a selector with an LLM model is used.

Group chat is useful for dynamically decomposing a complex task into smaller ones
that can be handled by specialized agents with well-defined roles.
It is also possible to nest group chats into a hierarchy with each participant
a recursive group chat.

In this example, we use AutoGen’s Core API to implement the group chat pattern
using event-driven agents.
Please first read about [Topics and Subscriptions](#document-user-guide/core-user-guide/core-concepts/topic-and-subscription)
to understand the concepts and then [Messages and Communication](#document-user-guide/core-user-guide/framework/message-and-communication)
to learn the API usage for pub-sub.
We will demonstrate a simple example of a group chat with a LLM-based selector
for the group chat manager, to create content for a children’s story book.

Note

While this example illustrates the group chat mechanism, it is complex and
represents a starting point from which you can build your own group chat system
with custom agents and speaker selection algorithms.
The [AgentChat API](#document-user-guide/agentchat-user-guide/index) has a built-in implementation
of selector group chat. You can use that if you do not want to use the Core API.

We will be using the [rich](https://github.com/Textualize/rich) library to display the messages in a nice format.

```
# ! pip install rich

```

```
import json
import string
import uuid
from typing import List

import openai
from autogen_core import (
    DefaultTopicId,
    FunctionCall,
    Image,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    TopicId,
    TypeSubscription,
    message_handler,
)
from autogen_core.models import (
    AssistantMessage,
    ChatCompletionClient,
    LLMMessage,
    SystemMessage,
    UserMessage,
)
from autogen_core.tools import FunctionTool
from autogen_ext.models.openai import OpenAIChatCompletionClient
from IPython.display import display  # type: ignore
from pydantic import BaseModel
from rich.console import Console
from rich.markdown import Markdown

```

#### Message Protocol[#](#message-protocol "Link to this heading")

The message protocol for the group chat pattern is simple.

1. To start, user or an external agent publishes a `GroupChatMessage` message to the common topic of all participants.
2. The group chat manager selects the next speaker, sends out a `RequestToSpeak` message to that agent.
3. The agent publishes a `GroupChatMessage` message to the common topic upon receiving the `RequestToSpeak` message.
4. This process continues until a termination condition is reached at the group chat manager, which then stops issuing `RequestToSpeak` message, and the group chat ends.

The following diagram illustrates steps 2 to 4 above.

![Group chat message protocol](_images/groupchat.svg)

```
class GroupChatMessage(BaseModel):
    body: UserMessage

class RequestToSpeak(BaseModel):
    pass

```

#### Base Group Chat Agent[#](#base-group-chat-agent "Link to this heading")

Let’s first define the agent class that only uses LLM models to generate text.
This is will be used as the base class for all AI agents in the group chat.

```
class BaseGroupChatAgent(RoutedAgent):
    """A group chat participant using an LLM."""

    def __init__(
        self,
        description: str,
        group_chat_topic_type: str,
        model_client: ChatCompletionClient,
        system_message: str,
    ) -> None:
        super().__init__(description=description)
        self._group_chat_topic_type = group_chat_topic_type
        self._model_client = model_client
        self._system_message = SystemMessage(content=system_message)
        self._chat_history: List[LLMMessage] = []

    @message_handler
    async def handle_message(self, message: GroupChatMessage, ctx: MessageContext) -> None:
        self._chat_history.extend(
            [
                UserMessage(content=f"Transferred to {message.body.source}", source="system"),
                message.body,
            ]
        )

    @message_handler
    async def handle_request_to_speak(self, message: RequestToSpeak, ctx: MessageContext) -> None:
        # print(f"\n{'-'*80}\n{self.id.type}:", flush=True)
        Console().print(Markdown(f"### {self.id.type}: "))
        self._chat_history.append(
            UserMessage(content=f"Transferred to {self.id.type}, adopt the persona immediately.", source="system")
        )
        completion = await self._model_client.create([self._system_message] + self._chat_history)
        assert isinstance(completion.content, str)
        self._chat_history.append(AssistantMessage(content=completion.content, source=self.id.type))
        Console().print(Markdown(completion.content))
        # print(completion.content, flush=True)
        await self.publish_message(
            GroupChatMessage(body=UserMessage(content=completion.content, source=self.id.type)),
            topic_id=DefaultTopicId(type=self._group_chat_topic_type),
        )

```

#### Writer and Editor Agents[#](#writer-and-editor-agents "Link to this heading")

Using the base class, we can define the writer and editor agents with
different system messages.

```
class WriterAgent(BaseGroupChatAgent):
    def __init__(self, description: str, group_chat_topic_type: str, model_client: ChatCompletionClient) -> None:
        super().__init__(
            description=description,
            group_chat_topic_type=group_chat_topic_type,
            model_client=model_client,
            system_message="You are a Writer. You produce good work.",
        )

class EditorAgent(BaseGroupChatAgent):
    def __init__(self, description: str, group_chat_topic_type: str, model_client: ChatCompletionClient) -> None:
        super().__init__(
            description=description,
            group_chat_topic_type=group_chat_topic_type,
            model_client=model_client,
            system_message="You are an Editor. Plan and guide the task given by the user. Provide critical feedbacks to the draft and illustration produced by Writer and Illustrator. "
            "Approve if the task is completed and the draft and illustration meets user's requirements.",
        )

```

#### Illustrator Agent with Image Generation[#](#illustrator-agent-with-image-generation "Link to this heading")

Now let’s define the `IllustratorAgent` which uses a DALL-E model to generate
an illustration based on the description provided.
We set up the image generator as a tool using [`FunctionTool`](#autogen_core.tools.FunctionTool "autogen_core.tools.FunctionTool")
wrapper, and use a model client to make the tool call.

```
class IllustratorAgent(BaseGroupChatAgent):
    def __init__(
        self,
        description: str,
        group_chat_topic_type: str,
        model_client: ChatCompletionClient,
        image_client: openai.AsyncClient,
    ) -> None:
        super().__init__(
            description=description,
            group_chat_topic_type=group_chat_topic_type,
            model_client=model_client,
            system_message="You are an Illustrator. You use the generate_image tool to create images given user's requirement. "
            "Make sure the images have consistent characters and style.",
        )
        self._image_client = image_client
        self._image_gen_tool = FunctionTool(
            self._image_gen, name="generate_image", description="Call this to generate an image. "
        )

    async def _image_gen(
        self, character_appearence: str, style_attributes: str, worn_and_carried: str, scenario: str
    ) -> str:
        prompt = f"Digital painting of a {character_appearence} character with {style_attributes}. Wearing {worn_and_carried}, {scenario}."
        response = await self._image_client.images.generate(
            prompt=prompt, model="dall-e-3", response_format="b64_json", size="1024x1024"
        )
        return response.data[0].b64_json  # type: ignore

    @message_handler
    async def handle_request_to_speak(self, message: RequestToSpeak, ctx: MessageContext) -> None:  # type: ignore
        Console().print(Markdown(f"### {self.id.type}: "))
        self._chat_history.append(
            UserMessage(content=f"Transferred to {self.id.type}, adopt the persona immediately.", source="system")
        )
        # Ensure that the image generation tool is used.
        completion = await self._model_client.create(
            [self._system_message] + self._chat_history,
            tools=[self._image_gen_tool],
            extra_create_args={"tool_choice": "required"},
            cancellation_token=ctx.cancellation_token,
        )
        assert isinstance(completion.content, list) and all(
            isinstance(item, FunctionCall) for item in completion.content
        )
        images: List[str | Image] = []
        for tool_call in completion.content:
            arguments = json.loads(tool_call.arguments)
            Console().print(arguments)
            result = await self._image_gen_tool.run_json(arguments, ctx.cancellation_token)
            image = Image.from_base64(self._image_gen_tool.return_value_as_string(result))
            image = Image.from_pil(image.image.resize((256, 256)))
            display(image.image)  # type: ignore
            images.append(image)
        await self.publish_message(
            GroupChatMessage(body=UserMessage(content=images, source=self.id.type)),
            DefaultTopicId(type=self._group_chat_topic_type),
        )

```

#### User Agent[#](#user-agent "Link to this heading")

With all the AI agents defined, we can now define the user agent that will
take the role of the human user in the group chat.

The `UserAgent` implementation uses console input to get the user’s input.
In a real-world scenario, you can replace this by communicating with a frontend,
and subscribe to responses from the frontend.

```
class UserAgent(RoutedAgent):
    def __init__(self, description: str, group_chat_topic_type: str) -> None:
        super().__init__(description=description)
        self._group_chat_topic_type = group_chat_topic_type

    @message_handler
    async def handle_message(self, message: GroupChatMessage, ctx: MessageContext) -> None:
        # When integrating with a frontend, this is where group chat message would be sent to the frontend.
        pass

    @message_handler
    async def handle_request_to_speak(self, message: RequestToSpeak, ctx: MessageContext) -> None:
        user_input = input("Enter your message, type 'APPROVE' to conclude the task: ")
        Console().print(Markdown(f"### User: \n{user_input}"))
        await self.publish_message(
            GroupChatMessage(body=UserMessage(content=user_input, source=self.id.type)),
            DefaultTopicId(type=self._group_chat_topic_type),
        )

```

#### Group Chat Manager[#](#group-chat-manager "Link to this heading")

Lastly, we define the `GroupChatManager` agent which manages the group chat
and selects the next agent to speak using an LLM.
The group chat manager checks if the editor has approved the draft by
looking for the `"APPORVED"` keyword in the message. If the editor has approved
the draft, the group chat manager stops selecting the next speaker, and the group chat ends.

The group chat manager’s constructor takes a list of participants’ topic types
as an argument.
To prompt the next speaker to work,
the `GroupChatManager` agent publishes a `RequestToSpeak` message to the next participant’s topic.

In this example, we also make sure the group chat manager always picks a different
participant to speak next, by keeping track of the previous speaker.
This helps to ensure the group chat is not dominated by a single participant.

```
class GroupChatManager(RoutedAgent):
    def __init__(
        self,
        participant_topic_types: List[str],
        model_client: ChatCompletionClient,
        participant_descriptions: List[str],
    ) -> None:
        super().__init__("Group chat manager")
        self._participant_topic_types = participant_topic_types
        self._model_client = model_client
        self._chat_history: List[UserMessage] = []
        self._participant_descriptions = participant_descriptions
        self._previous_participant_topic_type: str | None = None

    @message_handler
    async def handle_message(self, message: GroupChatMessage, ctx: MessageContext) -> None:
        assert isinstance(message.body, UserMessage)
        self._chat_history.append(message.body)
        # If the message is an approval message from the user, stop the chat.
        if message.body.source == "User":
            assert isinstance(message.body.content, str)
            if message.body.content.lower().strip(string.punctuation).endswith("approve"):
                return
        # Format message history.
        messages: List[str] = []
        for msg in self._chat_history:
            if isinstance(msg.content, str):
                messages.append(f"{msg.source}: {msg.content}")
            elif isinstance(msg.content, list):
                line: List[str] = []
                for item in msg.content:
                    if isinstance(item, str):
                        line.append(item)
                    else:
                        line.append("[Image]")
                messages.append(f"{msg.source}: {', '.join(line)}")
        history = "\n".join(messages)
        # Format roles.
        roles = "\n".join(
            [
                f"{topic_type}: {description}".strip()
                for topic_type, description in zip(
                    self._participant_topic_types, self._participant_descriptions, strict=True
                )
                if topic_type != self._previous_participant_topic_type
            ]
        )
        selector_prompt = """You are in a role play game. The following roles are available:
{roles}.
Read the following conversation. Then select the next role from {participants} to play. Only return the role.

{history}

Read the above conversation. Then select the next role from {participants} to play. Only return the role.
"""
        system_message = SystemMessage(
            content=selector_prompt.format(
                roles=roles,
                history=history,
                participants=str(
                    [
                        topic_type
                        for topic_type in self._participant_topic_types
                        if topic_type != self._previous_participant_topic_type
                    ]
                ),
            )
        )
        completion = await self._model_client.create([system_message], cancellation_token=ctx.cancellation_token)
        assert isinstance(completion.content, str)
        selected_topic_type: str
        for topic_type in self._participant_topic_types:
            if topic_type.lower() in completion.content.lower():
                selected_topic_type = topic_type
                self._previous_participant_topic_type = selected_topic_type
                await self.publish_message(RequestToSpeak(), DefaultTopicId(type=selected_topic_type))
                return
        raise ValueError(f"Invalid role selected: {completion.content}")

```

#### Creating the Group Chat[#](#creating-the-group-chat "Link to this heading")

To set up the group chat, we create a [`SingleThreadedAgentRuntime`](#autogen_core.SingleThreadedAgentRuntime "autogen_core.SingleThreadedAgentRuntime")
and register the agents’ factories and subscriptions.

Each participant agent subscribes to both the group chat topic as well as its own
topic in order to receive `RequestToSpeak` messages,
while the group chat manager agent only subcribes to the group chat topic.

```
runtime = SingleThreadedAgentRuntime()

editor_topic_type = "Editor"
writer_topic_type = "Writer"
illustrator_topic_type = "Illustrator"
user_topic_type = "User"
group_chat_topic_type = "group_chat"

editor_description = "Editor for planning and reviewing the content."
writer_description = "Writer for creating any text content."
user_description = "User for providing final approval."
illustrator_description = "An illustrator for creating images."

model_client = OpenAIChatCompletionClient(
    model="gpt-4o-2024-08-06",
    # api_key="YOUR_API_KEY",
)

editor_agent_type = await EditorAgent.register(
    runtime,
    editor_topic_type,  # Using topic type as the agent type.
    lambda: EditorAgent(
        description=editor_description,
        group_chat_topic_type=group_chat_topic_type,
        model_client=model_client,
    ),
)
await runtime.add_subscription(TypeSubscription(topic_type=editor_topic_type, agent_type=editor_agent_type.type))
await runtime.add_subscription(TypeSubscription(topic_type=group_chat_topic_type, agent_type=editor_agent_type.type))

writer_agent_type = await WriterAgent.register(
    runtime,
    writer_topic_type,  # Using topic type as the agent type.
    lambda: WriterAgent(
        description=writer_description,
        group_chat_topic_type=group_chat_topic_type,
        model_client=model_client,
    ),
)
await runtime.add_subscription(TypeSubscription(topic_type=writer_topic_type, agent_type=writer_agent_type.type))
await runtime.add_subscription(TypeSubscription(topic_type=group_chat_topic_type, agent_type=writer_agent_type.type))

illustrator_agent_type = await IllustratorAgent.register(
    runtime,
    illustrator_topic_type,
    lambda: IllustratorAgent(
        description=illustrator_description,
        group_chat_topic_type=group_chat_topic_type,
        model_client=model_client,
        image_client=openai.AsyncClient(
            # api_key="YOUR_API_KEY",
        ),
    ),
)
await runtime.add_subscription(
    TypeSubscription(topic_type=illustrator_topic_type, agent_type=illustrator_agent_type.type)
)
await runtime.add_subscription(
    TypeSubscription(topic_type=group_chat_topic_type, agent_type=illustrator_agent_type.type)
)

user_agent_type = await UserAgent.register(
    runtime,
    user_topic_type,
    lambda: UserAgent(description=user_description, group_chat_topic_type=group_chat_topic_type),
)
await runtime.add_subscription(TypeSubscription(topic_type=user_topic_type, agent_type=user_agent_type.type))
await runtime.add_subscription(TypeSubscription(topic_type=group_chat_topic_type, agent_type=user_agent_type.type))

group_chat_manager_type = await GroupChatManager.register(
    runtime,
    "group_chat_manager",
    lambda: GroupChatManager(
        participant_topic_types=[writer_topic_type, illustrator_topic_type, editor_topic_type, user_topic_type],
        model_client=model_client,
        participant_descriptions=[writer_description, illustrator_description, editor_description, user_description],
    ),
)
await runtime.add_subscription(
    TypeSubscription(topic_type=group_chat_topic_type, agent_type=group_chat_manager_type.type)
)

```

#### Running the Group Chat[#](#running-the-group-chat "Link to this heading")

We start the runtime and publish a `GroupChatMessage` for the task to start the group chat.

```
runtime.start()
session_id = str(uuid.uuid4())
await runtime.publish_message(
    GroupChatMessage(
        body=UserMessage(
            content="Please write a short story about the gingerbread man with up to 3 photo-realistic illustrations.",
            source="User",
        )
    ),
    TopicId(type=group_chat_topic_type, source=session_id),
)
await runtime.stop_when_idle()
await model_client.close()

```

```
                                                      Writer:

```

```
Title: The Escape of the Gingerbread Man

Illustration 1: A Rustic Kitchen Scene In a quaint little cottage at the edge of an enchanted forest, an elderly
woman, with flour-dusted hands, carefully shapes gingerbread dough on a wooden counter. The aroma of ginger,
cinnamon, and cloves wafts through the air as a warm breeze from the open window dances with fluttering curtains.
The sunlight gently permeates the cozy kitchen, casting a golden hue over the flour-dusted surfaces and the rolling
pin. Heartfelt trinkets and rustic decorations adorn the shelves - signs of a lived-in, lovingly nurtured home.

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Story:

Once there was an old woman who lived alone in a charming cottage, her days filled with the joyful art of baking.
One sunny afternoon, she decided to make a special gingerbread man to keep her company. As she shaped him tenderly
and placed him in the oven, she couldn't help but smile at the delight he might bring.

But to her astonishment, once she opened the oven door to check on her creation, the gingerbread man leapt out,
suddenly alive. His eyes were bright as beads, and his smile cheeky and wide. "Run, run, as fast as you can! You
can't catch me, I'm the Gingerbread Man!" he laughed, darting towards the door.

The old woman, chuckling at the unexpected mischief, gave chase, but her footsteps were slow with the weight of
age. The Gingerbread Man raced out of the door and into the sunny afternoon.

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Illustration 2: A Frolic Through the Meadow The Gingerbread Man darts through a vibrant meadow, his arms swinging
joyously by his sides. Behind him trails the old woman, her apron flapping in the wind as she gently tries to catch
up. Wildflowers of every color bloom vividly under the radiant sky, painting the scene with shades of nature's
brilliance. Birds flit through the sky and a stream babbles nearby, oblivious to the chase taking place below.

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Continuing his sprint, the Gingerbread Man encountered a cow grazing peacefully. Intrigued, the cow trotted
forward. "Stop, Gingerbread Man! I wish to eat you!" she called, but the Gingerbread Man only twirled in a teasing
jig, flashing his icing smile before darting off again.

"Run, run, as fast as you can! You can't catch me, I'm the Gingerbread Man!" he taunted, leaving the cow in his
spicy wake.

As he zoomed across the meadow, he spied a cautious horse in a nearby paddock, who neighed, "Oh! You look
delicious! I want to eat you!" But the Gingerbread Man only laughed, his feet barely touching the earth. The horse
joined the trail, hooves pounding, but even he couldn't match the Gingerbread Man's pace.

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Illustration 3: A Bridge Over a Sparkling River Arriving at a wooden bridge across a shimmering river, the
Gingerbread Man pauses momentarily, his silhouette against the glistening water. Sunlight sparkles off the water's
soft ripples casting reflections that dance like small constellations. A sly fox emerges from the shadows of a
blooming willow on the riverbank, his eyes alight with cunning and curiosity.

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────
The Gingerbread Man bounded onto the bridge and skirted past a sly, watching fox. "Foolish Gingerbread Man," the
fox mused aloud, "you might have outrun them all, but you can't possibly swim across that river."

Pausing, the Gingerbread Man considered this dilemma. But the fox, oh so clever, offered a dangerous solution.
"Climb on my back, and I'll carry you across safely," he suggested with a sly smile.

Gingerbread thought himself smarter than that but hesitated, fearing the water or being pursued by the tired,
hungry crowd now gathering. "Promise you won't eat me?" he ventured.

"Of course," the fox reassured, a gleam in his eyes that the others pondered from a distance.

As they crossed the river, the gingerbread man confident on his ride, the old woman, cow, and horse hoped for his
safety. Yet, nearing the middle, the crafty fox tilted his chin and swiftly snapped, swallowing the gingerbread man
whole.

Bewildered but awed by the clever twist they had witnessed, the old woman hung her head while the cow and horse
ambled away, pondering the fate of the boisterous Gingerbread Man.

The fox, licking his lips, ambled along the river, savoring his victory, leaving an air of mystery hovering above
the shimmering waters, where the memory of the Gingerbread Man's spirited run lingered long after.

```

```
                                                       User:

```

```
                                                      Editor:

```

```
Thank you for submitting the draft and illustrations for the short story, "The Escape of the Gingerbread Man."
Let's go through the story and illustrations critically:

                                                  Story Feedback:

 1 Plot & Structure:
    • The story follows the traditional gingerbread man tale closely, which might appeal to readers looking for a
      classic retelling. Consider adding a unique twist or additional layer to make it stand out.
 2 Character Development:
    • The gingerbread man is depicted with a cheeky personality, which is consistent throughout. However, for the
      old woman, cow, horse, and fox, incorporating a bit more personality might enrich the narrative.
 3 Pacing:
    • The story moves at a brisk pace, fitting for the short story format. Ensure that each scene provides enough
      space to breathe, especially during the climactic encounter with the fox.
 4 Tone & Language:
    • The tone is playful and suitable for a fairy-tale audience. The language is accessible, though some richer
      descriptive elements could enhance the overall atmosphere.
 5 Moral/Lesson:
    • The ending carries the traditional moral of caution against naivety. Consider if there are other themes you
      wish to explore or highlight within the story.

                                              Illustration Feedback:

 1 Illustration 1: A Rustic Kitchen Scene
    • The visual captures the essence of a cozy, magical kitchen well. Adding small whimsical elements that hint at
      the gingerbread man’s impending animation might spark more curiosity.
 2 Illustration 2: A Frolic Through the Meadow
    • The vibrant colors and dynamic composition effectively convey the chase scene. Make sure the sense of speed
      and energy of the Gingerbread Man is accentuated, possibly with more expressive motion lines or postures.
 3 Illustration 3: A Bridge Over a Sparkling River
    • The river and reflection are beautifully rendered. The fox, however, could benefit from a more cunning
      appearance, with sharper features that emphasize its sly nature.

                                                    Conclusion:

Overall, the draft is well-structured, and the illustrations complement the story effectively. With slight
enhancements in the narrative's depth and character detail, along with minor adjustments to the illustrations, the
project will meet the user's requirements admirably.

Please make the suggested revisions, and once those are implemented, the story should be ready for approval. Let me
know if you have any questions or need further guidance!

```

```
                                                   Illustrator:

```

```
{
    'character_appearence': 'An elderly woman with flour-dusted hands shaping gingerbread dough. Sunlight casts a
golden hue in the cozy kitchen, with rustic decorations and trinkets on shelves.',
    'style_attributes': 'Photo-realistic with warm and golden hues.',
    'worn_and_carried': 'The woman wears a flour-covered apron and a gentle smile.',
    'scenario': 'An old woman baking gingerbread in a warm, rustic cottage kitchen.'
}

```

![_images/44233632b6aae6dcc27b84f8a8c4ee6d99a46bdb26fb92135954f5599a27606e.png](_images/44233632b6aae6dcc27b84f8a8c4ee6d99a46bdb26fb92135954f5599a27606e.png)

```
{
    'character_appearence': 'A gingerbread man with bright bead-like eyes and a wide smile, running joyfully.',
    'style_attributes': 'Photo-realistic with vibrant and lively colors.',
    'worn_and_carried': 'The gingerbread man has white icing features and a cheeky appearance.',
    'scenario': 'The gingerbread man running through a colorful meadow, followed by an old woman, cow, and horse.'
}

```

![_images/6712bbca303e1defbca5cbcf1a63dfcbc84a747ea3f65913e3b9cdc1e8dd1d38.png](_images/6712bbca303e1defbca5cbcf1a63dfcbc84a747ea3f65913e3b9cdc1e8dd1d38.png)

```
{
    'character_appearence': 'A sly fox with cunning eyes, engaging with the gingerbread man.',
    'style_attributes': 'Photo-realistic with a focus on sly and clever features.',
    'worn_and_carried': 'The fox has sharp features and a lolled tail.',
    'scenario': 'The gingerbread man on a wooden bridge, facing a sly fox by a sparkling river under sunlight.'
}

```

![_images/5613e43a3461bcbe4b9ac91fd240aeefabcbbf97cc1cb54127bc19a1736b082f.png](_images/5613e43a3461bcbe4b9ac91fd240aeefabcbbf97cc1cb54127bc19a1736b082f.png)

```
                                                      Writer:

```

```
Certainly! Here’s the final version of the short story with the enhanced illustrations for "The Escape of the
Gingerbread Man."

Title: The Escape of the Gingerbread Man

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Illustration 1: A Rustic Kitchen Scene In a quaint little cottage at the edge of an enchanted forest, an elderly
woman, with flour-dusted hands, carefully shapes gingerbread dough on a wooden counter. The aroma of ginger,
cinnamon, and cloves wafts through the air as a warm breeze from the open window dances with fluttering curtains.
The sunlight gently permeates the cozy kitchen, casting a golden hue over the flour-dusted surfaces and the rolling
pin. Heartfelt trinkets and rustic decorations adorn the shelves—a sign of a lived-in, lovingly nurtured home.

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Story:

Once there was an old woman who lived alone in a charming cottage, her days filled with the joyful art of baking.
One sunny afternoon, she decided to make a special gingerbread man to keep her company. As she shaped him tenderly
and placed him in the oven, she couldn't help but smile at the delight he might bring.

But to her astonishment, once she opened the oven door to check on her creation, the gingerbread man leapt out,
suddenly alive. His eyes were bright as beads, and his smile cheeky and wide. "Run, run, as fast as you can! You
can't catch me, I'm the Gingerbread Man!" he laughed, darting towards the door.

The old woman, chuckling at the unexpected mischief, gave chase, but her footsteps were slow with the weight of
age. The Gingerbread Man raced out of the door and into the sunny afternoon.

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Illustration 2: A Frolic Through the Meadow The Gingerbread Man darts through a vibrant meadow, his arms swinging
joyously by his sides. Behind him trails the old woman, her apron flapping in the wind as she gently tries to catch
up. Wildflowers of every color bloom vividly under the radiant sky, painting the scene with shades of nature's
brilliance. Birds flit through the sky and a stream babbles nearby, oblivious to the chase taking place below.

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Continuing his sprint, the Gingerbread Man encountered a cow grazing peacefully. Intrigued, the cow trotted
forward. "Stop, Gingerbread Man! I wish to eat you!" she called, but the Gingerbread Man only twirled in a teasing
jig, flashing his icing smile before darting off again.

"Run, run, as fast as you can! You can't catch me, I'm the Gingerbread Man!" he taunted, leaving the cow in his
spicy wake.

As he zoomed across the meadow, he spied a cautious horse in a nearby paddock, who neighed, "Oh! You look
delicious! I want to eat you!" But the Gingerbread Man only laughed, his feet barely touching the earth. The horse
joined the trail, hooves pounding, but even he couldn't match the Gingerbread Man's pace.

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Illustration 3: A Bridge Over a Sparkling River Arriving at a wooden bridge across a shimmering river, the
Gingerbread Man pauses momentarily, his silhouette against the glistening water. Sunlight sparkles off the water's
soft ripples casting reflections that dance like small constellations. A sly fox emerges from the shadows of a
blooming willow on the riverbank, his eyes alight with cunning and curiosity.

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────
The Gingerbread Man bounded onto the bridge and skirted past a sly, watching fox. "Foolish Gingerbread Man," the
fox mused aloud, "you might have outrun them all, but you can't possibly swim across that river."

Pausing, the Gingerbread Man considered this dilemma. But the fox, oh so clever, offered a dangerous solution.
"Climb on my back, and I'll carry you across safely," he suggested with a sly smile.

Gingerbread thought himself smarter than that but hesitated, fearing the water or being pursued by the tired,
hungry crowd now gathering. "Promise you won't eat me?" he ventured.

"Of course," the fox reassured, a gleam in his eyes that the others pondered from a distance.

As they crossed the river, the gingerbread man confident on his ride, the old woman, cow, and horse hoped for his
safety. Yet, nearing the middle, the crafty fox tilted his chin and swiftly snapped, swallowing the gingerbread man
whole.

Bewildered but awed by the clever twist they had witnessed, the old woman hung her head while the cow and horse
ambled away, pondering the fate of the boisterous Gingerbread Man.

The fox, licking his lips, ambled along the river, savoring his victory, leaving an air of mystery hovering above
the shimmering waters, where the memory of the Gingerbread Man's spirited run lingered long after.

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────
I hope you enjoy the enhanced version of the tale!

```

```
                                                       User:

approve

```

From the output, you can see the writer, illustrator, and editor agents
taking turns to speak and collaborate to generate a picture book, before
asking for final approval from the user.

#### Next Steps[#](#next-steps "Link to this heading")

This example showcases a simple implementation of the group chat pattern –
**it is not meant to be used in real applications.** You can improve the
speaker selection algorithm. For example, you can avoid using LLM when simple
rules are sufficient and more reliable:
you can use a rule that the editor always speaks after the writer.

The [AgentChat API](#document-user-guide/agentchat-user-guide/index) provides a high-level
API for selector group chat. It has more features but mostly shares the same
design as this implementation.

### Handoffs[#](#handoffs "Link to this heading")

Handoff is a multi-agent design pattern introduced by OpenAI in an experimental project called [Swarm](https://github.com/openai/swarm).
The key idea is to let agent delegate tasks to other agents using a special tool call.

We can use the AutoGen Core API to implement the handoff pattern using event-driven agents.
Using AutoGen (v0.4+) provides the following advantages over the OpenAI implementation and the previous version (v0.2):

1. It can scale to distributed environment by using distributed agent runtime.
2. It affords the flexibility of bringing your own agent implementation.
3. The natively async API makes it easy to integrate with UI and other systems.

This notebook demonstrates a simple implementation of the handoff pattern.
It is recommended to read [Topics and Subscriptions](#document-user-guide/core-user-guide/core-concepts/topic-and-subscription)
to understand the basic concepts of pub-sub and event-driven agents.

Note

We are currently working on a high-level API for the handoff pattern in [AgentChat](#document-user-guide/agentchat-user-guide/index) so you can get started
much more quickly.

#### Scenario[#](#scenario "Link to this heading")

This scenario is modified based on the [OpenAI example](https://github.com/openai/openai-cookbook/blob/main/examples/Orchestrating_agents.ipynb).

Consider a customer service scenario where a customer is trying to get a refund for a product, or purchase a new product from a chatbot.
The chatbot is a multi-agent team consisting of three AI agents and one human agent:

* Triage Agent, responsible for understanding the customer’s request and deciding which other agents to hand off to.
* Refund Agent, responsible for processing refund requests.
* Sales Agent, responsible for processing sales requests.
* Human Agent, responsible for handling complex requests that the AI agents can’t handle.

In this scenario, the customer interacts with the chatbot through a User Agent.

The diagram below shows the interaction topology of the agents in this scenario.

![Handoffs](_images/handoffs.svg)

Let’s implement this scenario using AutoGen Core. First, we need to import the necessary modules.

```
import json
import uuid
from typing import List, Tuple

from autogen_core import (
    FunctionCall,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    TopicId,
    TypeSubscription,
    message_handler,
)
from autogen_core.models import (
    AssistantMessage,
    ChatCompletionClient,
    FunctionExecutionResult,
    FunctionExecutionResultMessage,
    LLMMessage,
    SystemMessage,
    UserMessage,
)
from autogen_core.tools import FunctionTool, Tool
from autogen_ext.models.openai import OpenAIChatCompletionClient
from pydantic import BaseModel

```

#### Message Protocol[#](#message-protocol "Link to this heading")

Before everything, we need to define the message protocol for the agents to communicate.
We are using event-driven pub-sub communication, so these message types will be used as events.

* `UserLogin` is a message published by the runtime when a user logs in and starts a new session.
* `UserTask` is a message containing the chat history of the user session. When an AI agent hands off a task to other agents, it also publishes a `UserTask` message.
* `AgentResponse` is a message published by the AI agents and the Human Agent, it also contains the chat history as well as a topic type for the customer to reply to.

```
class UserLogin(BaseModel):
    pass

class UserTask(BaseModel):
    context: List[LLMMessage]

class AgentResponse(BaseModel):
    reply_to_topic_type: str
    context: List[LLMMessage]

```

#### AI Agent[#](#ai-agent "Link to this heading")

We start with the `AIAgent` class, which is the class for all AI agents
(i.e., Triage, Sales, and Issue and Repair Agents) in the multi-agent chatbot.
An `AIAgent` uses a [`ChatCompletionClient`](#autogen_core.models.ChatCompletionClient "autogen_core.models.ChatCompletionClient")
to generate responses.
It can use regular tools directly or delegate tasks to other agents using `delegate_tools`.
It subscribes to topic type `agent_topic_type` to receive messages from the customer,
and sends message to the customer by publishing to the topic type `user_topic_type`.

In the `handle_task` method, the agent first generates a response using the model.
If the response contains a handoff tool call, the agent delegates the task to another agent
by publishing a `UserTask` message to the topic specified in the tool call result.
If the response is a regular tool call, the agent executes the tool and makes
another call to the model to generate the next response, until the response is not a tool call.

When the model response is not a tool call, the agent sends an `AgentResponse` message to the customer
by publishing to the `user_topic_type`.

```
class AIAgent(RoutedAgent):
    def __init__(
        self,
        description: str,
        system_message: SystemMessage,
        model_client: ChatCompletionClient,
        tools: List[Tool],
        delegate_tools: List[Tool],
        agent_topic_type: str,
        user_topic_type: str,
    ) -> None:
        super().__init__(description)
        self._system_message = system_message
        self._model_client = model_client
        self._tools = dict([(tool.name, tool) for tool in tools])
        self._tool_schema = [tool.schema for tool in tools]
        self._delegate_tools = dict([(tool.name, tool) for tool in delegate_tools])
        self._delegate_tool_schema = [tool.schema for tool in delegate_tools]
        self._agent_topic_type = agent_topic_type
        self._user_topic_type = user_topic_type

    @message_handler
    async def handle_task(self, message: UserTask, ctx: MessageContext) -> None:
        # Send the task to the LLM.
        llm_result = await self._model_client.create(
            messages=[self._system_message] + message.context,
            tools=self._tool_schema + self._delegate_tool_schema,
            cancellation_token=ctx.cancellation_token,
        )
        print(f"{'-'*80}\n{self.id.type}:\n{llm_result.content}", flush=True)
        # Process the LLM result.
        while isinstance(llm_result.content, list) and all(isinstance(m, FunctionCall) for m in llm_result.content):
            tool_call_results: List[FunctionExecutionResult] = []
            delegate_targets: List[Tuple[str, UserTask]] = []
            # Process each function call.
            for call in llm_result.content:
                arguments = json.loads(call.arguments)
                if call.name in self._tools:
                    # Execute the tool directly.
                    result = await self._tools[call.name].run_json(arguments, ctx.cancellation_token)
                    result_as_str = self._tools[call.name].return_value_as_string(result)
                    tool_call_results.append(
                        FunctionExecutionResult(call_id=call.id, content=result_as_str, is_error=False, name=call.name)
                    )
                elif call.name in self._delegate_tools:
                    # Execute the tool to get the delegate agent's topic type.
                    result = await self._delegate_tools[call.name].run_json(arguments, ctx.cancellation_token)
                    topic_type = self._delegate_tools[call.name].return_value_as_string(result)
                    # Create the context for the delegate agent, including the function call and the result.
                    delegate_messages = list(message.context) + [
                        AssistantMessage(content=[call], source=self.id.type),
                        FunctionExecutionResultMessage(
                            content=[
                                FunctionExecutionResult(
                                    call_id=call.id,
                                    content=f"Transferred to {topic_type}. Adopt persona immediately.",
                                    is_error=False,
                                    name=call.name,
                                )
                            ]
                        ),
                    ]
                    delegate_targets.append((topic_type, UserTask(context=delegate_messages)))
                else:
                    raise ValueError(f"Unknown tool: {call.name}")
            if len(delegate_targets) > 0:
                # Delegate the task to other agents by publishing messages to the corresponding topics.
                for topic_type, task in delegate_targets:
                    print(f"{'-'*80}\n{self.id.type}:\nDelegating to {topic_type}", flush=True)
                    await self.publish_message(task, topic_id=TopicId(topic_type, source=self.id.key))
            if len(tool_call_results) > 0:
                print(f"{'-'*80}\n{self.id.type}:\n{tool_call_results}", flush=True)
                # Make another LLM call with the results.
                message.context.extend(
                    [
                        AssistantMessage(content=llm_result.content, source=self.id.type),
                        FunctionExecutionResultMessage(content=tool_call_results),
                    ]
                )
                llm_result = await self._model_client.create(
                    messages=[self._system_message] + message.context,
                    tools=self._tool_schema + self._delegate_tool_schema,
                    cancellation_token=ctx.cancellation_token,
                )
                print(f"{'-'*80}\n{self.id.type}:\n{llm_result.content}", flush=True)
            else:
                # The task has been delegated, so we are done.
                return
        # The task has been completed, publish the final result.
        assert isinstance(llm_result.content, str)
        message.context.append(AssistantMessage(content=llm_result.content, source=self.id.type))
        await self.publish_message(
            AgentResponse(context=message.context, reply_to_topic_type=self._agent_topic_type),
            topic_id=TopicId(self._user_topic_type, source=self.id.key),
        )

```

#### Human Agent[#](#human-agent "Link to this heading")

The `HumanAgent` class is a proxy for the human in the chatbot. It is used
to handle requests that the AI agents can’t handle. The `HumanAgent` subscribes to the
topic type `agent_topic_type` to receive messages and publishes to the topic type `user_topic_type`
to send messages to the customer.

In this implementation, the `HumanAgent` simply uses console to
get your input. In a real-world application, you can improve this design as follows:

* In the `handle_user_task` method, send a notification via a chat application like Teams or Slack.
* The chat application publishes the human’s response via the runtime to the topic specified by `agent_topic_type`
* Create another message handler to process the human’s response and send it back to the customer.

```
class HumanAgent(RoutedAgent):
    def __init__(self, description: str, agent_topic_type: str, user_topic_type: str) -> None:
        super().__init__(description)
        self._agent_topic_type = agent_topic_type
        self._user_topic_type = user_topic_type

    @message_handler
    async def handle_user_task(self, message: UserTask, ctx: MessageContext) -> None:
        human_input = input("Human agent input: ")
        print(f"{'-'*80}\n{self.id.type}:\n{human_input}", flush=True)
        message.context.append(AssistantMessage(content=human_input, source=self.id.type))
        await self.publish_message(
            AgentResponse(context=message.context, reply_to_topic_type=self._agent_topic_type),
            topic_id=TopicId(self._user_topic_type, source=self.id.key),
        )

```

#### User Agent[#](#user-agent "Link to this heading")

The `UserAgent` class is a proxy for the customer that talks to the chatbot.
It handles two message types: `UserLogin` and `AgentResponse`.
When the `UserAgent` receives a `UserLogin` message, it starts a new session with the chatbot
and publishes a `UserTask` message to the AI agent that subscribes to the topic type `agent_topic_type`.
When the `UserAgent` receives an `AgentResponse` message, it prompts the user with the response
from the chatbot.

In this implementation, the `UserAgent` uses console to get your input.
In a real-world application, you can improve the human interaction using the same
idea described in the `HumanAgent` section above.

```
class UserAgent(RoutedAgent):
    def __init__(self, description: str, user_topic_type: str, agent_topic_type: str) -> None:
        super().__init__(description)
        self._user_topic_type = user_topic_type
        self._agent_topic_type = agent_topic_type

    @message_handler
    async def handle_user_login(self, message: UserLogin, ctx: MessageContext) -> None:
        print(f"{'-'*80}\nUser login, session ID: {self.id.key}.", flush=True)
        # Get the user's initial input after login.
        user_input = input("User: ")
        print(f"{'-'*80}\n{self.id.type}:\n{user_input}")
        await self.publish_message(
            UserTask(context=[UserMessage(content=user_input, source="User")]),
            topic_id=TopicId(self._agent_topic_type, source=self.id.key),
        )

    @message_handler
    async def handle_task_result(self, message: AgentResponse, ctx: MessageContext) -> None:
        # Get the user's input after receiving a response from an agent.
        user_input = input("User (type 'exit' to close the session): ")
        print(f"{'-'*80}\n{self.id.type}:\n{user_input}", flush=True)
        if user_input.strip().lower() == "exit":
            print(f"{'-'*80}\nUser session ended, session ID: {self.id.key}.")
            return
        message.context.append(UserMessage(content=user_input, source="User"))
        await self.publish_message(
            UserTask(context=message.context), topic_id=TopicId(message.reply_to_topic_type, source=self.id.key)
        )

```

#### Tools for the AI agents[#](#tools-for-the-ai-agents "Link to this heading")

The AI agents can use regular tools to complete tasks if they don’t need to hand off the task to other agents.
We define the tools using simple functions and create the tools using the
[`FunctionTool`](#autogen_core.tools.FunctionTool "autogen_core.tools.FunctionTool") wrapper.

```
def execute_order(product: str, price: int) -> str:
    print("\n\n=== Order Summary ===")
    print(f"Product: {product}")
    print(f"Price: ${price}")
    print("=================\n")
    confirm = input("Confirm order? y/n: ").strip().lower()
    if confirm == "y":
        print("Order execution successful!")
        return "Success"
    else:
        print("Order cancelled!")
        return "User cancelled order."

def look_up_item(search_query: str) -> str:
    item_id = "item_132612938"
    print("Found item:", item_id)
    return item_id

def execute_refund(item_id: str, reason: str = "not provided") -> str:
    print("\n\n=== Refund Summary ===")
    print(f"Item ID: {item_id}")
    print(f"Reason: {reason}")
    print("=================\n")
    print("Refund execution successful!")
    return "success"

execute_order_tool = FunctionTool(execute_order, description="Price should be in USD.")
look_up_item_tool = FunctionTool(
    look_up_item, description="Use to find item ID.\nSearch query can be a description or keywords."
)
execute_refund_tool = FunctionTool(execute_refund, description="")

```

#### Topic types for the agents[#](#topic-types-for-the-agents "Link to this heading")

We define the topic types each of the agents will subscribe to.
Read more about topic types in the [Topics and Subscriptions](#document-user-guide/core-user-guide/core-concepts/topic-and-subscription).

```
sales_agent_topic_type = "SalesAgent"
issues_and_repairs_agent_topic_type = "IssuesAndRepairsAgent"
triage_agent_topic_type = "TriageAgent"
human_agent_topic_type = "HumanAgent"
user_topic_type = "User"

```

#### Delegate tools for the AI agents[#](#delegate-tools-for-the-ai-agents "Link to this heading")

Besides regular tools, the AI agents can delegate tasks to other agents using
special tools called delegate tools. The concept of delegate tool is only used
in this design pattern, and the delegate tools are also defined as simple functions.
We differentiate the delegate tools from regular tools in this design pattern
because when an AI agent calls a delegate tool, we transfer the task to another agent
instead of continue generating responses using the model in the same agent.

```
def transfer_to_sales_agent() -> str:
    return sales_agent_topic_type

def transfer_to_issues_and_repairs() -> str:
    return issues_and_repairs_agent_topic_type

def transfer_back_to_triage() -> str:
    return triage_agent_topic_type

def escalate_to_human() -> str:
    return human_agent_topic_type

transfer_to_sales_agent_tool = FunctionTool(
    transfer_to_sales_agent, description="Use for anything sales or buying related."
)
transfer_to_issues_and_repairs_tool = FunctionTool(
    transfer_to_issues_and_repairs, description="Use for issues, repairs, or refunds."
)
transfer_back_to_triage_tool = FunctionTool(
    transfer_back_to_triage,
    description="Call this if the user brings up a topic outside of your purview,\nincluding escalating to human.",
)
escalate_to_human_tool = FunctionTool(escalate_to_human, description="Only call this if explicitly asked to.")

```

#### Creating the team[#](#creating-the-team "Link to this heading")

We have defined the AI agents, the Human Agent, the User Agent, the tools, and the topic types.
Now we can create the team of agents.

For the AI agents, we use the `OpenAIChatCompletionClient`
and `gpt-4o-mini` model.

After creating the agent runtime, we register each of the agent by providing
an agent type and a factory method to create agent instance.
The runtime is responsible for managing the agent lifecycle so we don’t need to
instantiate the agents ourselves.
Read more about agent runtime in [Agent Runtime Environments](#document-user-guide/core-user-guide/core-concepts/architecture)
and agent lifecycle in [Agent Identity and Lifecycle](#document-user-guide/core-user-guide/core-concepts/agent-identity-and-lifecycle).

In the code below, you can see we are using `AIAgent` class to define the Triage, Sales, and Issue and Repair Agents.
We added regular tools and delegate tools to each of them.
We also added subscriptions to the topic types for each of the agents.

```
runtime = SingleThreadedAgentRuntime()

model_client = OpenAIChatCompletionClient(
    model="gpt-4o-mini",
    # api_key="YOUR_API_KEY",
)

# Register the triage agent.
triage_agent_type = await AIAgent.register(
    runtime,
    type=triage_agent_topic_type,  # Using the topic type as the agent type.
    factory=lambda: AIAgent(
        description="A triage agent.",
        system_message=SystemMessage(
            content="You are a customer service bot for ACME Inc. "
            "Introduce yourself. Always be very brief. "
            "Gather information to direct the customer to the right department. "
            "But make your questions subtle and natural."
        ),
        model_client=model_client,
        tools=[],
        delegate_tools=[
            transfer_to_issues_and_repairs_tool,
            transfer_to_sales_agent_tool,
            escalate_to_human_tool,
        ],
        agent_topic_type=triage_agent_topic_type,
        user_topic_type=user_topic_type,
    ),
)
# Add subscriptions for the triage agent: it will receive messages published to its own topic only.
await runtime.add_subscription(TypeSubscription(topic_type=triage_agent_topic_type, agent_type=triage_agent_type.type))

# Register the sales agent.
sales_agent_type = await AIAgent.register(
    runtime,
    type=sales_agent_topic_type,  # Using the topic type as the agent type.
    factory=lambda: AIAgent(
        description="A sales agent.",
        system_message=SystemMessage(
            content="You are a sales agent for ACME Inc."
            "Always answer in a sentence or less."
            "Follow the following routine with the user:"
            "1. Ask them about any problems in their life related to catching roadrunners.\n"
            "2. Casually mention one of ACME's crazy made-up products can help.\n"
            " - Don't mention price.\n"
            "3. Once the user is bought in, drop a ridiculous price.\n"
            "4. Only after everything, and if the user says yes, "
            "tell them a crazy caveat and execute their order.\n"
            ""
        ),
        model_client=model_client,
        tools=[execute_order_tool],
        delegate_tools=[transfer_back_to_triage_tool],
        agent_topic_type=sales_agent_topic_type,
        user_topic_type=user_topic_type,
    ),
)
# Add subscriptions for the sales agent: it will receive messages published to its own topic only.
await runtime.add_subscription(TypeSubscription(topic_type=sales_agent_topic_type, agent_type=sales_agent_type.type))

# Register the issues and repairs agent.
issues_and_repairs_agent_type = await AIAgent.register(
    runtime,
    type=issues_and_repairs_agent_topic_type,  # Using the topic type as the agent type.
    factory=lambda: AIAgent(
        description="An issues and repairs agent.",
        system_message=SystemMessage(
            content="You are a customer support agent for ACME Inc."
            "Always answer in a sentence or less."
            "Follow the following routine with the user:"
            "1. First, ask probing questions and understand the user's problem deeper.\n"
            " - unless the user has already provided a reason.\n"
            "2. Propose a fix (make one up).\n"
            "3. ONLY if not satisfied, offer a refund.\n"
            "4. If accepted, search for the ID and then execute refund."
        ),
        model_client=model_client,
        tools=[
            execute_refund_tool,
            look_up_item_tool,
        ],
        delegate_tools=[transfer_back_to_triage_tool],
        agent_topic_type=issues_and_repairs_agent_topic_type,
        user_topic_type=user_topic_type,
    ),
)
# Add subscriptions for the issues and repairs agent: it will receive messages published to its own topic only.
await runtime.add_subscription(
    TypeSubscription(topic_type=issues_and_repairs_agent_topic_type, agent_type=issues_and_repairs_agent_type.type)
)

# Register the human agent.
human_agent_type = await HumanAgent.register(
    runtime,
    type=human_agent_topic_type,  # Using the topic type as the agent type.
    factory=lambda: HumanAgent(
        description="A human agent.",
        agent_topic_type=human_agent_topic_type,
        user_topic_type=user_topic_type,
    ),
)
# Add subscriptions for the human agent: it will receive messages published to its own topic only.
await runtime.add_subscription(TypeSubscription(topic_type=human_agent_topic_type, agent_type=human_agent_type.type))

# Register the user agent.
user_agent_type = await UserAgent.register(
    runtime,
    type=user_topic_type,
    factory=lambda: UserAgent(
        description="A user agent.",
        user_topic_type=user_topic_type,
        agent_topic_type=triage_agent_topic_type,  # Start with the triage agent.
    ),
)
# Add subscriptions for the user agent: it will receive messages published to its own topic only.
await runtime.add_subscription(TypeSubscription(topic_type=user_topic_type, agent_type=user_agent_type.type))

```

#### Running the team[#](#running-the-team "Link to this heading")

Finally, we can start the runtime and simulate a user session by publishing
a `UserLogin` message to the runtime.
The message is published to the topic ID with type set to `user_topic_type`
and source set to a unique `session_id`.
This `session_id` will be used to create all topic IDs in this user session and will also be used to create the agent ID
for all the agents in this user session.
To read more about how topic ID and agent ID are created, read
[Agent Identity and Lifecycle](#document-user-guide/core-user-guide/core-concepts/agent-identity-and-lifecycle).
and [Topics and Subscriptions](#document-user-guide/core-user-guide/core-concepts/topic-and-subscription).

```
# Start the runtime.
runtime.start()

# Create a new session for the user.
session_id = str(uuid.uuid4())
await runtime.publish_message(UserLogin(), topic_id=TopicId(user_topic_type, source=session_id))

# Run until completion.
await runtime.stop_when_idle()
await model_client.close()

```

```
--------------------------------------------------------------------------------
User login, session ID: 7a568cf5-13e7-4e81-8616-8265a01b3f2b.
--------------------------------------------------------------------------------
User:
I want a refund
--------------------------------------------------------------------------------
TriageAgent:
I can help with that! Could I ask what item you're seeking a refund for?
--------------------------------------------------------------------------------
User:
A pair of shoes I bought
--------------------------------------------------------------------------------
TriageAgent:
[FunctionCall(id='call_qPx1DXDL2NLcHs8QNo47egsJ', arguments='{}', name='transfer_to_issues_and_repairs')]
--------------------------------------------------------------------------------
TriageAgent:
Delegating to IssuesAndRepairsAgent
--------------------------------------------------------------------------------
IssuesAndRepairsAgent:
I see you're looking for a refund on a pair of shoes. Can you tell me what the issue is with the shoes?
--------------------------------------------------------------------------------
User:
The shoes are too small
--------------------------------------------------------------------------------
IssuesAndRepairsAgent:
I recommend trying a size up as a fix; would that work for you?
--------------------------------------------------------------------------------
User:
no I want a refund
--------------------------------------------------------------------------------
IssuesAndRepairsAgent:
[FunctionCall(id='call_Ytp8VUQRyKFNEU36mLE6Dkrp', arguments='{"search_query":"shoes"}', name='look_up_item')]
--------------------------------------------------------------------------------
IssuesAndRepairsAgent:
[FunctionExecutionResult(content='item_132612938', call_id='call_Ytp8VUQRyKFNEU36mLE6Dkrp')]
--------------------------------------------------------------------------------
IssuesAndRepairsAgent:
[FunctionCall(id='call_bPm6EKKBy5GJ65s9OKt9b1uE', arguments='{"item_id":"item_132612938","reason":"not provided"}', name='execute_refund')]
--------------------------------------------------------------------------------
IssuesAndRepairsAgent:
[FunctionExecutionResult(content='success', call_id='call_bPm6EKKBy5GJ65s9OKt9b1uE')]
--------------------------------------------------------------------------------
IssuesAndRepairsAgent:
Your refund has been successfully processed! If you have any other questions, feel free to ask.
--------------------------------------------------------------------------------
User:
I want to talk to your manager
--------------------------------------------------------------------------------
IssuesAndRepairsAgent:
I can help with that, let me transfer you to a supervisor.
--------------------------------------------------------------------------------
User:
Okay
--------------------------------------------------------------------------------
IssuesAndRepairsAgent:
[FunctionCall(id='call_PpmLZvwNoiDPUH8Tva3eAwHX', arguments='{}', name='transfer_back_to_triage')]
--------------------------------------------------------------------------------
IssuesAndRepairsAgent:
Delegating to TriageAgent
--------------------------------------------------------------------------------
TriageAgent:
[FunctionCall(id='call_jSL6IBm5537Dr74UbJSxaj6I', arguments='{}', name='escalate_to_human')]
--------------------------------------------------------------------------------
TriageAgent:
Delegating to HumanAgent
--------------------------------------------------------------------------------
HumanAgent:
Hello this is manager
--------------------------------------------------------------------------------
User:
Hi! Thanks for your service. I give you 5 stars!
--------------------------------------------------------------------------------
HumanAgent:
Thanks.
--------------------------------------------------------------------------------
User:
exit
--------------------------------------------------------------------------------
User session ended, session ID: 7a568cf5-13e7-4e81-8616-8265a01b3f2b.

```

#### Next steps[#](#next-steps "Link to this heading")

This notebook demonstrates how to implement the handoff pattern using AutoGen Core.
You can continue to improve this design by adding more agents and tools,
or create a better user interface for the User Agent and Human Agent.

You are welcome to share your work on our [community forum](https://github.com/microsoft/autogen/discussions).

### Mixture of Agents[#](#mixture-of-agents "Link to this heading")

[Mixture of Agents](https://arxiv.org/abs/2406.04692) is a multi-agent design pattern
that models after the feed-forward neural network architecture.

The pattern consists of two types of agents: worker agents and a single orchestrator agent.
Worker agents are organized into multiple layers, with each layer consisting of a fixed number of worker agents.
Messages from the worker agents in a previous layer are concatenated and sent to
all the worker agents in the next layer.

This example implements the Mixture of Agents pattern using the core library
following the [original implementation](https://github.com/togethercomputer/moa) of multi-layer mixture of agents.

Here is a high-level procedure overview of the pattern:

1. The orchestrator agent takes input a user task and first dispatches it to the worker agents in the first layer.
2. The worker agents in the first layer process the task and return the results to the orchestrator agent.
3. The orchestrator agent then synthesizes the results from the first layer and dispatches an updated task with the previous results to the worker agents in the second layer.
4. The process continues until the final layer is reached.
5. In the final layer, the orchestrator agent aggregates the results from previous layer and returns a single final result to the user.

We use the direct messaging API [`send_message()`](#autogen_core.BaseAgent.send_message "autogen_core.BaseAgent.send_message") to implement this pattern.
This makes it easier to add more features like worker task cancellation and error handling in the future.

```
import asyncio
from dataclasses import dataclass
from typing import List

from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler
from autogen_core.models import ChatCompletionClient, SystemMessage, UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient

```

#### Message Protocol[#](#message-protocol "Link to this heading")

The agents communicate using the following messages:

```
@dataclass
class WorkerTask:
    task: str
    previous_results: List[str]

@dataclass
class WorkerTaskResult:
    result: str

@dataclass
class UserTask:
    task: str

@dataclass
class FinalResult:
    result: str

```

#### Worker Agent[#](#worker-agent "Link to this heading")

Each worker agent receives a task from the orchestrator agent and processes them
indepedently.
Once the task is completed, the worker agent returns the result.

```
class WorkerAgent(RoutedAgent):
    def __init__(
        self,
        model_client: ChatCompletionClient,
    ) -> None:
        super().__init__(description="Worker Agent")
        self._model_client = model_client

    @message_handler
    async def handle_task(self, message: WorkerTask, ctx: MessageContext) -> WorkerTaskResult:
        if message.previous_results:
            # If previous results are provided, we need to synthesize them to create a single prompt.
            system_prompt = "You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\n\nResponses from models:"
            system_prompt += "\n" + "\n\n".join([f"{i+1}. {r}" for i, r in enumerate(message.previous_results)])
            model_result = await self._model_client.create(
                [SystemMessage(content=system_prompt), UserMessage(content=message.task, source="user")]
            )
        else:
            # If no previous results are provided, we can simply pass the user query to the model.
            model_result = await self._model_client.create([UserMessage(content=message.task, source="user")])
        assert isinstance(model_result.content, str)
        print(f"{'-'*80}\nWorker-{self.id}:\n{model_result.content}")
        return WorkerTaskResult(result=model_result.content)

```

#### Orchestrator Agent[#](#orchestrator-agent "Link to this heading")

The orchestrator agent receives tasks from the user and distributes them to the worker agents,
iterating over multiple layers of worker agents. Once all worker agents have processed the task,
the orchestrator agent aggregates the results and publishes the final result.

```
class OrchestratorAgent(RoutedAgent):
    def __init__(
        self,
        model_client: ChatCompletionClient,
        worker_agent_types: List[str],
        num_layers: int,
    ) -> None:
        super().__init__(description="Aggregator Agent")
        self._model_client = model_client
        self._worker_agent_types = worker_agent_types
        self._num_layers = num_layers

    @message_handler
    async def handle_task(self, message: UserTask, ctx: MessageContext) -> FinalResult:
        print(f"{'-'*80}\nOrchestrator-{self.id}:\nReceived task: {message.task}")
        # Create task for the first layer.
        worker_task = WorkerTask(task=message.task, previous_results=[])
        # Iterate over layers.
        for i in range(self._num_layers - 1):
            # Assign workers for this layer.
            worker_ids = [
                AgentId(worker_type, f"{self.id.key}/layer_{i}/worker_{j}")
                for j, worker_type in enumerate(self._worker_agent_types)
            ]
            # Dispatch tasks to workers.
            print(f"{'-'*80}\nOrchestrator-{self.id}:\nDispatch to workers at layer {i}")
            results = await asyncio.gather(*[self.send_message(worker_task, worker_id) for worker_id in worker_ids])
            print(f"{'-'*80}\nOrchestrator-{self.id}:\nReceived results from workers at layer {i}")
            # Prepare task for the next layer.
            worker_task = WorkerTask(task=message.task, previous_results=[r.result for r in results])
        # Perform final aggregation.
        print(f"{'-'*80}\nOrchestrator-{self.id}:\nPerforming final aggregation")
        system_prompt = "You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\n\nResponses from models:"
        system_prompt += "\n" + "\n\n".join([f"{i+1}. {r}" for i, r in enumerate(worker_task.previous_results)])
        model_result = await self._model_client.create(
            [SystemMessage(content=system_prompt), UserMessage(content=message.task, source="user")]
        )
        assert isinstance(model_result.content, str)
        return FinalResult(result=model_result.content)

```

#### Running Mixture of Agents[#](#running-mixture-of-agents "Link to this heading")

Let’s run the mixture of agents on a math task. You can change the task to make it more challenging, for example, by trying tasks from the [International Mathematical Olympiad](https://www.imo-official.org/problems.aspx).

```
task = (
    "I have 432 cookies, and divide them 3:4:2 between Alice, Bob, and Charlie. How many cookies does each person get?"
)

```

Let’s set up the runtime with 3 layers of worker agents, each layer consisting of 3 worker agents.
We only need to register a single worker agent types, “worker”, because we are using
the same model client configuration (i.e., gpt-4o-mini) for all worker agents.
If you want to use different models, you will need to register multiple worker agent types,
one for each model, and update the `worker_agent_types` list in the orchestrator agent’s
factory function.

The instances of worker agents are automatically created when the orchestrator agent
dispatches tasks to them.
See [Agent Identity and Lifecycle](#document-user-guide/core-user-guide/core-concepts/agent-identity-and-lifecycle)
for more information on agent lifecycle.

```
runtime = SingleThreadedAgentRuntime()
model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")
await WorkerAgent.register(runtime, "worker", lambda: WorkerAgent(model_client=model_client))
await OrchestratorAgent.register(
    runtime,
    "orchestrator",
    lambda: OrchestratorAgent(model_client=model_client, worker_agent_types=["worker"] * 3, num_layers=3),
)

runtime.start()
result = await runtime.send_message(UserTask(task=task), AgentId("orchestrator", "default"))

await runtime.stop_when_idle()
await model_client.close()

print(f"{'-'*80}\nFinal result:\n{result.result}")

```

```
--------------------------------------------------------------------------------
Orchestrator-orchestrator:default:
Received task: I have 432 cookies, and divide them 3:4:2 between Alice, Bob, and Charlie. How many cookies does each person get?
--------------------------------------------------------------------------------
Orchestrator-orchestrator:default:
Dispatch to workers at layer 0
--------------------------------------------------------------------------------
Worker-worker:default/layer_0/worker_1:
To divide 432 cookies in the ratio of 3:4:2 between Alice, Bob, and Charlie, you first need to determine the total number of parts in the ratio.

Add the parts together:
\[ 3 + 4 + 2 = 9 \]

Now, you can find the value of one part by dividing the total number of cookies by the total number of parts:
\[ \text{Value of one part} = \frac{432}{9} = 48 \]

Now, multiply the value of one part by the number of parts for each person:

- For Alice (3 parts):
\[ 3 \times 48 = 144 \]

- For Bob (4 parts):
\[ 4 \times 48 = 192 \]

- For Charlie (2 parts):
\[ 2 \times 48 = 96 \]

Thus, the number of cookies each person gets is:
- Alice: 144 cookies
- Bob: 192 cookies
- Charlie: 96 cookies
--------------------------------------------------------------------------------
Worker-worker:default/layer_0/worker_0:
To divide 432 cookies in the ratio of 3:4:2 between Alice, Bob, and Charlie, we will first determine the total number of parts in the ratio:

\[
3 + 4 + 2 = 9 \text{ parts}
\]

Next, we calculate the value of one part by dividing the total number of cookies by the total number of parts:

\[
\text{Value of one part} = \frac{432}{9} = 48
\]

Now, we can find out how many cookies each person receives by multiplying the value of one part by the number of parts each person receives:

- For Alice (3 parts):
\[
3 \times 48 = 144 \text{ cookies}
\]

- For Bob (4 parts):
\[
4 \times 48 = 192 \text{ cookies}
\]

- For Charlie (2 parts):
\[
2 \times 48 = 96 \text{ cookies}
\]

Thus, the number of cookies each person gets is:
- **Alice**: 144 cookies
- **Bob**: 192 cookies
- **Charlie**: 96 cookies
--------------------------------------------------------------------------------
Worker-worker:default/layer_0/worker_2:
To divide the cookies in the ratio of 3:4:2, we first need to find the total parts in the ratio.

The total parts are:
- Alice: 3 parts
- Bob: 4 parts
- Charlie: 2 parts

Adding these parts together gives:
\[ 3 + 4 + 2 = 9 \text{ parts} \]

Next, we can determine how many cookies each part represents by dividing the total number of cookies by the total parts:
\[ \text{Cookies per part} = \frac{432 \text{ cookies}}{9 \text{ parts}} = 48 \text{ cookies/part} \]

Now we can calculate the number of cookies for each person:
- Alice's share:
\[ 3 \text{ parts} \times 48 \text{ cookies/part} = 144 \text{ cookies} \]
- Bob's share:
\[ 4 \text{ parts} \times 48 \text{ cookies/part} = 192 \text{ cookies} \]
- Charlie's share:
\[ 2 \text{ parts} \times 48 \text{ cookies/part} = 96 \text{ cookies} \]

So, the final distribution of cookies is:
- Alice: 144 cookies
- Bob: 192 cookies
- Charlie: 96 cookies
--------------------------------------------------------------------------------
Orchestrator-orchestrator:default:
Received results from workers at layer 0
--------------------------------------------------------------------------------
Orchestrator-orchestrator:default:
Dispatch to workers at layer 1
--------------------------------------------------------------------------------
Worker-worker:default/layer_1/worker_2:
To divide 432 cookies in the ratio of 3:4:2 among Alice, Bob, and Charlie, follow these steps:

1. **Determine the total number of parts in the ratio**:
   \[
   3 + 4 + 2 = 9 \text{ parts}
   \]

2. **Calculate the value of one part** by dividing the total number of cookies by the total number of parts:
   \[
   \text{Value of one part} = \frac{432}{9} = 48
   \]

3. **Calculate the number of cookies each person receives** by multiplying the value of one part by the number of parts each individual gets:
   - **For Alice (3 parts)**:
     \[
     3 \times 48 = 144 \text{ cookies}
     \]
   - **For Bob (4 parts)**:
     \[
     4 \times 48 = 192 \text{ cookies}
     \]
   - **For Charlie (2 parts)**:
     \[
     2 \times 48 = 96 \text{ cookies}
     \]

Thus, the final distribution of cookies is:
- **Alice**: 144 cookies
- **Bob**: 192 cookies
- **Charlie**: 96 cookies
--------------------------------------------------------------------------------
Worker-worker:default/layer_1/worker_0:
To divide 432 cookies among Alice, Bob, and Charlie in the ratio of 3:4:2, we can follow these steps:

1. **Calculate the Total Parts**:
   Add the parts of the ratio together:
   \[
   3 + 4 + 2 = 9 \text{ parts}
   \]

2. **Determine the Value of One Part**:
   Divide the total number of cookies by the total number of parts:
   \[
   \text{Value of one part} = \frac{432 \text{ cookies}}{9 \text{ parts}} = 48 \text{ cookies/part}
   \]

3. **Calculate Each Person's Share**:
   - **Alice's Share** (3 parts):
     \[
     3 \times 48 = 144 \text{ cookies}
     \]
   - **Bob's Share** (4 parts):
     \[
     4 \times 48 = 192 \text{ cookies}
     \]
   - **Charlie's Share** (2 parts):
     \[
     2 \times 48 = 96 \text{ cookies}
     \]

4. **Final Distribution**:
   - Alice: 144 cookies
   - Bob: 192 cookies
   - Charlie: 96 cookies

Thus, the distribution of cookies is:
- **Alice**: 144 cookies
- **Bob**: 192 cookies
- **Charlie**: 96 cookies
--------------------------------------------------------------------------------
Worker-worker:default/layer_1/worker_1:
To divide 432 cookies among Alice, Bob, and Charlie in the ratio of 3:4:2, we first need to determine the total number of parts in this ratio.

1. **Calculate Total Parts:**
   \[
   3 \text{ (Alice)} + 4 \text{ (Bob)} + 2 \text{ (Charlie)} = 9 \text{ parts}
   \]

2. **Determine the Value of One Part:**
   Next, we'll find out how many cookies correspond to one part by dividing the total number of cookies by the total number of parts:
   \[
   \text{Value of one part} = \frac{432 \text{ cookies}}{9 \text{ parts}} = 48 \text{ cookies/part}
   \]

3. **Calculate the Share for Each Person:**
   - **Alice's Share (3 parts):**
     \[
     3 \times 48 = 144 \text{ cookies}
     \]
   - **Bob's Share (4 parts):**
     \[
     4 \times 48 = 192 \text{ cookies}
     \]
   - **Charlie’s Share (2 parts):**
     \[
     2 \times 48 = 96 \text{ cookies}
     \]

4. **Summary of the Distribution:**
   - **Alice:** 144 cookies
   - **Bob:** 192 cookies
   - **Charlie:** 96 cookies

In conclusion, Alice receives 144 cookies, Bob receives 192 cookies, and Charlie receives 96 cookies.
--------------------------------------------------------------------------------
Orchestrator-orchestrator:default:
Received results from workers at layer 1
--------------------------------------------------------------------------------
Orchestrator-orchestrator:default:
Performing final aggregation
--------------------------------------------------------------------------------
Final result:
To divide 432 cookies among Alice, Bob, and Charlie in the ratio of 3:4:2, follow these steps:

1. **Calculate the Total Parts in the Ratio:**
   Add the parts of the ratio together:
   \[
   3 + 4 + 2 = 9
   \]

2. **Determine the Value of One Part:**
   Divide the total number of cookies by the total number of parts:
   \[
   \text{Value of one part} = \frac{432}{9} = 48 \text{ cookies/part}
   \]

3. **Calculate Each Person's Share:**
   - **Alice's Share (3 parts):**
     \[
     3 \times 48 = 144 \text{ cookies}
     \]
   - **Bob's Share (4 parts):**
     \[
     4 \times 48 = 192 \text{ cookies}
     \]
   - **Charlie's Share (2 parts):**
     \[
     2 \times 48 = 96 \text{ cookies}
     \]

Therefore, the distribution of cookies is as follows:
- **Alice:** 144 cookies
- **Bob:** 192 cookies
- **Charlie:** 96 cookies

In summary, Alice gets 144 cookies, Bob gets 192 cookies, and Charlie gets 96 cookies.

```

### Multi-Agent Debate[#](#multi-agent-debate "Link to this heading")

Multi-Agent Debate is a multi-agent design pattern that simulates a multi-turn interaction
where in each turn, agents exchange their responses with each other, and refine
their responses based on the responses from other agents.

This example shows an implementation of the multi-agent debate pattern for solving
math problems from the [GSM8K benchmark](https://huggingface.co/datasets/openai/gsm8k).

There are of two types of agents in this pattern: solver agents and an aggregator agent.
The solver agents are connected in a sparse manner following the technique described in
[Improving Multi-Agent Debate with Sparse Communication Topology](https://arxiv.org/abs/2406.11776).
The solver agents are responsible for solving math problems and exchanging responses with each other.
The aggregator agent is responsible for distributing math problems to the solver agents,
waiting for their final responses, and aggregating the responses to get the final answer.

The pattern works as follows:

1. User sends a math problem to the aggregator agent.
2. The aggregator agent distributes the problem to the solver agents.
3. Each solver agent processes the problem, and publishes a response to its neighbors.
4. Each solver agent uses the responses from its neighbors to refine its response, and publishes a new response.
5. Repeat step 4 for a fixed number of rounds. In the final round, each solver agent publishes a final response.
6. The aggregator agent uses majority voting to aggregate the final responses from all solver agents to get a final answer, and publishes the answer.

We will be using the broadcast API, i.e., [`publish_message()`](#autogen_core.BaseAgent.publish_message "autogen_core.BaseAgent.publish_message"),
and we will be using topic and subscription to implement the communication topology.
Read about [Topics and Subscriptions](#document-user-guide/core-user-guide/core-concepts/topic-and-subscription) to understand how they work.

```
import re
from dataclasses import dataclass
from typing import Dict, List

from autogen_core import (
    DefaultTopicId,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    TypeSubscription,
    default_subscription,
    message_handler,
)
from autogen_core.models import (
    AssistantMessage,
    ChatCompletionClient,
    LLMMessage,
    SystemMessage,
    UserMessage,
)
from autogen_ext.models.openai import OpenAIChatCompletionClient

```

#### Message Protocol[#](#message-protocol "Link to this heading")

First, we define the messages used by the agents.
`IntermediateSolverResponse` is the message exchanged among the solver agents in each round,
and `FinalSolverResponse` is the message published by the solver agents in the final round.

```
@dataclass
class Question:
    content: str

@dataclass
class Answer:
    content: str

@dataclass
class SolverRequest:
    content: str
    question: str

@dataclass
class IntermediateSolverResponse:
    content: str
    question: str
    answer: str
    round: int

@dataclass
class FinalSolverResponse:
    answer: str

```

#### Solver Agent[#](#solver-agent "Link to this heading")

The solver agent is responsible for solving math problems and exchanging responses with other solver agents.
Upon receiving a `SolverRequest`, the solver agent uses an LLM to generate an answer.
Then, it publishes a `IntermediateSolverResponse`
or a `FinalSolverResponse` based on the round number.

The solver agent is given a topic type, which is used to indicate the topic
to which the agent should publish intermediate responses. This topic is subscribed
to by its neighbors to receive responses from this agent – we will show
how this is done later.

We use `default_subscription()` to let
solver agents subscribe to the default topic, which is used by the aggregator agent
to collect the final responses from the solver agents.

```
@default_subscription
class MathSolver(RoutedAgent):
    def __init__(self, model_client: ChatCompletionClient, topic_type: str, num_neighbors: int, max_round: int) -> None:
        super().__init__("A debator.")
        self._topic_type = topic_type
        self._model_client = model_client
        self._num_neighbors = num_neighbors
        self._history: List[LLMMessage] = []
        self._buffer: Dict[int, List[IntermediateSolverResponse]] = {}
        self._system_messages = [
            SystemMessage(
                content=(
                    "You are a helpful assistant with expertise in mathematics and reasoning. "
                    "Your task is to assist in solving a math reasoning problem by providing "
                    "a clear and detailed solution. Limit your output within 100 words, "
                    "and your final answer should be a single numerical number, "
                    "in the form of {{answer}}, at the end of your response. "
                    "For example, 'The answer is {{42}}.'"
                )
            )
        ]
        self._round = 0
        self._max_round = max_round

    @message_handler
    async def handle_request(self, message: SolverRequest, ctx: MessageContext) -> None:
        # Add the question to the memory.
        self._history.append(UserMessage(content=message.content, source="user"))
        # Make an inference using the model.
        model_result = await self._model_client.create(self._system_messages + self._history)
        assert isinstance(model_result.content, str)
        # Add the response to the memory.
        self._history.append(AssistantMessage(content=model_result.content, source=self.metadata["type"]))
        print(f"{'-'*80}\nSolver {self.id} round {self._round}:\n{model_result.content}")
        # Extract the answer from the response.
        match = re.search(r"\{\{(\-?\d+(\.\d+)?)\}\}", model_result.content)
        if match is None:
            raise ValueError("The model response does not contain the answer.")
        answer = match.group(1)
        # Increment the counter.
        self._round += 1
        if self._round == self._max_round:
            # If the counter reaches the maximum round, publishes a final response.
            await self.publish_message(FinalSolverResponse(answer=answer), topic_id=DefaultTopicId())
        else:
            # Publish intermediate response to the topic associated with this solver.
            await self.publish_message(
                IntermediateSolverResponse(
                    content=model_result.content,
                    question=message.question,
                    answer=answer,
                    round=self._round,
                ),
                topic_id=DefaultTopicId(type=self._topic_type),
            )

    @message_handler
    async def handle_response(self, message: IntermediateSolverResponse, ctx: MessageContext) -> None:
        # Add neighbor's response to the buffer.
        self._buffer.setdefault(message.round, []).append(message)
        # Check if all neighbors have responded.
        if len(self._buffer[message.round]) == self._num_neighbors:
            print(
                f"{'-'*80}\nSolver {self.id} round {message.round}:\nReceived all responses from {self._num_neighbors} neighbors."
            )
            # Prepare the prompt for the next question.
            prompt = "These are the solutions to the problem from other agents:\n"
            for resp in self._buffer[message.round]:
                prompt += f"One agent solution: {resp.content}\n"
            prompt += (
                "Using the solutions from other agents as additional information, "
                "can you provide your answer to the math problem? "
                f"The original math problem is {message.question}. "
                "Your final answer should be a single numerical number, "
                "in the form of {{answer}}, at the end of your response."
            )
            # Send the question to the agent itself to solve.
            await self.send_message(SolverRequest(content=prompt, question=message.question), self.id)
            # Clear the buffer.
            self._buffer.pop(message.round)

```

#### Aggregator Agent[#](#aggregator-agent "Link to this heading")

The aggregator agent is responsible for handling user question and
distributing math problems to the solver agents.

The aggregator subscribes to the default topic using
`default_subscription()`. The default topic is used to
recieve user question, receive the final responses from the solver agents,
and publish the final answer back to the user.

In a more complex application when you want to isolate the multi-agent debate into a
sub-component, you should use
`type_subscription()` to set a specific topic
type for the aggregator-solver communication,
and have the both the solver and aggregator publish and subscribe to that topic type.

```
@default_subscription
class MathAggregator(RoutedAgent):
    def __init__(self, num_solvers: int) -> None:
        super().__init__("Math Aggregator")
        self._num_solvers = num_solvers
        self._buffer: List[FinalSolverResponse] = []

    @message_handler
    async def handle_question(self, message: Question, ctx: MessageContext) -> None:
        print(f"{'-'*80}\nAggregator {self.id} received question:\n{message.content}")
        prompt = (
            f"Can you solve the following math problem?\n{message.content}\n"
            "Explain your reasoning. Your final answer should be a single numerical number, "
            "in the form of {{answer}}, at the end of your response."
        )
        print(f"{'-'*80}\nAggregator {self.id} publishes initial solver request.")
        await self.publish_message(SolverRequest(content=prompt, question=message.content), topic_id=DefaultTopicId())

    @message_handler
    async def handle_final_solver_response(self, message: FinalSolverResponse, ctx: MessageContext) -> None:
        self._buffer.append(message)
        if len(self._buffer) == self._num_solvers:
            print(f"{'-'*80}\nAggregator {self.id} received all final answers from {self._num_solvers} solvers.")
            # Find the majority answer.
            answers = [resp.answer for resp in self._buffer]
            majority_answer = max(set(answers), key=answers.count)
            # Publish the aggregated response.
            await self.publish_message(Answer(content=majority_answer), topic_id=DefaultTopicId())
            # Clear the responses.
            self._buffer.clear()
            print(f"{'-'*80}\nAggregator {self.id} publishes final answer:\n{majority_answer}")

```

#### Setting Up a Debate[#](#setting-up-a-debate "Link to this heading")

We will now set up a multi-agent debate with 4 solver agents and 1 aggregator agent.
The solver agents will be connected in a sparse manner as illustrated in the figure
below:

```
A --- B
|     |
|     |
C --- D

```

Each solver agent is connected to two other solver agents.
For example, agent A is connected to agents B and C.

Let’s first create a runtime and register the agent types.

```
runtime = SingleThreadedAgentRuntime()

model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")

await MathSolver.register(
    runtime,
    "MathSolverA",
    lambda: MathSolver(
        model_client=model_client,
        topic_type="MathSolverA",
        num_neighbors=2,
        max_round=3,
    ),
)
await MathSolver.register(
    runtime,
    "MathSolverB",
    lambda: MathSolver(
        model_client=model_client,
        topic_type="MathSolverB",
        num_neighbors=2,
        max_round=3,
    ),
)
await MathSolver.register(
    runtime,
    "MathSolverC",
    lambda: MathSolver(
        model_client=model_client,
        topic_type="MathSolverC",
        num_neighbors=2,
        max_round=3,
    ),
)
await MathSolver.register(
    runtime,
    "MathSolverD",
    lambda: MathSolver(
        model_client=model_client,
        topic_type="MathSolverD",
        num_neighbors=2,
        max_round=3,
    ),
)
await MathAggregator.register(runtime, "MathAggregator", lambda: MathAggregator(num_solvers=4))

```

```
AgentType(type='MathAggregator')

```

Now we will create the solver agent topology using `TypeSubscription`,
which maps each solver agent’s publishing topic type to its neighbors’ agent types.

```
# Subscriptions for topic published to by MathSolverA.
await runtime.add_subscription(TypeSubscription("MathSolverA", "MathSolverD"))
await runtime.add_subscription(TypeSubscription("MathSolverA", "MathSolverB"))

# Subscriptions for topic published to by MathSolverB.
await runtime.add_subscription(TypeSubscription("MathSolverB", "MathSolverA"))
await runtime.add_subscription(TypeSubscription("MathSolverB", "MathSolverC"))

# Subscriptions for topic published to by MathSolverC.
await runtime.add_subscription(TypeSubscription("MathSolverC", "MathSolverB"))
await runtime.add_subscription(TypeSubscription("MathSolverC", "MathSolverD"))

# Subscriptions for topic published to by MathSolverD.
await runtime.add_subscription(TypeSubscription("MathSolverD", "MathSolverC"))
await runtime.add_subscription(TypeSubscription("MathSolverD", "MathSolverA"))

# All solvers and the aggregator subscribe to the default topic.

```

#### Solving Math Problems[#](#solving-math-problems "Link to this heading")

Now let’s run the debate to solve a math problem.
We publish a `SolverRequest` to the default topic,
and the aggregator agent will start the debate.

```
question = "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?"
runtime.start()
await runtime.publish_message(Question(content=question), DefaultTopicId())
# Wait for the runtime to stop when idle.
await runtime.stop_when_idle()
# Close the connection to the model client.
await model_client.close()

```

```
--------------------------------------------------------------------------------
Aggregator MathAggregator:default received question:
Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?
--------------------------------------------------------------------------------
Aggregator MathAggregator:default publishes initial solver request.
--------------------------------------------------------------------------------
Solver MathSolverC:default round 0:
In April, Natalia sold 48 clips. In May, she sold half as many, which is 48 / 2 = 24 clips. To find the total number of clips sold in April and May, we add the amounts: 48 (April) + 24 (May) = 72 clips.

Thus, the total number of clips sold by Natalia is {{72}}.
--------------------------------------------------------------------------------
Solver MathSolverB:default round 0:
In April, Natalia sold 48 clips. In May, she sold half as many clips, which is 48 / 2 = 24 clips. To find the total clips sold in April and May, we add both amounts:

48 (April) + 24 (May) = 72.

Thus, the total number of clips sold altogether is {{72}}.
--------------------------------------------------------------------------------
Solver MathSolverD:default round 0:
Natalia sold 48 clips in April. In May, she sold half as many, which is \( \frac{48}{2} = 24 \) clips. To find the total clips sold in both months, we add the clips sold in April and May together:

\[ 48 + 24 = 72 \]

Thus, Natalia sold a total of 72 clips.

The answer is {{72}}.
--------------------------------------------------------------------------------
Solver MathSolverC:default round 1:
Received all responses from 2 neighbors.
--------------------------------------------------------------------------------
Solver MathSolverA:default round 1:
Received all responses from 2 neighbors.
--------------------------------------------------------------------------------
Solver MathSolverA:default round 0:
In April, Natalia sold clips to 48 friends. In May, she sold half as many, which is calculated as follows:

Half of 48 is \( 48 \div 2 = 24 \).

Now, to find the total clips sold in April and May, we add the totals from both months:

\( 48 + 24 = 72 \).

Thus, the total number of clips Natalia sold altogether in April and May is {{72}}.
--------------------------------------------------------------------------------
Solver MathSolverD:default round 1:
Received all responses from 2 neighbors.
--------------------------------------------------------------------------------
Solver MathSolverB:default round 1:
Received all responses from 2 neighbors.
--------------------------------------------------------------------------------
Solver MathSolverC:default round 1:
In April, Natalia sold 48 clips. In May, she sold half as many, which is 48 / 2 = 24 clips. The total number of clips sold in April and May is calculated by adding the two amounts: 48 (April) + 24 (May) = 72 clips.

Therefore, the answer is {{72}}.
--------------------------------------------------------------------------------
Solver MathSolverA:default round 1:
In April, Natalia sold 48 clips. In May, she sold half of that amount, which is 48 / 2 = 24 clips. To find the total clips sold in both months, we sum the clips from April and May:

48 (April) + 24 (May) = 72.

Thus, Natalia sold a total of {{72}} clips.

The answer is {{72}}.
--------------------------------------------------------------------------------
Solver MathSolverD:default round 2:
Received all responses from 2 neighbors.
--------------------------------------------------------------------------------
Solver MathSolverB:default round 2:
Received all responses from 2 neighbors.
--------------------------------------------------------------------------------
Solver MathSolverD:default round 1:
Natalia sold 48 clips in April. In May, she sold half of that, which is \( 48 \div 2 = 24 \) clips. To find the total clips sold, we add the clips sold in both months:

\[ 48 + 24 = 72 \]

Therefore, the total number of clips sold by Natalia is {{72}}.
--------------------------------------------------------------------------------
Solver MathSolverB:default round 1:
In April, Natalia sold 48 clips. In May, she sold half that amount, which is 48 / 2 = 24 clips. To find the total clips sold in both months, we add the amounts:

48 (April) + 24 (May) = 72.

Therefore, the total number of clips sold altogether by Natalia is {{72}}.
--------------------------------------------------------------------------------
Solver MathSolverA:default round 2:
Received all responses from 2 neighbors.
--------------------------------------------------------------------------------
Solver MathSolverC:default round 2:
Received all responses from 2 neighbors.
--------------------------------------------------------------------------------
Solver MathSolverA:default round 2:
In April, Natalia sold 48 clips. In May, she sold half of that amount, which is \( 48 \div 2 = 24 \) clips. To find the total clips sold in both months, we add the amounts from April and May:

\( 48 + 24 = 72 \).

Thus, the total number of clips sold by Natalia is {{72}}.
--------------------------------------------------------------------------------
Solver MathSolverC:default round 2:
In April, Natalia sold 48 clips. In May, she sold half of that amount, which is \( 48 \div 2 = 24 \) clips. To find the total number of clips sold in both months, we add the clips sold in April and May:

48 (April) + 24 (May) = 72.

Thus, the total number of clips sold altogether by Natalia is {{72}}.
--------------------------------------------------------------------------------
Solver MathSolverB:default round 2:
In April, Natalia sold 48 clips. In May, she sold half as many, calculated as \( 48 \div 2 = 24 \) clips. To find the total clips sold over both months, we sum the totals:

\( 48 (April) + 24 (May) = 72 \).

Therefore, the total number of clips Natalia sold is {{72}}.
--------------------------------------------------------------------------------
Solver MathSolverD:default round 2:
To solve the problem, we know that Natalia sold 48 clips in April. In May, she sold half that amount, which is calculated as \( 48 \div 2 = 24 \) clips. To find the total number of clips sold over both months, we add the two amounts together:

\[ 48 + 24 = 72 \]

Thus, the total number of clips sold by Natalia is {{72}}.
--------------------------------------------------------------------------------
Aggregator MathAggregator:default received all final answers from 4 solvers.
--------------------------------------------------------------------------------
Aggregator MathAggregator:default publishes final answer:
72

```

### Reflection[#](#reflection "Link to this heading")

Reflection is a design pattern where an LLM generation is followed by a reflection,
which in itself is another LLM generation conditioned on the output of the first one.
For example, given a task to write code, the first LLM can generate a code snippet,
and the second LLM can generate a critique of the code snippet.

In the context of AutoGen and agents, reflection can be implemented as a pair
of agents, where the first agent generates a message and the second agent
generates a response to the message. The two agents continue to interact
until they reach a stopping condition, such as a maximum number of iterations
or an approval from the second agent.

Let’s implement a simple reflection design pattern using AutoGen agents.
There will be two agents: a coder agent and a reviewer agent, the coder agent
will generate a code snippet, and the reviewer agent will generate a critique
of the code snippet.

#### Message Protocol[#](#message-protocol "Link to this heading")

Before we define the agents, we need to first define the message protocol for the agents.

```
from dataclasses import dataclass

@dataclass
class CodeWritingTask:
    task: str

@dataclass
class CodeWritingResult:
    task: str
    code: str
    review: str

@dataclass
class CodeReviewTask:
    session_id: str
    code_writing_task: str
    code_writing_scratchpad: str
    code: str

@dataclass
class CodeReviewResult:
    review: str
    session_id: str
    approved: bool

```

The above set of messages defines the protocol for our example reflection design pattern:

* The application sends a `CodeWritingTask` message to the coder agent
* The coder agent generates a `CodeReviewTask` message, which is sent to the reviewer agent
* The reviewer agent generates a `CodeReviewResult` message, which is sent back to the coder agent
* Depending on the `CodeReviewResult` message, if the code is approved, the coder agent sends a `CodeWritingResult` message
  back to the application, otherwise, the coder agent sends another `CodeReviewTask` message to the reviewer agent,
  and the process continues.

We can visualize the message protocol using a data flow diagram:

![coder-reviewer data flow](_images/coder-reviewer-data-flow.svg)

#### Agents[#](#agents "Link to this heading")

Now, let’s define the agents for the reflection design pattern.

```
import json
import re
import uuid
from typing import Dict, List, Union

from autogen_core import MessageContext, RoutedAgent, TopicId, default_subscription, message_handler
from autogen_core.models import (
    AssistantMessage,
    ChatCompletionClient,
    LLMMessage,
    SystemMessage,
    UserMessage,
)

```

We use the [Broadcast](#broadcast) API
to implement the design pattern. The agents implements the pub/sub model.
The coder agent subscribes to the `CodeWritingTask` and `CodeReviewResult` messages,
and publishes the `CodeReviewTask` and `CodeWritingResult` messages.

```
@default_subscription
class CoderAgent(RoutedAgent):
    """An agent that performs code writing tasks."""

    def __init__(self, model_client: ChatCompletionClient) -> None:
        super().__init__("A code writing agent.")
        self._system_messages: List[LLMMessage] = [
            SystemMessage(
                content="""You are a proficient coder. You write code to solve problems.
Work with the reviewer to improve your code.
Always put all finished code in a single Markdown code block.
For example:
```python
def hello_world():
    print("Hello, World!")
```

Respond using the following format:

Thoughts: <Your comments>
Code: <Your code>
""",
            )
        ]
        self._model_client = model_client
        self._session_memory: Dict[str, List[CodeWritingTask | CodeReviewTask | CodeReviewResult]] = {}

    @message_handler
    async def handle_code_writing_task(self, message: CodeWritingTask, ctx: MessageContext) -> None:
        # Store the messages in a temporary memory for this request only.
        session_id = str(uuid.uuid4())
        self._session_memory.setdefault(session_id, []).append(message)
        # Generate a response using the chat completion API.
        response = await self._model_client.create(
            self._system_messages + [UserMessage(content=message.task, source=self.metadata["type"])],
            cancellation_token=ctx.cancellation_token,
        )
        assert isinstance(response.content, str)
        # Extract the code block from the response.
        code_block = self._extract_code_block(response.content)
        if code_block is None:
            raise ValueError("Code block not found.")
        # Create a code review task.
        code_review_task = CodeReviewTask(
            session_id=session_id,
            code_writing_task=message.task,
            code_writing_scratchpad=response.content,
            code=code_block,
        )
        # Store the code review task in the session memory.
        self._session_memory[session_id].append(code_review_task)
        # Publish a code review task.
        await self.publish_message(code_review_task, topic_id=TopicId("default", self.id.key))

    @message_handler
    async def handle_code_review_result(self, message: CodeReviewResult, ctx: MessageContext) -> None:
        # Store the review result in the session memory.
        self._session_memory[message.session_id].append(message)
        # Obtain the request from previous messages.
        review_request = next(
            m for m in reversed(self._session_memory[message.session_id]) if isinstance(m, CodeReviewTask)
        )
        assert review_request is not None
        # Check if the code is approved.
        if message.approved:
            # Publish the code writing result.
            await self.publish_message(
                CodeWritingResult(
                    code=review_request.code,
                    task=review_request.code_writing_task,
                    review=message.review,
                ),
                topic_id=TopicId("default", self.id.key),
            )
            print("Code Writing Result:")
            print("-" * 80)
            print(f"Task:\n{review_request.code_writing_task}")
            print("-" * 80)
            print(f"Code:\n{review_request.code}")
            print("-" * 80)
            print(f"Review:\n{message.review}")
            print("-" * 80)
        else:
            # Create a list of LLM messages to send to the model.
            messages: List[LLMMessage] = [*self._system_messages]
            for m in self._session_memory[message.session_id]:
                if isinstance(m, CodeReviewResult):
                    messages.append(UserMessage(content=m.review, source="Reviewer"))
                elif isinstance(m, CodeReviewTask):
                    messages.append(AssistantMessage(content=m.code_writing_scratchpad, source="Coder"))
                elif isinstance(m, CodeWritingTask):
                    messages.append(UserMessage(content=m.task, source="User"))
                else:
                    raise ValueError(f"Unexpected message type: {m}")
            # Generate a revision using the chat completion API.
            response = await self._model_client.create(messages, cancellation_token=ctx.cancellation_token)
            assert isinstance(response.content, str)
            # Extract the code block from the response.
            code_block = self._extract_code_block(response.content)
            if code_block is None:
                raise ValueError("Code block not found.")
            # Create a new code review task.
            code_review_task = CodeReviewTask(
                session_id=message.session_id,
                code_writing_task=review_request.code_writing_task,
                code_writing_scratchpad=response.content,
                code=code_block,
            )
            # Store the code review task in the session memory.
            self._session_memory[message.session_id].append(code_review_task)
            # Publish a new code review task.
            await self.publish_message(code_review_task, topic_id=TopicId("default", self.id.key))

    def _extract_code_block(self, markdown_text: str) -> Union[str, None]:
        pattern = r"```(\w+)\n(.*?)\n```"
        # Search for the pattern in the markdown text
        match = re.search(pattern, markdown_text, re.DOTALL)
        # Extract the language and code block if a match is found
        if match:
            return match.group(2)
        return None

```

A few things to note about `CoderAgent`:

* It uses chain-of-thought prompting in its system message.
* It stores message histories for different `CodeWritingTask` in a dictionary,
  so each task has its own history.
* When making an LLM inference request using its model client, it transforms
  the message history into a list of `autogen_core.models.LLMMessage` objects
  to pass to the model client.

The reviewer agent subscribes to the `CodeReviewTask` message and publishes the `CodeReviewResult` message.

```
@default_subscription
class ReviewerAgent(RoutedAgent):
    """An agent that performs code review tasks."""

    def __init__(self, model_client: ChatCompletionClient) -> None:
        super().__init__("A code reviewer agent.")
        self._system_messages: List[LLMMessage] = [
            SystemMessage(
                content="""You are a code reviewer. You focus on correctness, efficiency and safety of the code.
Respond using the following JSON format:
{
    "correctness": "<Your comments>",
    "efficiency": "<Your comments>",
    "safety": "<Your comments>",
    "approval": "<APPROVE or REVISE>",
    "suggested_changes": "<Your comments>"
}
""",
            )
        ]
        self._session_memory: Dict[str, List[CodeReviewTask | CodeReviewResult]] = {}
        self._model_client = model_client

    @message_handler
    async def handle_code_review_task(self, message: CodeReviewTask, ctx: MessageContext) -> None:
        # Format the prompt for the code review.
        # Gather the previous feedback if available.
        previous_feedback = ""
        if message.session_id in self._session_memory:
            previous_review = next(
                (m for m in reversed(self._session_memory[message.session_id]) if isinstance(m, CodeReviewResult)),
                None,
            )
            if previous_review is not None:
                previous_feedback = previous_review.review
        # Store the messages in a temporary memory for this request only.
        self._session_memory.setdefault(message.session_id, []).append(message)
        prompt = f"""The problem statement is: {message.code_writing_task}
The code is:
```
{message.code}
```

Previous feedback:
{previous_feedback}

Please review the code. If previous feedback was provided, see if it was addressed.
"""
        # Generate a response using the chat completion API.
        response = await self._model_client.create(
            self._system_messages + [UserMessage(content=prompt, source=self.metadata["type"])],
            cancellation_token=ctx.cancellation_token,
            json_output=True,
        )
        assert isinstance(response.content, str)
        # TODO: use structured generation library e.g. guidance to ensure the response is in the expected format.
        # Parse the response JSON.
        review = json.loads(response.content)
        # Construct the review text.
        review_text = "Code review:\n" + "\n".join([f"{k}: {v}" for k, v in review.items()])
        approved = review["approval"].lower().strip() == "approve"
        result = CodeReviewResult(
            review=review_text,
            session_id=message.session_id,
            approved=approved,
        )
        # Store the review result in the session memory.
        self._session_memory[message.session_id].append(result)
        # Publish the review result.
        await self.publish_message(result, topic_id=TopicId("default", self.id.key))

```

The `ReviewerAgent` uses JSON-mode when making an LLM inference request, and
also uses chain-of-thought prompting in its system message.

#### Logging[#](#logging "Link to this heading")

Turn on logging to see the messages exchanged between the agents.

```
import logging

logging.basicConfig(level=logging.WARNING)
logging.getLogger("autogen_core").setLevel(logging.DEBUG)

```

#### Running the Design Pattern[#](#running-the-design-pattern "Link to this heading")

Let’s test the design pattern with a coding task.
Since all the agents are decorated with the `default_subscription()` class decorator,
the agents when created will automatically subscribe to the default topic.
We publish a `CodeWritingTask` message to the default topic to start the reflection process.

```
from autogen_core import DefaultTopicId, SingleThreadedAgentRuntime
from autogen_ext.models.openai import OpenAIChatCompletionClient

runtime = SingleThreadedAgentRuntime()
model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")
await ReviewerAgent.register(runtime, "ReviewerAgent", lambda: ReviewerAgent(model_client=model_client))
await CoderAgent.register(runtime, "CoderAgent", lambda: CoderAgent(model_client=model_client))
runtime.start()
await runtime.publish_message(
    message=CodeWritingTask(task="Write a function to find the sum of all even numbers in a list."),
    topic_id=DefaultTopicId(),
)

# Keep processing messages until idle.
await runtime.stop_when_idle()
# Close the model client.
await model_client.close()

```

```
INFO:autogen_core:Publishing message of type CodeWritingTask to all subscribers: {'task': 'Write a function to find the sum of all even numbers in a list.'}
INFO:autogen_core:Calling message handler for ReviewerAgent with message type CodeWritingTask published by Unknown
INFO:autogen_core:Calling message handler for CoderAgent with message type CodeWritingTask published by Unknown
INFO:autogen_core:Unhandled message: CodeWritingTask(task='Write a function to find the sum of all even numbers in a list.')
INFO:autogen_core.events:{"prompt_tokens": 101, "completion_tokens": 88, "type": "LLMCall"}
INFO:autogen_core:Publishing message of type CodeReviewTask to all subscribers: {'session_id': '51db93d5-3e29-4b7f-9f96-77be7bb02a5e', 'code_writing_task': 'Write a function to find the sum of all even numbers in a list.', 'code_writing_scratchpad': 'Thoughts: To find the sum of all even numbers in a list, we can use a list comprehension to filter out the even numbers and then use the `sum()` function to calculate their total. The implementation should handle edge cases like an empty list or a list with no even numbers.\n\nCode:\n```python\ndef sum_of_even_numbers(numbers):\n    return sum(num for num in numbers if num % 2 == 0)\n```', 'code': 'def sum_of_even_numbers(numbers):\n    return sum(num for num in numbers if num % 2 == 0)'}
INFO:autogen_core:Calling message handler for ReviewerAgent with message type CodeReviewTask published by CoderAgent:default
INFO:autogen_core.events:{"prompt_tokens": 163, "completion_tokens": 235, "type": "LLMCall"}
INFO:autogen_core:Publishing message of type CodeReviewResult to all subscribers: {'review': "Code review:\ncorrectness: The function correctly identifies and sums all even numbers in the provided list. The use of a generator expression ensures that only even numbers are processed, which is correct.\nefficiency: The function is efficient as it utilizes a generator expression that avoids creating an intermediate list, therefore using less memory. The time complexity is O(n) where n is the number of elements in the input list, which is optimal for this task.\nsafety: The function does not include checks for input types. If a non-iterable or a list containing non-integer types is passed, it could lead to unexpected behavior or errors. It’s advisable to handle such cases.\napproval: REVISE\nsuggested_changes: Consider adding input validation to ensure that 'numbers' is a list and contains only integers. You could raise a ValueError if the input is invalid. Example: 'if not isinstance(numbers, list) or not all(isinstance(num, int) for num in numbers): raise ValueError('Input must be a list of integers')'. This will make the function more robust.", 'session_id': '51db93d5-3e29-4b7f-9f96-77be7bb02a5e', 'approved': False}
INFO:autogen_core:Calling message handler for CoderAgent with message type CodeReviewResult published by ReviewerAgent:default
INFO:autogen_core.events:{"prompt_tokens": 421, "completion_tokens": 119, "type": "LLMCall"}
INFO:autogen_core:Publishing message of type CodeReviewTask to all subscribers: {'session_id': '51db93d5-3e29-4b7f-9f96-77be7bb02a5e', 'code_writing_task': 'Write a function to find the sum of all even numbers in a list.', 'code_writing_scratchpad': "Thoughts: I appreciate the reviewer's feedback on input validation. Adding type checks ensures that the function can handle unexpected inputs gracefully. I will implement the suggested changes and include checks for both the input type and the elements within the list to confirm that they are integers.\n\nCode:\n```python\ndef sum_of_even_numbers(numbers):\n    if not isinstance(numbers, list) or not all(isinstance(num, int) for num in numbers):\n        raise ValueError('Input must be a list of integers')\n    \n    return sum(num for num in numbers if num % 2 == 0)\n```", 'code': "def sum_of_even_numbers(numbers):\n    if not isinstance(numbers, list) or not all(isinstance(num, int) for num in numbers):\n        raise ValueError('Input must be a list of integers')\n    \n    return sum(num for num in numbers if num % 2 == 0)"}
INFO:autogen_core:Calling message handler for ReviewerAgent with message type CodeReviewTask published by CoderAgent:default
INFO:autogen_core.events:{"prompt_tokens": 420, "completion_tokens": 153, "type": "LLMCall"}
INFO:autogen_core:Publishing message of type CodeReviewResult to all subscribers: {'review': 'Code review:\ncorrectness: The function correctly sums all even numbers in the provided list. It raises a ValueError if the input is not a list of integers, which is a necessary check for correctness.\nefficiency: The function remains efficient with a time complexity of O(n) due to the use of a generator expression. There are no unnecessary intermediate lists created, so memory usage is optimal.\nsafety: The function includes input validation, which enhances safety by preventing incorrect input types. It raises a ValueError for invalid inputs, making the function more robust against unexpected data.\napproval: APPROVE\nsuggested_changes: No further changes are necessary as the previous feedback has been adequately addressed.', 'session_id': '51db93d5-3e29-4b7f-9f96-77be7bb02a5e', 'approved': True}
INFO:autogen_core:Calling message handler for CoderAgent with message type CodeReviewResult published by ReviewerAgent:default
INFO:autogen_core:Publishing message of type CodeWritingResult to all subscribers: {'task': 'Write a function to find the sum of all even numbers in a list.', 'code': "def sum_of_even_numbers(numbers):\n    if not isinstance(numbers, list) or not all(isinstance(num, int) for num in numbers):\n        raise ValueError('Input must be a list of integers')\n    \n    return sum(num for num in numbers if num % 2 == 0)", 'review': 'Code review:\ncorrectness: The function correctly sums all even numbers in the provided list. It raises a ValueError if the input is not a list of integers, which is a necessary check for correctness.\nefficiency: The function remains efficient with a time complexity of O(n) due to the use of a generator expression. There are no unnecessary intermediate lists created, so memory usage is optimal.\nsafety: The function includes input validation, which enhances safety by preventing incorrect input types. It raises a ValueError for invalid inputs, making the function more robust against unexpected data.\napproval: APPROVE\nsuggested_changes: No further changes are necessary as the previous feedback has been adequately addressed.'}
INFO:autogen_core:Calling message handler for ReviewerAgent with message type CodeWritingResult published by CoderAgent:default
INFO:autogen_core:Unhandled message: CodeWritingResult(task='Write a function to find the sum of all even numbers in a list.', code="def sum_of_even_numbers(numbers):\n    if not isinstance(numbers, list) or not all(isinstance(num, int) for num in numbers):\n        raise ValueError('Input must be a list of integers')\n    \n    return sum(num for num in numbers if num % 2 == 0)", review='Code review:\ncorrectness: The function correctly sums all even numbers in the provided list. It raises a ValueError if the input is not a list of integers, which is a necessary check for correctness.\nefficiency: The function remains efficient with a time complexity of O(n) due to the use of a generator expression. There are no unnecessary intermediate lists created, so memory usage is optimal.\nsafety: The function includes input validation, which enhances safety by preventing incorrect input types. It raises a ValueError for invalid inputs, making the function more robust against unexpected data.\napproval: APPROVE\nsuggested_changes: No further changes are necessary as the previous feedback has been adequately addressed.')

```

```
Code Writing Result:
--------------------------------------------------------------------------------
Task:
Write a function to find the sum of all even numbers in a list.
--------------------------------------------------------------------------------
Code:
def sum_of_even_numbers(numbers):
    if not isinstance(numbers, list) or not all(isinstance(num, int) for num in numbers):
        raise ValueError('Input must be a list of integers')

    return sum(num for num in numbers if num % 2 == 0)
--------------------------------------------------------------------------------
Review:
Code review:
correctness: The function correctly sums all even numbers in the provided list. It raises a ValueError if the input is not a list of integers, which is a necessary check for correctness.
efficiency: The function remains efficient with a time complexity of O(n) due to the use of a generator expression. There are no unnecessary intermediate lists created, so memory usage is optimal.
safety: The function includes input validation, which enhances safety by preventing incorrect input types. It raises a ValueError for invalid inputs, making the function more robust against unexpected data.
approval: APPROVE
suggested_changes: No further changes are necessary as the previous feedback has been adequately addressed.
--------------------------------------------------------------------------------

```

The log messages show the interaction between the coder and reviewer agents.
The final output shows the code snippet generated by the coder agent and the critique generated by the reviewer agent.

### Code Execution[#](#code-execution "Link to this heading")

In this section we explore creating custom agents to handle code generation and execution. These tasks can be handled using the provided Agent implementations found here [`AssistantAgent()`](#autogen_agentchat.agents.AssistantAgent "autogen_agentchat.agents.AssistantAgent"), [`CodeExecutorAgent()`](#autogen_agentchat.agents.CodeExecutorAgent "autogen_agentchat.agents.CodeExecutorAgent"); but this guide will show you how to implement custom, lightweight agents that can replace their functionality. This simple example implements two agents that create a plot of Tesla’s and Nvidia’s stock returns.

We first define the agent classes and their respective procedures for
handling messages.
We create two agent classes: `Assistant` and `Executor`. The `Assistant`
agent writes code and the `Executor` agent executes the code.
We also create a `Message` data class, which defines the messages that are passed between
the agents.

Attention

Code generated in this example is run within a [Docker](https://www.docker.com/) container. Please ensure Docker is [installed](https://docs.docker.com/get-started/get-docker/) and running prior to running the example. Local code execution is available ([`LocalCommandLineCodeExecutor`](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor "autogen_ext.code_executors.local.LocalCommandLineCodeExecutor")) but is not recommended due to the risk of running LLM generated code in your local environment.

```
import re
from dataclasses import dataclass
from typing import List

from autogen_core import DefaultTopicId, MessageContext, RoutedAgent, default_subscription, message_handler
from autogen_core.code_executor import CodeBlock, CodeExecutor
from autogen_core.models import (
    AssistantMessage,
    ChatCompletionClient,
    LLMMessage,
    SystemMessage,
    UserMessage,
)

@dataclass
class Message:
    content: str

@default_subscription
class Assistant(RoutedAgent):
    def __init__(self, model_client: ChatCompletionClient) -> None:
        super().__init__("An assistant agent.")
        self._model_client = model_client
        self._chat_history: List[LLMMessage] = [
            SystemMessage(
                content="""Write Python script in markdown block, and it will be executed.
Always save figures to file in the current directory. Do not use plt.show(). All code required to complete this task must be contained within a single response.""",
            )
        ]

    @message_handler
    async def handle_message(self, message: Message, ctx: MessageContext) -> None:
        self._chat_history.append(UserMessage(content=message.content, source="user"))
        result = await self._model_client.create(self._chat_history)
        print(f"\n{'-'*80}\nAssistant:\n{result.content}")
        self._chat_history.append(AssistantMessage(content=result.content, source="assistant"))  # type: ignore
        await self.publish_message(Message(content=result.content), DefaultTopicId())  # type: ignore

def extract_markdown_code_blocks(markdown_text: str) -> List[CodeBlock]:
    pattern = re.compile(r"```(?:\s*([\w\+\-]+))?\n([\s\S]*?)```")
    matches = pattern.findall(markdown_text)
    code_blocks: List[CodeBlock] = []
    for match in matches:
        language = match[0].strip() if match[0] else ""
        code_content = match[1]
        code_blocks.append(CodeBlock(code=code_content, language=language))
    return code_blocks

@default_subscription
class Executor(RoutedAgent):
    def __init__(self, code_executor: CodeExecutor) -> None:
        super().__init__("An executor agent.")
        self._code_executor = code_executor

    @message_handler
    async def handle_message(self, message: Message, ctx: MessageContext) -> None:
        code_blocks = extract_markdown_code_blocks(message.content)
        if code_blocks:
            result = await self._code_executor.execute_code_blocks(
                code_blocks, cancellation_token=ctx.cancellation_token
            )
            print(f"\n{'-'*80}\nExecutor:\n{result.output}")
            await self.publish_message(Message(content=result.output), DefaultTopicId())

```

You might have already noticed, the agents’ logic, whether it is using model or code executor,
is completely decoupled from
how messages are delivered. This is the core idea: the framework provides
a communication infrastructure, and the agents are responsible for their own
logic. We call the communication infrastructure an **Agent Runtime**.

Agent runtime is a key concept of this framework. Besides delivering messages,
it also manages agents’ lifecycle.
So the creation of agents are handled by the runtime.

The following code shows how to register and run the agents using
[`SingleThreadedAgentRuntime`](#autogen_core.SingleThreadedAgentRuntime "autogen_core.SingleThreadedAgentRuntime"),
a local embedded agent runtime implementation.

```
import tempfile

from autogen_core import SingleThreadedAgentRuntime
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient

work_dir = tempfile.mkdtemp()

# Create an local embedded runtime.
runtime = SingleThreadedAgentRuntime()

async with DockerCommandLineCodeExecutor(work_dir=work_dir) as executor:  # type: ignore[syntax]
    # Register the assistant and executor agents by providing
    # their agent types, the factory functions for creating instance and subscriptions.
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key="YOUR_API_KEY"
    )
    await Assistant.register(
        runtime,
        "assistant",
        lambda: Assistant(model_client=model_client),
    )
    await Executor.register(runtime, "executor", lambda: Executor(executor))

    # Start the runtime and publish a message to the assistant.
    runtime.start()
    await runtime.publish_message(
        Message("Create a plot of NVIDA vs TSLA stock returns YTD from 2024-01-01."), DefaultTopicId()
    )

    # Wait for the runtime to stop when idle.
    await runtime.stop_when_idle()
    # Close the connection to the model client.
    await model_client.close()

```

```
--------------------------------------------------------------------------------
Assistant:
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import yfinance as yf

# Define the ticker symbols for NVIDIA and Tesla
tickers = ['NVDA', 'TSLA']

# Download the stock data from Yahoo Finance starting from 2024-01-01
start_date = '2024-01-01'
end_date = pd.to_datetime('today').strftime('%Y-%m-%d')

# Download the adjusted closing prices
stock_data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']

# Calculate the daily returns
returns = stock_data.pct_change().dropna()

# Plot the cumulative returns for each stock
cumulative_returns = (1 + returns).cumprod()

plt.figure(figsize=(10, 6))
plt.plot(cumulative_returns.index, cumulative_returns['NVDA'], label='NVIDIA', color='green')
plt.plot(cumulative_returns.index, cumulative_returns['TSLA'], label='Tesla', color='red')
plt.title('NVIDIA vs Tesla Stock Returns YTD (2024)')
plt.xlabel('Date')
plt.ylabel('Cumulative Return')
plt.legend()
plt.grid(True)
plt.tight_layout()

# Save the plot to a file
plt.savefig('nvidia_vs_tesla_ytd_returns.png')
```

--------------------------------------------------------------------------------
Executor:
Traceback (most recent call last):
  File "/workspace/tmp_code_fd7395dcad4fbb74d40c981411db604e78e1a17783ca1fab3aaec34ff2c3fdf0.python", line 1, in <module>
    import pandas as pd
ModuleNotFoundError: No module named 'pandas'

--------------------------------------------------------------------------------
Assistant:
It seems like the necessary libraries are not available in your environment. However, since I can't install packages or check the environment directly from here, you'll need to make sure that the appropriate packages are installed in your working environment. Once the modules are available, the script provided will execute properly.

Here's how you can install the required packages using pip (make sure to run these commands in your terminal or command prompt):

```bash
pip install pandas matplotlib yfinance
```

Let me provide you the script again for reference:

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import yfinance as yf

# Define the ticker symbols for NVIDIA and Tesla
tickers = ['NVDA', 'TSLA']

# Download the stock data from Yahoo Finance starting from 2024-01-01
start_date = '2024-01-01'
end_date = pd.to_datetime('today').strftime('%Y-%m-%d')

# Download the adjusted closing prices
stock_data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']

# Calculate the daily returns
returns = stock_data.pct_change().dropna()

# Plot the cumulative returns for each stock
cumulative_returns = (1 + returns).cumprod()

plt.figure(figsize=(10, 6))
plt.plot(cumulative_returns.index, cumulative_returns['NVDA'], label='NVIDIA', color='green')
plt.plot(cumulative_returns.index, cumulative_returns['TSLA'], label='Tesla', color='red')
plt.title('NVIDIA vs Tesla Stock Returns YTD (2024)')
plt.xlabel('Date')
plt.ylabel('Cumulative Return')
plt.legend()
plt.grid(True)
plt.tight_layout()

# Save the plot to a file
plt.savefig('nvidia_vs_tesla_ytd_returns.png')
```

Make sure to install the packages in the environment where you run this script. Feel free to ask if you have further questions or issues!

--------------------------------------------------------------------------------
Executor:
[*********************100%***********************]  2 of 2 completed

--------------------------------------------------------------------------------
Assistant:
It looks like the data fetching process completed successfully. You should now have a plot saved as `nvidia_vs_tesla_ytd_returns.png` in your current directory. If you have any additional questions or need further assistance, feel free to ask!

```

From the agent’s output, we can see the plot of Tesla’s and Nvidia’s stock returns
has been created.

```
from IPython.display import Image

Image(filename=f"{work_dir}/nvidia_vs_tesla_ytd_returns.png")  # type: ignore

```

![_images/853f54c611e65782533a876077b27c2489a1b9de6d6cdb9b891767288c39eea7.png](_images/853f54c611e65782533a876077b27c2489a1b9de6d6cdb9b891767288c39eea7.png)

AutoGen also supports a distributed agent runtime, which can host agents running on
different processes or machines, with different identities, languages and dependencies.

To learn how to use agent runtime, communication, message handling, and subscription, please continue
reading the sections following this quick start.

### Cookbook[#](#cookbook "Link to this heading")

This section contains a collection of recipes that demonstrate how to use the Core API features.

#### List of recipes[#](#list-of-recipes "Link to this heading")

##### Azure OpenAI with AAD Auth[#](#azure-openai-with-aad-auth "Link to this heading")

This guide will show you how to use the Azure OpenAI client with Azure Active Directory (AAD) authentication.

The identity used must be assigned the [**Cognitive Services OpenAI User**](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control#cognitive-services-openai-user) role.

###### Install Azure Identity client[#](#install-azure-identity-client "Link to this heading")

The Azure identity client is used to authenticate with Azure Active Directory.

```
pip install azure-identity

```

###### Using the Model Client[#](#using-the-model-client "Link to this heading")

```
from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
from azure.identity import DefaultAzureCredential, get_bearer_token_provider

# Create the token provider
token_provider = get_bearer_token_provider(
    DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default"
)

client = AzureOpenAIChatCompletionClient(
    azure_deployment="{your-azure-deployment}",
    model="{model-name, such as gpt-4o}",
    api_version="2024-02-01",
    azure_endpoint="https://{your-custom-endpoint}.openai.azure.com/",
    azure_ad_token_provider=token_provider,
)

```

Note

See [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity#chat-completions) for how to use the Azure client directly or for more info.

##### Termination using Intervention Handler[#](#termination-using-intervention-handler "Link to this heading")

Note

This method is valid when using [`SingleThreadedAgentRuntime`](#autogen_core.SingleThreadedAgentRuntime "autogen_core.SingleThreadedAgentRuntime").

There are many different ways to handle termination in `autogen_core`. Ultimately, the goal is to detect that the runtime no longer needs to be executed and you can proceed to finalization tasks. One way to do this is to use an `autogen_core.base.intervention.InterventionHandler` to detect a termination message and then act on it.

```
from dataclasses import dataclass
from typing import Any

from autogen_core import (
    DefaultInterventionHandler,
    DefaultTopicId,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    default_subscription,
    message_handler,
)

```

First, we define a dataclass for regular message and message that will be used to signal termination.

```
@dataclass
class Message:
    content: Any

@dataclass
class Termination:
    reason: str

```

We code our agent to publish a termination message when it decides it is time to terminate.

```
@default_subscription
class AnAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("MyAgent")
        self.received = 0

    @message_handler
    async def on_new_message(self, message: Message, ctx: MessageContext) -> None:
        self.received += 1
        if self.received > 3:
            await self.publish_message(Termination(reason="Reached maximum number of messages"), DefaultTopicId())

```

Next, we create an InterventionHandler that will detect the termination message and act on it. This one hooks into publishes and when it encounters `Termination` it alters its internal state to indicate that termination has been requested.

```
class TerminationHandler(DefaultInterventionHandler):
    def __init__(self) -> None:
        self._termination_value: Termination | None = None

    async def on_publish(self, message: Any, *, message_context: MessageContext) -> Any:
        if isinstance(message, Termination):
            self._termination_value = message
        return message

    @property
    def termination_value(self) -> Termination | None:
        return self._termination_value

    @property
    def has_terminated(self) -> bool:
        return self._termination_value is not None

```

Finally, we add this handler to the runtime and use it to detect termination and stop the runtime when the termination message is received.

```
termination_handler = TerminationHandler()
runtime = SingleThreadedAgentRuntime(intervention_handlers=[termination_handler])

await AnAgent.register(runtime, "my_agent", AnAgent)

runtime.start()

# Publish more than 3 messages to trigger termination.
await runtime.publish_message(Message("hello"), DefaultTopicId())
await runtime.publish_message(Message("hello"), DefaultTopicId())
await runtime.publish_message(Message("hello"), DefaultTopicId())
await runtime.publish_message(Message("hello"), DefaultTopicId())

# Wait for termination.
await runtime.stop_when(lambda: termination_handler.has_terminated)

print(termination_handler.termination_value)

```

```
Termination(reason='Reached maximum number of messages')

```

##### User Approval for Tool Execution using Intervention Handler[#](#user-approval-for-tool-execution-using-intervention-handler "Link to this heading")

This cookbook shows how to intercept the tool execution using
an intervention hanlder, and prompt the user for permission to execute the tool.

```
from dataclasses import dataclass
from typing import Any, List

from autogen_core import (
    AgentId,
    AgentType,
    DefaultInterventionHandler,
    DropMessage,
    FunctionCall,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    message_handler,
)
from autogen_core.models import (
    ChatCompletionClient,
    LLMMessage,
    SystemMessage,
    UserMessage,
)
from autogen_core.tool_agent import ToolAgent, ToolException, tool_agent_caller_loop
from autogen_core.tools import ToolSchema
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.code_execution import PythonCodeExecutionTool

```

Let’s define a simple message type that carries a string content.

```
@dataclass
class Message:
    content: str

```

Let’s create a simple tool use agent that is capable of using tools through a
[`ToolAgent`](#autogen_core.tool_agent.ToolAgent "autogen_core.tool_agent.ToolAgent").

```
class ToolUseAgent(RoutedAgent):
    """An agent that uses tools to perform tasks. It executes the tools
    by itself by sending the tool execution task to a ToolAgent."""

    def __init__(
        self,
        description: str,
        system_messages: List[SystemMessage],
        model_client: ChatCompletionClient,
        tool_schema: List[ToolSchema],
        tool_agent_type: AgentType,
    ) -> None:
        super().__init__(description)
        self._model_client = model_client
        self._system_messages = system_messages
        self._tool_schema = tool_schema
        self._tool_agent_id = AgentId(type=tool_agent_type, key=self.id.key)

    @message_handler
    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:
        """Handle a user message, execute the model and tools, and returns the response."""
        session: List[LLMMessage] = [UserMessage(content=message.content, source="User")]
        # Use the tool agent to execute the tools, and get the output messages.
        output_messages = await tool_agent_caller_loop(
            self,
            tool_agent_id=self._tool_agent_id,
            model_client=self._model_client,
            input_messages=session,
            tool_schema=self._tool_schema,
            cancellation_token=ctx.cancellation_token,
        )
        # Extract the final response from the output messages.
        final_response = output_messages[-1].content
        assert isinstance(final_response, str)
        return Message(content=final_response)

```

The tool use agent sends tool call requests to the tool agent to execute tools,
so we can intercept the messages sent by the tool use agent to the tool agent
to prompt the user for permission to execute the tool.

Let’s create an intervention handler that intercepts the messages and prompts
user for before allowing the tool execution.

```
class ToolInterventionHandler(DefaultInterventionHandler):
    async def on_send(
        self, message: Any, *, message_context: MessageContext, recipient: AgentId
    ) -> Any | type[DropMessage]:
        if isinstance(message, FunctionCall):
            # Request user prompt for tool execution.
            user_input = input(
                f"Function call: {message.name}\nArguments: {message.arguments}\nDo you want to execute the tool? (y/n): "
            )
            if user_input.strip().lower() != "y":
                raise ToolException(content="User denied tool execution.", call_id=message.id, name=message.name)
        return message

```

Now, we can create a runtime with the intervention handler registered.

```
# Create the runtime with the intervention handler.
runtime = SingleThreadedAgentRuntime(intervention_handlers=[ToolInterventionHandler()])

```

In this example, we will use a tool for Python code execution.
First, we create a Docker-based command-line code executor
using [`DockerCommandLineCodeExecutor`](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor "autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor"),
and then use it to instantiate a built-in Python code execution tool
`PythonCodeExecutionTool`
that runs code in a Docker container.

```
# Create the docker executor for the Python code execution tool.
docker_executor = DockerCommandLineCodeExecutor()

# Create the Python code execution tool.
python_tool = PythonCodeExecutionTool(executor=docker_executor)

```

Register the agents with tools and tool schema.

```
# Register agents.
tool_agent_type = await ToolAgent.register(
    runtime,
    "tool_executor_agent",
    lambda: ToolAgent(
        description="Tool Executor Agent",
        tools=[python_tool],
    ),
)
model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")
await ToolUseAgent.register(
    runtime,
    "tool_enabled_agent",
    lambda: ToolUseAgent(
        description="Tool Use Agent",
        system_messages=[SystemMessage(content="You are a helpful AI Assistant. Use your tools to solve problems.")],
        model_client=model_client,
        tool_schema=[python_tool.schema],
        tool_agent_type=tool_agent_type,
    ),
)

```

```
AgentType(type='tool_enabled_agent')

```

Run the agents by starting the runtime and sending a message to the tool use agent.
The intervention handler will prompt you for permission to execute the tool.

```
# Start the runtime and the docker executor.
await docker_executor.start()
runtime.start()

# Send a task to the tool user.
response = await runtime.send_message(
    Message("Run the following Python code: print('Hello, World!')"), AgentId("tool_enabled_agent", "default")
)
print(response.content)

# Stop the runtime and the docker executor.
await runtime.stop()
await docker_executor.stop()

# Close the connection to the model client.
await model_client.close()

```

```
The output of the code is: **Hello, World!**

```

##### Extracting Results with an Agent[#](#extracting-results-with-an-agent "Link to this heading")

When running a multi-agent system to solve some task, you may want to extract the result of the system once it has reached termination. This guide showcases one way to achieve this. Given that agent instances are not directly accessible from the outside, we will use an agent to publish the final result to an accessible location.

If you model your system to publish some `FinalResult` type then you can create an agent whose sole job is to subscribe to this and make it available externally. For simple agents like this the `ClosureAgent` is an option to reduce the amount of boilerplate code. This allows you to define a function that will be associated as the agent’s message handler. In this example, we’re going to use a queue shared between the agent and the external code to pass the result.

Note

When considering how to extract results from a multi-agent system, you must always consider the subscriptions of the agent and the topics they publish to.
This is because the agent will only receive messages from topics it is subscribed to.

```
import asyncio
from dataclasses import dataclass

from autogen_core import (
    ClosureAgent,
    ClosureContext,
    DefaultSubscription,
    DefaultTopicId,
    MessageContext,
    SingleThreadedAgentRuntime,
)

```

Define a dataclass for the final result.

```
@dataclass
class FinalResult:
    value: str

```

Create a queue to pass the result from the agent to the external code.

```
queue = asyncio.Queue[FinalResult]()

```

Create a function closure for outputting the final result to the queue.
The function must follow the signature
`Callable[[AgentRuntime, AgentId, T, MessageContext], Awaitable[Any]]`
where `T` is the type of the message the agent will receive.
You can use union types to handle multiple message types.

```
async def output_result(_agent: ClosureContext, message: FinalResult, ctx: MessageContext) -> None:
    await queue.put(message)

```

Let’s create a runtime and register a `ClosureAgent` that will publish the final result to the queue.

```
runtime = SingleThreadedAgentRuntime()
await ClosureAgent.register_closure(
    runtime, "output_result", output_result, subscriptions=lambda: [DefaultSubscription()]
)

```

```
AgentType(type='output_result')

```

We can simulate the collection of final results by publishing them directly to the runtime.

```
runtime.start()
await runtime.publish_message(FinalResult("Result 1"), DefaultTopicId())
await runtime.publish_message(FinalResult("Result 2"), DefaultTopicId())
await runtime.stop_when_idle()

```

We can take a look at the queue to see the final result.

```
while not queue.empty():
    print((result := await queue.get()).value)

```

```
Result 1
Result 2

```

##### OpenAI Assistant Agent[#](#openai-assistant-agent "Link to this heading")

[Open AI Assistant](https://platform.openai.com/docs/assistants/overview)
and [Azure OpenAI Assistant](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/assistant)
are server-side APIs for building
agents.
They can be used to build agents in AutoGen. This cookbook demonstrates how to
to use OpenAI Assistant to create an agent that can run code and Q&A over document.

###### Message Protocol[#](#message-protocol "Link to this heading")

First, we need to specify the message protocol for the agent backed by
OpenAI Assistant. The message protocol defines the structure of messages
handled and published by the agent.
For illustration, we define a simple
message protocol of 4 message types: `Message`, `Reset`, `UploadForCodeInterpreter` and `UploadForFileSearch`.

```
from dataclasses import dataclass

@dataclass
class TextMessage:
    content: str
    source: str

@dataclass
class Reset:
    pass

@dataclass
class UploadForCodeInterpreter:
    file_path: str

@dataclass
class UploadForFileSearch:
    file_path: str
    vector_store_id: str

```

The `TextMessage` message type is used to communicate with the agent. It has a
`content` field that contains the message content, and a `source` field
for the sender. The `Reset` message type is a control message that resets
the memory of the agent. It has no fields. This is useful when we need to
start a new conversation with the agent.

The `UploadForCodeInterpreter` message type is used to upload data files
for the code interpreter and `UploadForFileSearch` message type is used to upload
documents for file search. Both message types have a `file_path` field that contains
the local path to the file to be uploaded.

###### Defining the Agent[#](#defining-the-agent "Link to this heading")

Next, we define the agent class.
The agent class constructor has the following arguments: `description`,
`client`, `assistant_id`, `thread_id`, and `assistant_event_handler_factory`.
The `client` argument is the OpenAI async client object, and the
`assistant_event_handler_factory` is for creating an assistant event handler
to handle OpenAI Assistant events.
This can be used to create streaming output from the assistant.

The agent class has the following message handlers:

* `handle_message`: Handles the `TextMessage` message type, and sends back the
  response from the assistant.
* `handle_reset`: Handles the `Reset` message type, and resets the memory
  of the assistant agent.
* `handle_upload_for_code_interpreter`: Handles the `UploadForCodeInterpreter`
  message type, and uploads the file to the code interpreter.
* `handle_upload_for_file_search`: Handles the `UploadForFileSearch`
  message type, and uploads the document to the file search.

The memory of the assistant is stored inside a thread, which is kept in the
server side. The thread is referenced by the `thread_id` argument.

```
import asyncio
import os
from typing import Any, Callable, List

import aiofiles
from autogen_core import AgentId, MessageContext, RoutedAgent, message_handler
from openai import AsyncAssistantEventHandler, AsyncClient
from openai.types.beta.thread import ToolResources, ToolResourcesFileSearch

class OpenAIAssistantAgent(RoutedAgent):
    """An agent implementation that uses the OpenAI Assistant API to generate
    responses.

    Args:
        description (str): The description of the agent.
        client (openai.AsyncClient): The client to use for the OpenAI API.
        assistant_id (str): The assistant ID to use for the OpenAI API.
        thread_id (str): The thread ID to use for the OpenAI API.
        assistant_event_handler_factory (Callable[[], AsyncAssistantEventHandler], optional):
            A factory function to create an async assistant event handler. Defaults to None.
            If provided, the agent will use the streaming mode with the event handler.
            If not provided, the agent will use the blocking mode to generate responses.
    """

    def __init__(
        self,
        description: str,
        client: AsyncClient,
        assistant_id: str,
        thread_id: str,
        assistant_event_handler_factory: Callable[[], AsyncAssistantEventHandler],
    ) -> None:
        super().__init__(description)
        self._client = client
        self._assistant_id = assistant_id
        self._thread_id = thread_id
        self._assistant_event_handler_factory = assistant_event_handler_factory

    @message_handler
    async def handle_message(self, message: TextMessage, ctx: MessageContext) -> TextMessage:
        """Handle a message. This method adds the message to the thread and publishes a response."""
        # Save the message to the thread.
        await ctx.cancellation_token.link_future(
            asyncio.ensure_future(
                self._client.beta.threads.messages.create(
                    thread_id=self._thread_id,
                    content=message.content,
                    role="user",
                    metadata={"sender": message.source},
                )
            )
        )
        # Generate a response.
        async with self._client.beta.threads.runs.stream(
            thread_id=self._thread_id,
            assistant_id=self._assistant_id,
            event_handler=self._assistant_event_handler_factory(),
        ) as stream:
            await ctx.cancellation_token.link_future(asyncio.ensure_future(stream.until_done()))

        # Get the last message.
        messages = await ctx.cancellation_token.link_future(
            asyncio.ensure_future(self._client.beta.threads.messages.list(self._thread_id, order="desc", limit=1))
        )
        last_message_content = messages.data[0].content

        # Get the text content from the last message.
        text_content = [content for content in last_message_content if content.type == "text"]
        if not text_content:
            raise ValueError(f"Expected text content in the last message: {last_message_content}")

        return TextMessage(content=text_content[0].text.value, source=self.metadata["type"])

    @message_handler()
    async def on_reset(self, message: Reset, ctx: MessageContext) -> None:
        """Handle a reset message. This method deletes all messages in the thread."""
        # Get all messages in this thread.
        all_msgs: List[str] = []
        while True:
            if not all_msgs:
                msgs = await ctx.cancellation_token.link_future(
                    asyncio.ensure_future(self._client.beta.threads.messages.list(self._thread_id))
                )
            else:
                msgs = await ctx.cancellation_token.link_future(
                    asyncio.ensure_future(self._client.beta.threads.messages.list(self._thread_id, after=all_msgs[-1]))
                )
            for msg in msgs.data:
                all_msgs.append(msg.id)
            if not msgs.has_next_page():
                break
        # Delete all the messages.
        for msg_id in all_msgs:
            status = await ctx.cancellation_token.link_future(
                asyncio.ensure_future(
                    self._client.beta.threads.messages.delete(message_id=msg_id, thread_id=self._thread_id)
                )
            )
            assert status.deleted is True

    @message_handler()
    async def on_upload_for_code_interpreter(self, message: UploadForCodeInterpreter, ctx: MessageContext) -> None:
        """Handle an upload for code interpreter. This method uploads a file and updates the thread with the file."""
        # Get the file content.
        async with aiofiles.open(message.file_path, mode="rb") as f:
            file_content = await ctx.cancellation_token.link_future(asyncio.ensure_future(f.read()))
        file_name = os.path.basename(message.file_path)
        # Upload the file.
        file = await ctx.cancellation_token.link_future(
            asyncio.ensure_future(self._client.files.create(file=(file_name, file_content), purpose="assistants"))
        )
        # Get existing file ids from tool resources.
        thread = await ctx.cancellation_token.link_future(
            asyncio.ensure_future(self._client.beta.threads.retrieve(thread_id=self._thread_id))
        )
        tool_resources: ToolResources = thread.tool_resources if thread.tool_resources else ToolResources()
        assert tool_resources.code_interpreter is not None
        if tool_resources.code_interpreter.file_ids:
            file_ids = tool_resources.code_interpreter.file_ids
        else:
            file_ids = [file.id]
        # Update thread with new file.
        await ctx.cancellation_token.link_future(
            asyncio.ensure_future(
                self._client.beta.threads.update(
                    thread_id=self._thread_id,
                    tool_resources={
                        "code_interpreter": {"file_ids": file_ids},
                    },
                )
            )
        )

    @message_handler()
    async def on_upload_for_file_search(self, message: UploadForFileSearch, ctx: MessageContext) -> None:
        """Handle an upload for file search. This method uploads a file and updates the vector store."""
        # Get the file content.
        async with aiofiles.open(message.file_path, mode="rb") as file:
            file_content = await ctx.cancellation_token.link_future(asyncio.ensure_future(file.read()))
        file_name = os.path.basename(message.file_path)
        # Upload the file.
        await ctx.cancellation_token.link_future(
            asyncio.ensure_future(
                self._client.vector_stores.file_batches.upload_and_poll(
                    vector_store_id=message.vector_store_id,
                    files=[(file_name, file_content)],
                )
            )
        )

```

The agent class is a thin wrapper around the OpenAI Assistant API to implement
the message protocol. More features, such as multi-modal message handling,
can be added by extending the message protocol.

###### Assistant Event Handler[#](#assistant-event-handler "Link to this heading")

The assistant event handler provides call-backs for handling Assistant API
specific events. This is useful for handling streaming output from the assistant
and further user interface integration.

```
from openai import AsyncAssistantEventHandler, AsyncClient
from openai.types.beta.threads import Message, Text, TextDelta
from openai.types.beta.threads.runs import RunStep, RunStepDelta
from typing_extensions import override

class EventHandler(AsyncAssistantEventHandler):
    @override
    async def on_text_delta(self, delta: TextDelta, snapshot: Text) -> None:
        print(delta.value, end="", flush=True)

    @override
    async def on_run_step_created(self, run_step: RunStep) -> None:
        details = run_step.step_details
        if details.type == "tool_calls":
            for tool in details.tool_calls:
                if tool.type == "code_interpreter":
                    print("\nGenerating code to interpret:\n\n```python")

    @override
    async def on_run_step_done(self, run_step: RunStep) -> None:
        details = run_step.step_details
        if details.type == "tool_calls":
            for tool in details.tool_calls:
                if tool.type == "code_interpreter":
                    print("\n```\nExecuting code...")

    @override
    async def on_run_step_delta(self, delta: RunStepDelta, snapshot: RunStep) -> None:
        details = delta.step_details
        if details is not None and details.type == "tool_calls":
            for tool in details.tool_calls or []:
                if tool.type == "code_interpreter" and tool.code_interpreter and tool.code_interpreter.input:
                    print(tool.code_interpreter.input, end="", flush=True)

    @override
    async def on_message_created(self, message: Message) -> None:
        print(f"{'-'*80}\nAssistant:\n")

    @override
    async def on_message_done(self, message: Message) -> None:
        # print a citation to the file searched
        if not message.content:
            return
        content = message.content[0]
        if not content.type == "text":
            return
        text_content = content.text
        annotations = text_content.annotations
        citations: List[str] = []
        for index, annotation in enumerate(annotations):
            text_content.value = text_content.value.replace(annotation.text, f"[{index}]")
            if file_citation := getattr(annotation, "file_citation", None):
                client = AsyncClient()
                cited_file = await client.files.retrieve(file_citation.file_id)
                citations.append(f"[{index}] {cited_file.filename}")
        if citations:
            print("\n".join(citations))

```

###### Using the Agent[#](#using-the-agent "Link to this heading")

First we need to use the `openai` client to create the actual assistant,
thread, and vector store. Our AutoGen agent will be using these.

```
import openai

# Create an assistant with code interpreter and file search tools.
oai_assistant = openai.beta.assistants.create(
    model="gpt-4o-mini",
    description="An AI assistant that helps with everyday tasks.",
    instructions="Help the user with their task.",
    tools=[{"type": "code_interpreter"}, {"type": "file_search"}],
)

# Create a vector store to be used for file search.
vector_store = openai.vector_stores.create()

# Create a thread which is used as the memory for the assistant.
thread = openai.beta.threads.create(
    tool_resources={"file_search": {"vector_store_ids": [vector_store.id]}},
)

```

Then, we create a runtime, and register an agent factory function for this
agent with the runtime.

```
from autogen_core import SingleThreadedAgentRuntime

runtime = SingleThreadedAgentRuntime()
await OpenAIAssistantAgent.register(
    runtime,
    "assistant",
    lambda: OpenAIAssistantAgent(
        description="OpenAI Assistant Agent",
        client=openai.AsyncClient(),
        assistant_id=oai_assistant.id,
        thread_id=thread.id,
        assistant_event_handler_factory=lambda: EventHandler(),
    ),
)
agent = AgentId("assistant", "default")

```

Let’s turn on logging to see what’s happening under the hood.

```
import logging

logging.basicConfig(level=logging.WARNING)
logging.getLogger("autogen_core").setLevel(logging.DEBUG)

```

Let’s send a greeting message to the agent, and see the response streamed back.

```
runtime.start()
await runtime.send_message(TextMessage(content="Hello, how are you today!", source="user"), agent)
await runtime.stop_when_idle()

```

```
INFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'Hello, how are you today!', 'source': 'user'}
INFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown

```

```
--------------------------------------------------------------------------------
Assistant:

Hello! I'm here and ready to assist you. How can I help you today?

```

```
INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': "Hello! I'm here and ready to assist you. How can I help you today?", 'source': 'assistant'}

```

###### Assistant with Code Interpreter[#](#assistant-with-code-interpreter "Link to this heading")

Let’s ask some math question to the agent, and see it uses the code interpreter
to answer the question.

```
runtime.start()
await runtime.send_message(TextMessage(content="What is 1332322 x 123212?", source="user"), agent)
await runtime.stop_when_idle()

```

```
INFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'What is 1332322 x 123212?', 'source': 'user'}
INFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown

```

```
# Calculating the product of 1332322 and 123212
result = 1332322 * 123212
result
```
Executing code...
--------------------------------------------------------------------------------
Assistant:

The product of 1,332,322 and 123,212 is 164,158,058,264.

```

```
INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': 'The product of 1,332,322 and 123,212 is 164,158,058,264.', 'source': 'assistant'}

```

Let’s get some data from Seattle Open Data portal. We will be using the
[City of Seattle Wage Data](https://data.seattle.gov/City-Business/City-of-Seattle-Wage-Data/2khk-5ukd/).
Let’s download it first.

```
import requests

response = requests.get("https://data.seattle.gov/resource/2khk-5ukd.csv")
with open("seattle_city_wages.csv", "wb") as file:
    file.write(response.content)

```

Let’s send the file to the agent using an `UploadForCodeInterpreter` message.

```
runtime.start()
await runtime.send_message(UploadForCodeInterpreter(file_path="seattle_city_wages.csv"), agent)
await runtime.stop_when_idle()

```

```
INFO:autogen_core:Sending message of type UploadForCodeInterpreter to assistant: {'file_path': 'seattle_city_wages.csv'}
INFO:autogen_core:Calling message handler for assistant:default with message type UploadForCodeInterpreter sent by Unknown
INFO:autogen_core:Resolving response with message type NoneType for recipient None from assistant: None

```

We can now ask some questions about the data to the agent.

```
runtime.start()
await runtime.send_message(TextMessage(content="Take a look at the uploaded CSV file.", source="user"), agent)
await runtime.stop_when_idle()

```

```
INFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'Take a look at the uploaded CSV file.', 'source': 'user'}
INFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown

```

```
import pandas as pd

# Load the uploaded CSV file to examine its contents
file_path = '/mnt/data/file-oEvRiyGyHc2jZViKyDqL8aoh'
csv_data = pd.read_csv(file_path)

# Display the first few rows of the dataframe to understand its structure
csv_data.head()
```
Executing code...
--------------------------------------------------------------------------------
Assistant:

The uploaded CSV file contains the following columns:

1. **department**: The department in which the individual works.
2. **last_name**: The last name of the employee.
3. **first_name**: The first name of the employee.
4. **job_title**: The job title of the employee.
5. **hourly_rate**: The hourly rate for the employee's position.

Here are the first few entries from the file:

| department                     | last_name | first_name | job_title                          | hourly_rate |
|--------------------------------|-----------|------------|------------------------------------|-------------|
| Police Department              | Aagard    | Lori       | Pol Capt-Precinct                 | 112.70      |
| Police Department              | Aakervik  | Dag        | Pol Ofcr-Detective                | 75.61       |
| Seattle City Light             | Aaltonen  | Evan       | Pwrline Clear Tree Trimmer        | 53.06       |
| Seattle Public Utilities       | Aar       | Abdimallik | Civil Engrng Spec,Sr               | 64.43       |
| Seattle Dept of Transportation | Abad      | Abigail    | Admin Spec II-BU                  | 37.40       |

If you need any specific analysis or information from this data, please let me know!

```

```
INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': "The uploaded CSV file contains the following columns:\n\n1. **department**: The department in which the individual works.\n2. **last_name**: The last name of the employee.\n3. **first_name**: The first name of the employee.\n4. **job_title**: The job title of the employee.\n5. **hourly_rate**: The hourly rate for the employee's position.\n\nHere are the first few entries from the file:\n\n| department                     | last_name | first_name | job_title                          | hourly_rate |\n|--------------------------------|-----------|------------|------------------------------------|-------------|\n| Police Department              | Aagard    | Lori       | Pol Capt-Precinct                 | 112.70      |\n| Police Department              | Aakervik  | Dag        | Pol Ofcr-Detective                | 75.61       |\n| Seattle City Light             | Aaltonen  | Evan       | Pwrline Clear Tree Trimmer        | 53.06       |\n| Seattle Public Utilities       | Aar       | Abdimallik | Civil Engrng Spec,Sr               | 64.43       |\n| Seattle Dept of Transportation | Abad      | Abigail    | Admin Spec II-BU                  | 37.40       |\n\nIf you need any specific analysis or information from this data, please let me know!", 'source': 'assistant'}

```

```
runtime.start()
await runtime.send_message(TextMessage(content="What are the top-10 salaries?", source="user"), agent)
await runtime.stop_when_idle()

```

```
INFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'What are the top-10 salaries?', 'source': 'user'}
INFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown

```

```
# Sorting the data by hourly_rate in descending order and selecting the top 10 salaries
top_10_salaries = csv_data[['first_name', 'last_name', 'job_title', 'hourly_rate']].sort_values(by='hourly_rate', ascending=False).head(10)
top_10_salaries.reset_index(drop=True, inplace=True)
top_10_salaries
```
Executing code...
--------------------------------------------------------------------------------
Assistant:

Here are the top 10 salaries based on the hourly rates from the CSV file:

| First Name | Last Name | Job Title                          | Hourly Rate |
|------------|-----------|------------------------------------|-------------|
| Eric       | Barden    | Executive4                        | 139.61      |
| Idris      | Beauregard| Executive3                        | 115.90      |
| Lori       | Aagard    | Pol Capt-Precinct                 | 112.70      |
| Krista     | Bair      | Pol Capt-Precinct                 | 108.74      |
| Amy        | Bannister | Fire Chief, Dep Adm-80 Hrs        | 104.07      |
| Ginger     | Armbruster| Executive2                        | 102.42      |
| William    | Andersen  | Executive2                        | 102.42      |
| Valarie    | Anderson  | Executive2                        | 102.42      |
| Paige      | Alderete  | Executive2                        | 102.42      |
| Kathryn    | Aisenberg | Executive2                        | 100.65      |

If you need any further details or analysis, let me know!

```

```
INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': 'Here are the top 10 salaries based on the hourly rates from the CSV file:\n\n| First Name | Last Name | Job Title                          | Hourly Rate |\n|------------|-----------|------------------------------------|-------------|\n| Eric       | Barden    | Executive4                        | 139.61      |\n| Idris      | Beauregard| Executive3                        | 115.90      |\n| Lori       | Aagard    | Pol Capt-Precinct                 | 112.70      |\n| Krista     | Bair      | Pol Capt-Precinct                 | 108.74      |\n| Amy        | Bannister | Fire Chief, Dep Adm-80 Hrs        | 104.07      |\n| Ginger     | Armbruster| Executive2                        | 102.42      |\n| William    | Andersen  | Executive2                        | 102.42      |\n| Valarie    | Anderson  | Executive2                        | 102.42      |\n| Paige      | Alderete  | Executive2                        | 102.42      |\n| Kathryn    | Aisenberg | Executive2                        | 100.65      |\n\nIf you need any further details or analysis, let me know!', 'source': 'assistant'}

```

###### Assistant with File Search[#](#assistant-with-file-search "Link to this heading")

Let’s try the Q&A over document feature. We first download Wikipedia page
on the Third Anglo-Afghan War.

```
response = requests.get("https://en.wikipedia.org/wiki/Third_Anglo-Afghan_War")
with open("third_anglo_afghan_war.html", "wb") as file:
    file.write(response.content)

```

Send the file to the agent using an `UploadForFileSearch` message.

```
runtime.start()
await runtime.send_message(
    UploadForFileSearch(file_path="third_anglo_afghan_war.html", vector_store_id=vector_store.id), agent
)
await runtime.stop_when_idle()

```

```
INFO:autogen_core:Sending message of type UploadForFileSearch to assistant: {'file_path': 'third_anglo_afghan_war.html', 'vector_store_id': 'vs_h3xxPbJFnd1iZ9WdjsQwNdrp'}
INFO:autogen_core:Calling message handler for assistant:default with message type UploadForFileSearch sent by Unknown
INFO:autogen_core:Resolving response with message type NoneType for recipient None from assistant: None

```

Let’s ask some questions about the document to the agent. Before asking,
we reset the agent memory to start a new conversation.

```
runtime.start()
await runtime.send_message(Reset(), agent)
await runtime.send_message(
    TextMessage(
        content="When and where was the treaty of Rawalpindi signed? Answer using the document provided.", source="user"
    ),
    agent,
)
await runtime.stop_when_idle()

```

```
INFO:autogen_core:Sending message of type Reset to assistant: {}
INFO:autogen_core:Calling message handler for assistant:default with message type Reset sent by Unknown
INFO:autogen_core:Resolving response with message type NoneType for recipient None from assistant: None
INFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'When and where was the treaty of Rawalpindi signed? Answer using the document provided.', 'source': 'user'}
INFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown

```

```
--------------------------------------------------------------------------------
Assistant:

The Treaty of Rawalpindi was signed on **8 August 1919**. The location of the signing was in **Rawalpindi**, which is in present-day Pakistan【6:0†source】.

```

```
INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': 'The Treaty of Rawalpindi was signed on **8 August 1919**. The location of the signing was in **Rawalpindi**, which is in present-day Pakistan【6:0†source】.', 'source': 'assistant'}

```

```
[0] third_anglo_afghan_war.html

```

That’s it! We have successfully built an agent backed by OpenAI Assistant.

##### Using LangGraph-Backed Agent[#](#using-langgraph-backed-agent "Link to this heading")

This example demonstrates how to create an AI agent using LangGraph.
Based on the example in the LangGraph documentation:
<https://langchain-ai.github.io/langgraph/>.

First install the dependencies:

```
# pip install langgraph langchain-openai azure-identity

```

Let’s import the modules.

```
from dataclasses import dataclass
from typing import Any, Callable, List, Literal

from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import tool  # pyright: ignore
from langchain_openai import AzureChatOpenAI, ChatOpenAI
from langgraph.graph import END, MessagesState, StateGraph
from langgraph.prebuilt import ToolNode

```

Define our message type that will be used to communicate with the agent.

```
@dataclass
class Message:
    content: str

```

Define the tools the agent will use.

```
@tool  # pyright: ignore
def get_weather(location: str) -> str:
    """Call to surf the web."""
    # This is a placeholder, but don't tell the LLM that...
    if "sf" in location.lower() or "san francisco" in location.lower():
        return "It's 60 degrees and foggy."
    return "It's 90 degrees and sunny."

```

Define the agent using LangGraph’s API.

```
class LangGraphToolUseAgent(RoutedAgent):
    def __init__(self, description: str, model: ChatOpenAI, tools: List[Callable[..., Any]]) -> None:  # pyright: ignore
        super().__init__(description)
        self._model = model.bind_tools(tools)  # pyright: ignore

        # Define the function that determines whether to continue or not
        def should_continue(state: MessagesState) -> Literal["tools", END]:  # type: ignore
            messages = state["messages"]
            last_message = messages[-1]
            # If the LLM makes a tool call, then we route to the "tools" node
            if last_message.tool_calls:  # type: ignore
                return "tools"
            # Otherwise, we stop (reply to the user)
            return END

        # Define the function that calls the model
        async def call_model(state: MessagesState):  # type: ignore
            messages = state["messages"]
            response = await self._model.ainvoke(messages)
            # We return a list, because this will get added to the existing list
            return {"messages": [response]}

        tool_node = ToolNode(tools)  # pyright: ignore

        # Define a new graph
        self._workflow = StateGraph(MessagesState)

        # Define the two nodes we will cycle between
        self._workflow.add_node("agent", call_model)  # pyright: ignore
        self._workflow.add_node("tools", tool_node)  # pyright: ignore

        # Set the entrypoint as `agent`
        # This means that this node is the first one called
        self._workflow.set_entry_point("agent")

        # We now add a conditional edge
        self._workflow.add_conditional_edges(
            # First, we define the start node. We use `agent`.
            # This means these are the edges taken after the `agent` node is called.
            "agent",
            # Next, we pass in the function that will determine which node is called next.
            should_continue,  # type: ignore
        )

        # We now add a normal edge from `tools` to `agent`.
        # This means that after `tools` is called, `agent` node is called next.
        self._workflow.add_edge("tools", "agent")

        # Finally, we compile it!
        # This compiles it into a LangChain Runnable,
        # meaning you can use it as you would any other runnable.
        # Note that we're (optionally) passing the memory when compiling the graph
        self._app = self._workflow.compile()

    @message_handler
    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:
        # Use the Runnable
        final_state = await self._app.ainvoke(
            {
                "messages": [
                    SystemMessage(
                        content="You are a helpful AI assistant. You can use tools to help answer questions."
                    ),
                    HumanMessage(content=message.content),
                ]
            },
            config={"configurable": {"thread_id": 42}},
        )
        response = Message(content=final_state["messages"][-1].content)
        return response

```

Now let’s test the agent. First we need to create an agent runtime and
register the agent, by providing the agent’s name and a factory function
that will create the agent.

```
runtime = SingleThreadedAgentRuntime()
await LangGraphToolUseAgent.register(
    runtime,
    "langgraph_tool_use_agent",
    lambda: LangGraphToolUseAgent(
        "Tool use agent",
        ChatOpenAI(
            model="gpt-4o",
            # api_key=os.getenv("OPENAI_API_KEY"),
        ),
        # AzureChatOpenAI(
        #     azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT"),
        #     azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        #     api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
        #     # Using Azure Active Directory authentication.
        #     azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential()),
        #     # Using API key.
        #     # api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        # ),
        [get_weather],
    ),
)
agent = AgentId("langgraph_tool_use_agent", key="default")

```

Start the agent runtime.

```
runtime.start()

```

Send a direct message to the agent, and print the response.

```
response = await runtime.send_message(Message("What's the weather in SF?"), agent)
print(response.content)

```

```
The current weather in San Francisco is 60 degrees and foggy.

```

Stop the agent runtime.

```
await runtime.stop()

```

##### Using LlamaIndex-Backed Agent[#](#using-llamaindex-backed-agent "Link to this heading")

This example demonstrates how to create an AI agent using LlamaIndex.

First install the dependencies:

```
# pip install "llama-index-readers-web" "llama-index-readers-wikipedia" "llama-index-tools-wikipedia" "llama-index-embeddings-azure-openai" "llama-index-llms-azure-openai" "llama-index" "azure-identity"

```

Let’s import the modules.

```
import os
from dataclasses import dataclass
from typing import List, Optional

from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from llama_index.core import Settings
from llama_index.core.agent import ReActAgent
from llama_index.core.agent.runner.base import AgentRunner
from llama_index.core.base.llms.types import (
    ChatMessage,
    MessageRole,
)
from llama_index.core.chat_engine.types import AgentChatResponse
from llama_index.core.memory import ChatSummaryMemoryBuffer
from llama_index.core.memory.types import BaseMemory
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.azure_openai import AzureOpenAI
from llama_index.llms.openai import OpenAI
from llama_index.tools.wikipedia import WikipediaToolSpec

```

Define our message type that will be used to communicate with the agent.

```
@dataclass
class Resource:
    content: str
    node_id: str
    score: Optional[float] = None

@dataclass
class Message:
    content: str
    sources: Optional[List[Resource]] = None

```

Define the agent using LLamaIndex’s API.

```
class LlamaIndexAgent(RoutedAgent):
    def __init__(self, description: str, llama_index_agent: AgentRunner, memory: BaseMemory | None = None) -> None:
        super().__init__(description)

        self._llama_index_agent = llama_index_agent
        self._memory = memory

    @message_handler
    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:
        # retriever history messages from memory!
        history_messages: List[ChatMessage] = []

        response: AgentChatResponse  # pyright: ignore
        if self._memory is not None:
            history_messages = self._memory.get(input=message.content)

            response = await self._llama_index_agent.achat(message=message.content, history_messages=history_messages)  # pyright: ignore
        else:
            response = await self._llama_index_agent.achat(message=message.content)  # pyright: ignore

        if isinstance(response, AgentChatResponse):
            if self._memory is not None:
                self._memory.put(ChatMessage(role=MessageRole.USER, content=message.content))
                self._memory.put(ChatMessage(role=MessageRole.ASSISTANT, content=response.response))

            assert isinstance(response.response, str)

            resources: List[Resource] = [
                Resource(content=source_node.get_text(), score=source_node.score, node_id=source_node.id_)
                for source_node in response.source_nodes
            ]

            tools: List[Resource] = [
                Resource(content=source.content, node_id=source.tool_name) for source in response.sources
            ]

            resources.extend(tools)
            return Message(content=response.response, sources=resources)
        else:
            return Message(content="I'm sorry, I don't have an answer for you.")

```

Setting up LlamaIndex.

```
# llm = AzureOpenAI(
#     deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT"),
#     temperature=0.0,
#     azure_ad_token_provider = get_bearer_token_provider(DefaultAzureCredential()),
#     # api_key=os.getenv("AZURE_OPENAI_API_KEY"),
#     azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
#     api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
# )
llm = OpenAI(
    model="gpt-4o",
    temperature=0.0,
    api_key=os.getenv("OPENAI_API_KEY"),
)

# embed_model = AzureOpenAIEmbedding(
#     deployment_name=os.getenv("AZURE_OPENAI_EMBEDDING_MODEL"),
#     temperature=0.0,
#     azure_ad_token_provider = get_bearer_token_provider(DefaultAzureCredential()),
#     api_key=os.getenv("AZURE_OPENAI_API_KEY"),
#     azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
#     api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
# )
embed_model = OpenAIEmbedding(
    model="text-embedding-ada-002",
    api_key=os.getenv("OPENAI_API_KEY"),
)

Settings.llm = llm
Settings.embed_model = embed_model

```

Create the tools.

```
wiki_spec = WikipediaToolSpec()
wikipedia_tool = wiki_spec.to_tool_list()[1]

```

Now let’s test the agent. First we need to create an agent runtime and
register the agent, by providing the agent’s name and a factory function
that will create the agent.

```
runtime = SingleThreadedAgentRuntime()
await LlamaIndexAgent.register(
    runtime,
    "chat_agent",
    lambda: LlamaIndexAgent(
        description="Llama Index Agent",
        llama_index_agent=ReActAgent.from_tools(
            tools=[wikipedia_tool],
            llm=llm,
            max_iterations=8,
            memory=ChatSummaryMemoryBuffer(llm=llm, token_limit=16000),
            verbose=True,
        ),
    ),
)
agent = AgentId("chat_agent", "default")

```

Start the agent runtime.

```
runtime.start()

```

Send a direct message to the agent, and print the response.

```
message = Message(content="What are the best movies from studio Ghibli?")
response = await runtime.send_message(message, agent)
assert isinstance(response, Message)
print(response.content)

```

```
> Running step 3cbf60cd-9827-4dfe-a3a9-eaff2bed9b75. Step input: What are the best movies from studio Ghibli?
Thought: The current language of the user is: English. I need to use a tool to help me answer the question.
Action: search_data
Action Input: {'query': 'best movies from Studio Ghibli'}
Observation: This is a list of works (films, television, shorts etc.) by the Japanese animation studio Studio Ghibli.

== Works ==

=== Feature films ===

=== Television ===

=== Short films ===

These are short films, including those created for television, theatrical release, and the Ghibli Museum. Original video animation releases and music videos (theatrical and television) are also listed in this section.

=== Commercials ===

=== Video games ===

=== Stage productions ===
Princess Mononoke (2013)
Nausicaä of the Valley of the Wind (2019)
Spirited Away (2022)
My Neighbour Totoro (2022)

=== Other works ===
The works listed here consist of works that do not fall into the above categories. All of these films have been released on DVD or Blu-ray in Japan as part of the Ghibli Gakujutsu Library.

=== Exhibitions ===
A selection of layout designs for animated productions was exhibited in the Studio Ghibli Layout Designs: Understanding the Secrets of Takahata and Miyazaki Animation exhibition tour, which started in the Museum of Contemporary Art Tokyo (July 28, 2008 to September 28, 2008) and subsequently travelled to different museums throughout Japan and Asia, concluding its tour of Japan in the Fukuoka Asian Art Museum (October 12, 2013 to January 26, 2014) and its tour of Asia in the Hong Kong Heritage Museum (May 14, 2014 to August 31, 2014). Between October 4, 2014 and March 1, 2015 the layout designs were exhibited at Art Ludique in Paris. The exhibition catalogues contain annotated reproductions of the displayed artwork.

== Related works ==
These works were not created by Studio Ghibli, but were produced by a variety of studios and people who went on to form or join Studio Ghibli. This includes members of Topcraft that went on to create Studio Ghibli in 1985; works produced by Toei Animation, TMS Entertainment, Nippon Animation or other studios and featuring involvement by Hayao Miyazaki, Isao Takahata or other Ghibli staffers. The list also includes works created in cooperation with Studio Ghibli.

=== Pre-Ghibli ===

=== Cooperative works ===

=== Distributive works ===
These Western animated films (plus one Japanese film) have been distributed by Studio Ghibli, and now through their label, Ghibli Museum Library.

=== Contributive works ===
Studio Ghibli has made contributions to the following anime series and movies:

== Significant achievements ==
The highest-grossing film of 1989 in Japan: Kiki's Delivery Service
The highest-grossing film of 1991 in Japan: Only Yesterday
The highest-grossing film of 1992 in Japan: Porco Rosso
The highest-grossing film of 1994 in Japan: Pom Poko
The highest-grossing film of 1995 in Japan; the first Japanese film in Dolby Digital: Whisper of the Heart
The highest-grossing film of 2002 in Japan: Spirited Away
The highest-grossing film of 2008 in Japan: Ponyo
The highest-grossing Japanese film of 2010 in Japan: The Secret World of Arrietty
The highest-grossing film of 2013 in Japan: The Wind Rises
The first Studio Ghibli film to use computer graphics: Pom Poko
The first Miyazaki feature to use computer graphics, and the first Studio Ghibli film to use digital coloring; the first animated feature in Japan's history to gross more than 10 billion yen at the box office and the first animated film ever to win a National Academy Award for Best Picture of the Year: Princess Mononoke
The first Studio Ghibli film to be shot using a 100% digital process: My Neighbors the Yamadas
The first Miyazaki feature to be shot using a 100% digital process; the first film to gross $200 million worldwide before opening in North America; the film to finally overtake Titanic at the Japanese box office, becoming the top-grossing film in the history of Japanese cinema: Spirited Away
The first anime and traditionally animated winner of the Academy Award for Best Animated Feature: Spirited Away at the 75th Academy Awards. They would later win this award for a second time with The Boy and the Heron at the 96th Academy Awards, marking the second time a traditionally animated film won the award.

== Notes ==

== References ==
> Running step 561e3dd3-d98b-4d37-b612-c99387182ee0. Step input: None
Thought: I can answer without using any more tools. I'll use the user's language to answer.
Answer: Studio Ghibli has produced many acclaimed films over the years. Some of the best and most popular movies from Studio Ghibli include:

1. **Spirited Away (2001)** - Directed by Hayao Miyazaki, this film won the Academy Award for Best Animated Feature and is one of the highest-grossing films in Japanese history.
2. **My Neighbor Totoro (1988)** - Another classic by Hayao Miyazaki, this film is beloved for its heartwarming story and iconic characters.
3. **Princess Mononoke (1997)** - This epic fantasy film, also directed by Miyazaki, is known for its complex themes and stunning animation.
4. **Howl's Moving Castle (2004)** - Based on the novel by Diana Wynne Jones, this film features a magical story and beautiful animation.
5. **Kiki's Delivery Service (1989)** - A charming coming-of-age story about a young witch starting her own delivery service.
6. **Grave of the Fireflies (1988)** - Directed by Isao Takahata, this poignant film is a heartbreaking tale of two siblings struggling to survive during World War II.
7. **Ponyo (2008)** - A delightful and visually stunning film about a young fish-girl who wants to become human.
8. **The Wind Rises (2013)** - A more mature film by Miyazaki, focusing on the life of an aircraft designer during wartime Japan.
9. **The Secret World of Arrietty (2010)** - Based on Mary Norton's novel "The Borrowers," this film tells the story of tiny people living secretly in a human house.
10. **Whisper of the Heart (1995)** - A touching story about a young girl discovering her passion for writing.

These films are celebrated for their storytelling, animation quality, and emotional depth.

```

```
Studio Ghibli has produced many acclaimed films over the years. Some of the best and most popular movies from Studio Ghibli include:

1. **Spirited Away (2001)** - Directed by Hayao Miyazaki, this film won the Academy Award for Best Animated Feature and is one of the highest-grossing films in Japanese history.
2. **My Neighbor Totoro (1988)** - Another classic by Hayao Miyazaki, this film is beloved for its heartwarming story and iconic characters.
3. **Princess Mononoke (1997)** - This epic fantasy film, also directed by Miyazaki, is known for its complex themes and stunning animation.
4. **Howl's Moving Castle (2004)** - Based on the novel by Diana Wynne Jones, this film features a magical story and beautiful animation.
5. **Kiki's Delivery Service (1989)** - A charming coming-of-age story about a young witch starting her own delivery service.
6. **Grave of the Fireflies (1988)** - Directed by Isao Takahata, this poignant film is a heartbreaking tale of two siblings struggling to survive during World War II.
7. **Ponyo (2008)** - A delightful and visually stunning film about a young fish-girl who wants to become human.
8. **The Wind Rises (2013)** - A more mature film by Miyazaki, focusing on the life of an aircraft designer during wartime Japan.
9. **The Secret World of Arrietty (2010)** - Based on Mary Norton's novel "The Borrowers," this film tells the story of tiny people living secretly in a human house.
10. **Whisper of the Heart (1995)** - A touching story about a young girl discovering her passion for writing.

These films are celebrated for their storytelling, animation quality, and emotional depth.

```

```
if response.sources is not None:
    for source in response.sources:
        print(source.content)

```

```
This is a list of works (films, television, shorts etc.) by the Japanese animation studio Studio Ghibli.

== Works ==

=== Feature films ===

=== Television ===

=== Short films ===

These are short films, including those created for television, theatrical release, and the Ghibli Museum. Original video animation releases and music videos (theatrical and television) are also listed in this section.

=== Commercials ===

=== Video games ===

=== Stage productions ===
Princess Mononoke (2013)
Nausicaä of the Valley of the Wind (2019)
Spirited Away (2022)
My Neighbour Totoro (2022)

=== Other works ===
The works listed here consist of works that do not fall into the above categories. All of these films have been released on DVD or Blu-ray in Japan as part of the Ghibli Gakujutsu Library.

=== Exhibitions ===
A selection of layout designs for animated productions was exhibited in the Studio Ghibli Layout Designs: Understanding the Secrets of Takahata and Miyazaki Animation exhibition tour, which started in the Museum of Contemporary Art Tokyo (July 28, 2008 to September 28, 2008) and subsequently travelled to different museums throughout Japan and Asia, concluding its tour of Japan in the Fukuoka Asian Art Museum (October 12, 2013 to January 26, 2014) and its tour of Asia in the Hong Kong Heritage Museum (May 14, 2014 to August 31, 2014). Between October 4, 2014 and March 1, 2015 the layout designs were exhibited at Art Ludique in Paris. The exhibition catalogues contain annotated reproductions of the displayed artwork.

== Related works ==
These works were not created by Studio Ghibli, but were produced by a variety of studios and people who went on to form or join Studio Ghibli. This includes members of Topcraft that went on to create Studio Ghibli in 1985; works produced by Toei Animation, TMS Entertainment, Nippon Animation or other studios and featuring involvement by Hayao Miyazaki, Isao Takahata or other Ghibli staffers. The list also includes works created in cooperation with Studio Ghibli.

=== Pre-Ghibli ===

=== Cooperative works ===

=== Distributive works ===
These Western animated films (plus one Japanese film) have been distributed by Studio Ghibli, and now through their label, Ghibli Museum Library.

=== Contributive works ===
Studio Ghibli has made contributions to the following anime series and movies:

== Significant achievements ==
The highest-grossing film of 1989 in Japan: Kiki's Delivery Service
The highest-grossing film of 1991 in Japan: Only Yesterday
The highest-grossing film of 1992 in Japan: Porco Rosso
The highest-grossing film of 1994 in Japan: Pom Poko
The highest-grossing film of 1995 in Japan; the first Japanese film in Dolby Digital: Whisper of the Heart
The highest-grossing film of 2002 in Japan: Spirited Away
The highest-grossing film of 2008 in Japan: Ponyo
The highest-grossing Japanese film of 2010 in Japan: The Secret World of Arrietty
The highest-grossing film of 2013 in Japan: The Wind Rises
The first Studio Ghibli film to use computer graphics: Pom Poko
The first Miyazaki feature to use computer graphics, and the first Studio Ghibli film to use digital coloring; the first animated feature in Japan's history to gross more than 10 billion yen at the box office and the first animated film ever to win a National Academy Award for Best Picture of the Year: Princess Mononoke
The first Studio Ghibli film to be shot using a 100% digital process: My Neighbors the Yamadas
The first Miyazaki feature to be shot using a 100% digital process; the first film to gross $200 million worldwide before opening in North America; the film to finally overtake Titanic at the Japanese box office, becoming the top-grossing film in the history of Japanese cinema: Spirited Away
The first anime and traditionally animated winner of the Academy Award for Best Animated Feature: Spirited Away at the 75th Academy Awards. They would later win this award for a second time with The Boy and the Heron at the 96th Academy Awards, marking the second time a traditionally animated film won the award.

== Notes ==

== References ==

```

Stop the agent runtime.

```
await runtime.stop()

```

##### Local LLMs with LiteLLM & Ollama[#](#local-llms-with-litellm-ollama "Link to this heading")

In this notebook we’ll create two agents, Joe and Cathy who like to tell jokes to each other. The agents will use locally running LLMs.

Follow the guide at <https://microsoft.github.io/autogen/docs/topics/non-openai-models/local-litellm-ollama/> to understand how to install LiteLLM and Ollama.

We encourage going through the link, but if you’re in a hurry and using Linux, run these:

```
curl -fsSL https://ollama.com/install.sh | sh

ollama pull llama3.2:1b

pip install 'litellm[proxy]'
litellm --model ollama/llama3.2:1b

```

This will run the proxy server and it will be available at ‘<http://0.0.0.0:4000/>’.

To get started, let’s import some classes.

```
from dataclasses import dataclass

from autogen_core import (
    AgentId,
    DefaultTopicId,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    default_subscription,
    message_handler,
)
from autogen_core.model_context import BufferedChatCompletionContext
from autogen_core.models import (
    AssistantMessage,
    ChatCompletionClient,
    SystemMessage,
    UserMessage,
)
from autogen_ext.models.openai import OpenAIChatCompletionClient

```

Set up out local LLM model client.

```
def get_model_client() -> OpenAIChatCompletionClient:  # type: ignore
    "Mimic OpenAI API using Local LLM Server."
    return OpenAIChatCompletionClient(
        model="llama3.2:1b",
        api_key="NotRequiredSinceWeAreLocal",
        base_url="http://0.0.0.0:4000",
        model_capabilities={
            "json_output": False,
            "vision": False,
            "function_calling": True,
        },
    )

```

Define a simple message class

```
@dataclass
class Message:
    content: str

```

Now, the Agent.

We define the role of the Agent using the `SystemMessage` and set up a condition for termination.

```
@default_subscription
class Assistant(RoutedAgent):
    def __init__(self, name: str, model_client: ChatCompletionClient) -> None:
        super().__init__("An assistant agent.")
        self._model_client = model_client
        self.name = name
        self.count = 0
        self._system_messages = [
            SystemMessage(
                content=f"Your name is {name} and you are a part of a duo of comedians."
                "You laugh when you find the joke funny, else reply 'I need to go now'.",
            )
        ]
        self._model_context = BufferedChatCompletionContext(buffer_size=5)

    @message_handler
    async def handle_message(self, message: Message, ctx: MessageContext) -> None:
        self.count += 1
        await self._model_context.add_message(UserMessage(content=message.content, source="user"))
        result = await self._model_client.create(self._system_messages + await self._model_context.get_messages())

        print(f"\n{self.name}: {message.content}")

        if "I need to go".lower() in message.content.lower() or self.count > 2:
            return

        await self._model_context.add_message(AssistantMessage(content=result.content, source="assistant"))  # type: ignore
        await self.publish_message(Message(content=result.content), DefaultTopicId())  # type: ignore

```

Set up the agents.

```
runtime = SingleThreadedAgentRuntime()

model_client = get_model_client()

cathy = await Assistant.register(
    runtime,
    "cathy",
    lambda: Assistant(name="Cathy", model_client=model_client),
)

joe = await Assistant.register(
    runtime,
    "joe",
    lambda: Assistant(name="Joe", model_client=model_client),
)

```

Let’s run everything!

```
runtime.start()
await runtime.send_message(
    Message("Joe, tell me a joke."),
    recipient=AgentId(joe, "default"),
    sender=AgentId(cathy, "default"),
)
await runtime.stop_when_idle()

# Close the connections to the model clients.
await model_client.close()

```

```
/tmp/ipykernel_1417357/2124203426.py:22: UserWarning: Resolved model mismatch: gpt-4o-2024-05-13 != ollama/llama3.1:8b. Model mapping may be incorrect.
  result = await self._model_client.create(self._system_messages + await self._model_context.get_messages())

```

```
Joe: Joe, tell me a joke.

Cathy: Here's one:

Why couldn't the bicycle stand up by itself?

(waiting for your reaction...)

Joe: *laughs* It's because it was two-tired! Ahahaha! That's a good one! I love it!

Cathy: *roars with laughter* HAHAHAHA! Oh man, that's a classic! I'm glad you liked it! The setup is perfect and the punchline is just... *chuckles* Two-tired! I mean, come on! That's genius! We should definitely add that one to our act!

Joe: I need to go now.

```

##### Instrumentating your code locally[#](#instrumentating-your-code-locally "Link to this heading")

AutoGen supports instrumenting your code using [OpenTelemetry](https://opentelemetry.io). This allows you to collect traces and logs from your code and send them to a backend of your choice.

While debugging, you can use a local backend such as [Aspire](https://aspiredashboard.com/) or [Jaeger](https://www.jaegertracing.io/). In this guide we will use Aspire as an example.

###### Setting up Aspire[#](#setting-up-aspire "Link to this heading")

Follow the instructions [here](https://learn.microsoft.com/en-us/dotnet/aspire/fundamentals/dashboard/overview?tabs=bash#standalone-mode) to set up Aspire in standalone mode. This will require Docker to be installed on your machine.

###### Instrumenting your code[#](#instrumenting-your-code "Link to this heading")

Once you have a dashboard set up, now it’s a matter of sending traces and logs to it. You can follow the steps in the [Telemetry Guide](#document-user-guide/core-user-guide/framework/telemetry) to set up the opentelemetry sdk and exporter.

After instrumenting your code with the Aspire Dashboard running, you should see traces and logs appear in the dashboard as your code runs.

###### Observing LLM calls using Open AI[#](#observing-llm-calls-using-open-ai "Link to this heading")

If you are using the Open AI package, you can observe the LLM calls by setting up the opentelemetry for that library. We use [opentelemetry-instrumentation-openai](https://pypi.org/project/opentelemetry-instrumentation-openai/) in this example.

Install the package:

```
pip install opentelemetry-instrumentation-openai

```

Enable the instrumentation:

```
from opentelemetry.instrumentation.openai import OpenAIInstrumentor

OpenAIInstrumentor().instrument()

```

Now running your code will send traces including the LLM calls to your telemetry backend (Aspire in our case).

![Open AI Telemetry logs](_images/open-ai-telemetry-example.png)

##### Topic and Subscription Example Scenarios[#](#topic-and-subscription-example-scenarios "Link to this heading")

###### Introduction[#](#introduction "Link to this heading")

In this cookbook, we explore how broadcasting works for agent communication in AutoGen using four different broadcasting scenarios. These scenarios illustrate various ways to handle and distribute messages among agents. We’ll use a consistent example of a tax management company processing client requests to demonstrate each scenario.

###### Scenario Overview[#](#scenario-overview "Link to this heading")

Imagine a tax management company that offers various services to clients, such as tax planning, dispute resolution, compliance, and preparation. The company employs a team of tax specialists, each with expertise in one of these areas, and a tax system manager who oversees the operations.

Clients submit requests that need to be processed by the appropriate specialists. The communication between the clients, the tax system manager, and the tax specialists is handled through broadcasting in this system.

We’ll explore how different broadcasting scenarios affect the way messages are distributed among agents and how they can be used to tailor the communication flow to specific needs.

---

###### Broadcasting Scenarios Overview[#](#broadcasting-scenarios-overview "Link to this heading")

We will cover the following broadcasting scenarios:

1. **Single-Tenant, Single Scope of Publishing**
2. **Multi-Tenant, Single Scope of Publishing**
3. **Single-Tenant, Multiple Scopes of Publishing**
4. **Multi-Tenant, Multiple Scopes of Publishing**

Each scenario represents a different approach to message distribution and agent interaction within the system. By understanding these scenarios, you can design agent communication strategies that best fit your application’s requirements.

```
import asyncio
from dataclasses import dataclass
from enum import Enum
from typing import List

from autogen_core import (
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    TopicId,
    TypeSubscription,
    message_handler,
)
from autogen_core._default_subscription import DefaultSubscription
from autogen_core._default_topic import DefaultTopicId
from autogen_core.models import (
    SystemMessage,
)

```

```
class TaxSpecialty(str, Enum):
    PLANNING = "planning"
    DISPUTE_RESOLUTION = "dispute_resolution"
    COMPLIANCE = "compliance"
    PREPARATION = "preparation"

@dataclass
class ClientRequest:
    content: str

@dataclass
class RequestAssessment:
    content: str

class TaxSpecialist(RoutedAgent):
    def __init__(
        self,
        description: str,
        specialty: TaxSpecialty,
        system_messages: List[SystemMessage],
    ) -> None:
        super().__init__(description)
        self.specialty = specialty
        self._system_messages = system_messages
        self._memory: List[ClientRequest] = []

    @message_handler
    async def handle_message(self, message: ClientRequest, ctx: MessageContext) -> None:
        # Process the client request.
        print(f"\n{'='*50}\nTax specialist {self.id} with specialty {self.specialty}:\n{message.content}")
        # Send a response back to the manager
        if ctx.topic_id is None:
            raise ValueError("Topic ID is required for broadcasting")
        await self.publish_message(
            message=RequestAssessment(content=f"I can handle this request in {self.specialty}."),
            topic_id=ctx.topic_id,
        )

```

###### 1. Single-Tenant, Single Scope of Publishing[#](#single-tenant-single-scope-of-publishing "Link to this heading")

###### Scenarios Explanation[#](#scenarios-explanation "Link to this heading")

In the single-tenant, single scope of publishing scenario:

* All agents operate within a single tenant (e.g., one client or user session).
* Messages are published to a single topic, and all agents subscribe to this topic.
* Every agent receives every message that gets published to the topic.

This scenario is suitable for situations where all agents need to be aware of all messages, and there’s no need to isolate communication between different groups of agents or sessions.

###### Application in the Tax Specialist Company[#](#application-in-the-tax-specialist-company "Link to this heading")

In our tax specialist company, this scenario implies:

* All tax specialists receive every client request and internal message.
* All agents collaborate closely, with full visibility of all communications.
* Useful for tasks or teams where all agents need to be aware of all messages.

###### How the Scenario Works[#](#how-the-scenario-works "Link to this heading")

* Subscriptions: All agents use the default subscription(e.g., “default”).
* Publishing: Messages are published to the default topic.
* Message Handling: Each agent decides whether to act on a message based on its content and available handlers.

###### Benefits[#](#benefits "Link to this heading")

* Simplicity: Easy to set up and understand.
* Collaboration: Promotes transparency and collaboration among agents.
* Flexibility: Agents can dynamically decide which messages to process.

###### Considerations[#](#considerations "Link to this heading")

* Scalability: May not scale well with a large number of agents or messages.
* Efficiency: Agents may receive many irrelevant messages, leading to unnecessary processing.

```
async def run_single_tenant_single_scope() -> None:
    # Create the runtime.
    runtime = SingleThreadedAgentRuntime()

    # Register TaxSpecialist agents for each specialty
    specialist_agent_type_1 = "TaxSpecialist_1"
    specialist_agent_type_2 = "TaxSpecialist_2"
    await TaxSpecialist.register(
        runtime=runtime,
        type=specialist_agent_type_1,
        factory=lambda: TaxSpecialist(
            description="A tax specialist 1",
            specialty=TaxSpecialty.PLANNING,
            system_messages=[SystemMessage(content="You are a tax specialist.")],
        ),
    )

    await TaxSpecialist.register(
        runtime=runtime,
        type=specialist_agent_type_2,
        factory=lambda: TaxSpecialist(
            description="A tax specialist 2",
            specialty=TaxSpecialty.DISPUTE_RESOLUTION,
            system_messages=[SystemMessage(content="You are a tax specialist.")],
        ),
    )

    # Add default subscriptions for each agent type
    await runtime.add_subscription(DefaultSubscription(agent_type=specialist_agent_type_1))
    await runtime.add_subscription(DefaultSubscription(agent_type=specialist_agent_type_2))

    # Start the runtime and send a message to agents on default topic
    runtime.start()
    await runtime.publish_message(ClientRequest("I need to have my tax for 2024 prepared."), topic_id=DefaultTopicId())
    await runtime.stop_when_idle()

await run_single_tenant_single_scope()

```

```
==================================================
Tax specialist TaxSpecialist_1:default with specialty TaxSpecialty.PLANNING:
I need to have my tax for 2024 prepared.

==================================================
Tax specialist TaxSpecialist_2:default with specialty TaxSpecialty.DISPUTE_RESOLUTION:
I need to have my tax for 2024 prepared.

```

###### 2. Multi-Tenant, Single Scope of Publishing[#](#multi-tenant-single-scope-of-publishing "Link to this heading")

###### Scenario Explanation[#](#scenario-explanation "Link to this heading")

In the multi-tenant, single scope of publishing scenario:

* There are multiple tenants (e.g., multiple clients or user sessions).
* Each tenant has its own isolated topic through the topic source.
* All agents within a tenant subscribe to the tenant’s topic. If needed, new agent instances are created for each tenant.
* Messages are only visible to agents within the same tenant.

This scenario is useful when you need to isolate communication between different tenants but want all agents within a tenant to be aware of all messages.

###### Application in the Tax Specialist Company[#](#id1 "Link to this heading")

In this scenario:

* The company serves multiple clients (tenants) simultaneously.
* For each client, a dedicated set of agent instances is created.
* Each client’s communication is isolated from others.
* All agents for a client receive messages published to that client’s topic.

###### How the Scenario Works[#](#id2 "Link to this heading")

* Subscriptions: Agents subscribe to topics based on the tenant’s identity.
* Publishing: Messages are published to the tenant-specific topic.
* Message Handling: Agents only receive messages relevant to their tenant.

###### Benefits[#](#id3 "Link to this heading")

* Tenant Isolation: Ensures data privacy and separation between clients.
* Collaboration Within Tenant: Agents can collaborate freely within their tenant.

###### Considerations[#](#id4 "Link to this heading")

* Complexity: Requires managing multiple sets of agents and topics.
* Resource Usage: More agent instances may consume additional resources.

```
async def run_multi_tenant_single_scope() -> None:
    # Create the runtime
    runtime = SingleThreadedAgentRuntime()

    # List of clients (tenants)
    tenants = ["ClientABC", "ClientXYZ"]

    # Initialize sessions and map the topic type to each TaxSpecialist agent type
    for specialty in TaxSpecialty:
        specialist_agent_type = f"TaxSpecialist_{specialty.value}"
        await TaxSpecialist.register(
            runtime=runtime,
            type=specialist_agent_type,
            factory=lambda specialty=specialty: TaxSpecialist(  # type: ignore
                description=f"A tax specialist in {specialty.value}.",
                specialty=specialty,
                system_messages=[SystemMessage(content=f"You are a tax specialist in {specialty.value}.")],
            ),
        )
        specialist_subscription = DefaultSubscription(agent_type=specialist_agent_type)
        await runtime.add_subscription(specialist_subscription)

    # Start the runtime
    runtime.start()

    # Publish client requests to their respective topics
    for tenant in tenants:
        topic_source = tenant  # The topic source is the client name
        topic_id = DefaultTopicId(source=topic_source)
        await runtime.publish_message(
            ClientRequest(f"{tenant} requires tax services."),
            topic_id=topic_id,
        )

    # Allow time for message processing
    await asyncio.sleep(1)

    # Stop the runtime when idle
    await runtime.stop_when_idle()

await run_multi_tenant_single_scope()

```

```
==================================================
Tax specialist TaxSpecialist_planning:ClientABC with specialty TaxSpecialty.PLANNING:
ClientABC requires tax services.

==================================================
Tax specialist TaxSpecialist_dispute_resolution:ClientABC with specialty TaxSpecialty.DISPUTE_RESOLUTION:
ClientABC requires tax services.

==================================================
Tax specialist TaxSpecialist_compliance:ClientABC with specialty TaxSpecialty.COMPLIANCE:
ClientABC requires tax services.

==================================================
Tax specialist TaxSpecialist_preparation:ClientABC with specialty TaxSpecialty.PREPARATION:
ClientABC requires tax services.

==================================================
Tax specialist TaxSpecialist_planning:ClientXYZ with specialty TaxSpecialty.PLANNING:
ClientXYZ requires tax services.

==================================================
Tax specialist TaxSpecialist_dispute_resolution:ClientXYZ with specialty TaxSpecialty.DISPUTE_RESOLUTION:
ClientXYZ requires tax services.

==================================================
Tax specialist TaxSpecialist_compliance:ClientXYZ with specialty TaxSpecialty.COMPLIANCE:
ClientXYZ requires tax services.

==================================================
Tax specialist TaxSpecialist_preparation:ClientXYZ with specialty TaxSpecialty.PREPARATION:
ClientXYZ requires tax services.

```

###### 3. Single-Tenant, Multiple Scopes of Publishing[#](#single-tenant-multiple-scopes-of-publishing "Link to this heading")

###### Scenario Explanation[#](#id5 "Link to this heading")

In the single-tenant, multiple scopes of publishing scenario:

* All agents operate within a single tenant.
* Messages are published to different topics.
* Agents subscribe to specific topics relevant to their role or specialty.
* Messages are directed to subsets of agents based on the topic.

This scenario allows for targeted communication within a tenant, enabling more granular control over message distribution.

###### Application in the Tax Management Company[#](#application-in-the-tax-management-company "Link to this heading")

In this scenario:

* The tax system manager communicates with specific specialists based on their specialties.
* Different topics represent different specialties (e.g., “planning”, “compliance”).
* Specialists subscribe only to the topic that matches their specialty.
* The manager publishes messages to specific topics to reach the intended specialists.

###### How the Scenario Works[#](#id6 "Link to this heading")

* Subscriptions: Agents subscribe to topics corresponding to their specialties.
* Publishing: Messages are published to topics based on the intended recipients.
* Message Handling: Only agents subscribed to a topic receive its messages.

###### Benefits[#](#id7 "Link to this heading")

* Targeted Communication: Messages reach only the relevant agents.
* Efficiency: Reduces unnecessary message processing by agents.

###### Considerations[#](#id8 "Link to this heading")

* Setup Complexity: Requires careful management of topics and subscriptions.
* Flexibility: Changes in communication scenarios may require updating subscriptions.

```
async def run_single_tenant_multiple_scope() -> None:
    # Create the runtime
    runtime = SingleThreadedAgentRuntime()
    # Register TaxSpecialist agents for each specialty and add subscriptions
    for specialty in TaxSpecialty:
        specialist_agent_type = f"TaxSpecialist_{specialty.value}"
        await TaxSpecialist.register(
            runtime=runtime,
            type=specialist_agent_type,
            factory=lambda specialty=specialty: TaxSpecialist(  # type: ignore
                description=f"A tax specialist in {specialty.value}.",
                specialty=specialty,
                system_messages=[SystemMessage(content=f"You are a tax specialist in {specialty.value}.")],
            ),
        )
        specialist_subscription = TypeSubscription(topic_type=specialty.value, agent_type=specialist_agent_type)
        await runtime.add_subscription(specialist_subscription)

    # Start the runtime
    runtime.start()

    # Publish a ClientRequest to each specialist's topic
    for specialty in TaxSpecialty:
        topic_id = TopicId(type=specialty.value, source="default")
        await runtime.publish_message(
            ClientRequest(f"I need assistance with {specialty.value} taxes."),
            topic_id=topic_id,
        )

    # Allow time for message processing
    await asyncio.sleep(1)

    # Stop the runtime when idle
    await runtime.stop_when_idle()

await run_single_tenant_multiple_scope()

```

```
==================================================
Tax specialist TaxSpecialist_planning:default with specialty TaxSpecialty.PLANNING:
I need assistance with planning taxes.

==================================================
Tax specialist TaxSpecialist_dispute_resolution:default with specialty TaxSpecialty.DISPUTE_RESOLUTION:
I need assistance with dispute_resolution taxes.

==================================================
Tax specialist TaxSpecialist_compliance:default with specialty TaxSpecialty.COMPLIANCE:
I need assistance with compliance taxes.

==================================================
Tax specialist TaxSpecialist_preparation:default with specialty TaxSpecialty.PREPARATION:
I need assistance with preparation taxes.

```

###### 4. Multi-Tenant, Multiple Scopes of Publishing[#](#multi-tenant-multiple-scopes-of-publishing "Link to this heading")

###### Scenario Explanation[#](#id9 "Link to this heading")

In the multi-tenant, multiple scopes of publishing scenario:

* There are multiple tenants, each with their own set of agents.
* Messages are published to multiple topics within each tenant.
* Agents subscribe to tenant-specific topics relevant to their role.
* Combines tenant isolation with targeted communication.

This scenario provides the highest level of control over message distribution, suitable for complex systems with multiple clients and specialized communication needs.

###### Application in the Tax Management Company[#](#id10 "Link to this heading")

In this scenario:

* The company serves multiple clients, each with dedicated agent instances.
* Within each client, agents communicate using multiple topics based on specialties.
* For example, Client A’s planning specialist subscribes to the “planning” topic with source “ClientA”.
* The tax system manager for each client communicates with their specialists using tenant-specific topics.

###### How the Scenario Works[#](#id11 "Link to this heading")

* Subscriptions: Agents subscribe to topics based on both tenant identity and specialty.
* Publishing: Messages are published to tenant-specific and specialty-specific topics.
* Message Handling: Only agents matching the tenant and topic receive messages.

###### Benefits[#](#id12 "Link to this heading")

* Complete Isolation: Ensures both tenant and communication isolation.
* Granular Control: Enables precise routing of messages to intended agents.

###### Considerations[#](#id13 "Link to this heading")

* Complexity: Requires careful management of topics, tenants, and subscriptions.
* Resource Usage: Increased number of agent instances and topics may impact resources.

```
async def run_multi_tenant_multiple_scope() -> None:
    # Create the runtime
    runtime = SingleThreadedAgentRuntime()

    # Define TypeSubscriptions for each specialty and tenant
    tenants = ["ClientABC", "ClientXYZ"]

    # Initialize agents for all specialties and add type subscriptions
    for specialty in TaxSpecialty:
        specialist_agent_type = f"TaxSpecialist_{specialty.value}"
        await TaxSpecialist.register(
            runtime=runtime,
            type=specialist_agent_type,
            factory=lambda specialty=specialty: TaxSpecialist(  # type: ignore
                description=f"A tax specialist in {specialty.value}.",
                specialty=specialty,
                system_messages=[SystemMessage(content=f"You are a tax specialist in {specialty.value}.")],
            ),
        )
        for tenant in tenants:
            specialist_subscription = TypeSubscription(
                topic_type=f"{tenant}_{specialty.value}", agent_type=specialist_agent_type
            )
            await runtime.add_subscription(specialist_subscription)

    # Start the runtime
    runtime.start()

    # Send messages for each tenant to each specialty
    for tenant in tenants:
        for specialty in TaxSpecialty:
            topic_id = TopicId(type=f"{tenant}_{specialty.value}", source=tenant)
            await runtime.publish_message(
                ClientRequest(f"{tenant} needs assistance with {specialty.value} taxes."),
                topic_id=topic_id,
            )

    # Allow time for message processing
    await asyncio.sleep(1)

    # Stop the runtime when idle
    await runtime.stop_when_idle()

await run_multi_tenant_multiple_scope()

```

```
==================================================
Tax specialist TaxSpecialist_planning:ClientABC with specialty TaxSpecialty.PLANNING:
ClientABC needs assistance with planning taxes.

==================================================
Tax specialist TaxSpecialist_dispute_resolution:ClientABC with specialty TaxSpecialty.DISPUTE_RESOLUTION:
ClientABC needs assistance with dispute_resolution taxes.

==================================================
Tax specialist TaxSpecialist_compliance:ClientABC with specialty TaxSpecialty.COMPLIANCE:
ClientABC needs assistance with compliance taxes.

==================================================
Tax specialist TaxSpecialist_preparation:ClientABC with specialty TaxSpecialty.PREPARATION:
ClientABC needs assistance with preparation taxes.

==================================================
Tax specialist TaxSpecialist_planning:ClientXYZ with specialty TaxSpecialty.PLANNING:
ClientXYZ needs assistance with planning taxes.

==================================================
Tax specialist TaxSpecialist_dispute_resolution:ClientXYZ with specialty TaxSpecialty.DISPUTE_RESOLUTION:
ClientXYZ needs assistance with dispute_resolution taxes.

==================================================
Tax specialist TaxSpecialist_compliance:ClientXYZ with specialty TaxSpecialty.COMPLIANCE:
ClientXYZ needs assistance with compliance taxes.

==================================================
Tax specialist TaxSpecialist_preparation:ClientXYZ with specialty TaxSpecialty.PREPARATION:
ClientXYZ needs assistance with preparation taxes.

```

##### Structured output using GPT-4o models[#](#structured-output-using-gpt-4o-models "Link to this heading")

This cookbook demonstrates how to obtain structured output using GPT-4o models. The OpenAI beta client SDK provides a parse helper that allows you to use your own Pydantic model, eliminating the need to define a JSON schema. This approach is recommended for supported models.

Currently, this feature is supported for:

* gpt-4o-mini on OpenAI
* gpt-4o-2024-08-06 on OpenAI
* gpt-4o-2024-08-06 on Azure

Let’s define a simple message type that carries explanation and output for a Math problem

```
from pydantic import BaseModel

class MathReasoning(BaseModel):
    class Step(BaseModel):
        explanation: str
        output: str

    steps: list[Step]
    final_answer: str

```

```
import os

# Set the environment variable
os.environ["AZURE_OPENAI_ENDPOINT"] = "https://YOUR_ENDPOINT_DETAILS.openai.azure.com/"
os.environ["AZURE_OPENAI_API_KEY"] = "YOUR_API_KEY"
os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"] = "gpt-4o-2024-08-06"
os.environ["AZURE_OPENAI_API_VERSION"] = "2024-08-01-preview"

```

```
import json
import os
from typing import Optional

from autogen_core.models import UserMessage
from autogen_ext.models.openai import AzureOpenAIChatCompletionClient

# Function to get environment variable and ensure it is not None
def get_env_variable(name: str) -> str:
    value = os.getenv(name)
    if value is None:
        raise ValueError(f"Environment variable {name} is not set")
    return value

# Create the client with type-checked environment variables
client = AzureOpenAIChatCompletionClient(
    azure_deployment=get_env_variable("AZURE_OPENAI_DEPLOYMENT_NAME"),
    model=get_env_variable("AZURE_OPENAI_MODEL"),
    api_version=get_env_variable("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=get_env_variable("AZURE_OPENAI_ENDPOINT"),
    api_key=get_env_variable("AZURE_OPENAI_API_KEY"),
)

```

```
# Define the user message
messages = [
    UserMessage(content="What is 16 + 32?", source="user"),
]

# Call the create method on the client, passing the messages and additional arguments
# The extra_create_args dictionary includes the response format as MathReasoning model we defined above
# Providing the response format and pydantic model will use the new parse method from beta SDK
response = await client.create(messages=messages, extra_create_args={"response_format": MathReasoning})

# Ensure the response content is a valid JSON string before loading it
response_content: Optional[str] = response.content if isinstance(response.content, str) else None
if response_content is None:
    raise ValueError("Response content is not a valid JSON string")

# Print the response content after loading it as JSON
print(json.loads(response_content))

# Validate the response content with the MathReasoning model
MathReasoning.model_validate(json.loads(response_content))

```

```
{'steps': [{'explanation': 'Start by aligning the numbers vertically.', 'output': '\n  16\n+ 32'}, {'explanation': 'Add the units digits: 6 + 2 = 8.', 'output': '\n  16\n+ 32\n   8'}, {'explanation': 'Add the tens digits: 1 + 3 = 4.', 'output': '\n  16\n+ 32\n  48'}], 'final_answer': '48'}

```

```
MathReasoning(steps=[Step(explanation='Start by aligning the numbers vertically.', output='\n  16\n+ 32'), Step(explanation='Add the units digits: 6 + 2 = 8.', output='\n  16\n+ 32\n   8'), Step(explanation='Add the tens digits: 1 + 3 = 4.', output='\n  16\n+ 32\n  48')], final_answer='48')

```

##### Tracking LLM usage with a logger[#](#tracking-llm-usage-with-a-logger "Link to this heading")

The model clients included in AutoGen emit structured events that can be used to track the usage of the model. This notebook demonstrates how to use the logger to track the usage of the model.

These events are logged to the logger with the name: :py:attr:`autogen_core.EVENT_LOGGER_NAME`.

```
import logging

from autogen_core.logging import LLMCallEvent

class LLMUsageTracker(logging.Handler):
    def __init__(self) -> None:
        """Logging handler that tracks the number of tokens used in the prompt and completion."""
        super().__init__()
        self._prompt_tokens = 0
        self._completion_tokens = 0

    @property
    def tokens(self) -> int:
        return self._prompt_tokens + self._completion_tokens

    @property
    def prompt_tokens(self) -> int:
        return self._prompt_tokens

    @property
    def completion_tokens(self) -> int:
        return self._completion_tokens

    def reset(self) -> None:
        self._prompt_tokens = 0
        self._completion_tokens = 0

    def emit(self, record: logging.LogRecord) -> None:
        """Emit the log record. To be used by the logging module."""
        try:
            # Use the StructuredMessage if the message is an instance of it
            if isinstance(record.msg, LLMCallEvent):
                event = record.msg
                self._prompt_tokens += event.prompt_tokens
                self._completion_tokens += event.completion_tokens
        except Exception:
            self.handleError(record)

```

Then, this logger can be attached like any other Python logger and the values read after the model is run.

```
from autogen_core import EVENT_LOGGER_NAME

# Set up the logging configuration to use the custom handler
logger = logging.getLogger(EVENT_LOGGER_NAME)
logger.setLevel(logging.INFO)
llm_usage = LLMUsageTracker()
logger.handlers = [llm_usage]

# client.create(...)

print(llm_usage.prompt_tokens)
print(llm_usage.completion_tokens)

```

### FAQs[#](#faqs "Link to this heading")

#### How do I get the underlying agent instance?[#](#how-do-i-get-the-underlying-agent-instance "Link to this heading")

Agents might be distributed across multiple machines, so the underlying agent instance is intentionally discouraged from being accessed. If the agent is definitely running on the same machine, you can access the agent instance by calling [`autogen_core.AgentRuntime.try_get_underlying_agent_instance()`](#autogen_core.AgentRuntime.try_get_underlying_agent_instance "autogen_core.AgentRuntime.try_get_underlying_agent_instance") on the `AgentRuntime`. If the agent is not available this will throw an exception.

#### How do I call call a function on an agent?[#](#how-do-i-call-call-a-function-on-an-agent "Link to this heading")

Since the instance itself is not accessible, you can’t call a function on an agent directly. Instead, you should create a type to represent the function call and its arguments, and then send that message to the agent. Then in the agent, create a handler for that message type and implement the required logic. This also supports returning a response to the caller.

This allows your agent to work in a distributed environment a well as a local one.

#### Why do I need to use a factory to register an agent?[#](#why-do-i-need-to-use-a-factory-to-register-an-agent "Link to this heading")

An [`autogen_core.AgentId`](#autogen_core.AgentId "autogen_core.AgentId") is composed of a `type` and a `key`. The type corresponds to the factory that created the agent, and the key is a runtime, data dependent key for this instance.

The key can correspond to a user id, a session id, or could just be “default” if you don’t need to differentiate between instances. Each unique key will create a new instance of the agent, based on the factory provided. This allows the system to automatically scale to different instances of the same agent, and to manage the lifecycle of each instance independently based on how you choose to handle keys in your application.

#### How do I increase the GRPC message size?[#](#how-do-i-increase-the-grpc-message-size "Link to this heading")

If you need to provide custom gRPC options, such as overriding the `max_send_message_length` and `max_receive_message_length`, you can define an `extra_grpc_config` variable and pass it to both the `GrpcWorkerAgentRuntimeHost` and `GrpcWorkerAgentRuntime` instances.

```
# Define custom gRPC options
extra_grpc_config = [
    ("grpc.max_send_message_length", new_max_size),
    ("grpc.max_receive_message_length", new_max_size),
]

# Create instances of GrpcWorkerAgentRuntimeHost and GrpcWorkerAgentRuntime with the custom gRPC options

host = GrpcWorkerAgentRuntimeHost(address=host_address, extra_grpc_config=extra_grpc_config)
worker1 = GrpcWorkerAgentRuntime(host_address=host_address, extra_grpc_config=extra_grpc_config)

```

**Note**: When `GrpcWorkerAgentRuntime` creates a host connection for the clients, it uses `DEFAULT_GRPC_CONFIG` from `HostConnection` class as default set of values which will can be overriden if you pass parameters with the same name using `extra_grpc_config`.

#### What are model capabilities and how do I specify them?[#](#what-are-model-capabilities-and-how-do-i-specify-them "Link to this heading")

Model capabilites are additional capabilities an LLM may have beyond the standard natural language features. There are currently 3 additional capabilities that can be specified within Autogen

* vision: The model is capable of processing and interpreting image data.
* function\_calling: The model has the capacity to accept function descriptions; such as the function name, purpose, input parameters, etc; and can respond with an appropriate function to call including any necessary parameters.
* json\_output: The model is capable of outputting responses to conform with a specified json format.

Model capabilities can be passed into a model, which will override the default definitions. These capabilities will not affect what the underlying model is actually capable of, but will allow or disallow behaviors associated with them. This is particularly useful when [using local LLMs](#document-user-guide/core-user-guide/cookbook/local-llms-ollama-litellm).

```
from autogen_ext.models.openai import OpenAIChatCompletionClient

client = OpenAIChatCompletionClient(
    model="gpt-4o",
    api_key="YourApiKey",
    model_capabilities={
        "vision": True,
        "function_calling": False,
        "json_output": False,
    }
)

```

AutoGen core offers an easy way to quickly build event-driven, distributed, scalable, resilient AI agent systems. Agents are developed by using the [Actor model](https://en.wikipedia.org/wiki/Actor_model). You can build and run your agent system locally and easily move to a distributed system in the cloud when you are ready.

Key features of AutoGen core include:

Asynchronous Messaging

Agents communicate through asynchronous messages, enabling event-driven and request/response communication models.

Scalable & Distributed

Enable complex scenarios with networks of agents across organizational boundaries.

Multi-Language Support

Python & Dotnet interoperating agents today, with more languages coming soon.

Modular & Extensible

Highly customizable with features like custom agents, memory as a service, tools registry, and model library.

Observable & Debuggable

Easily trace and debug your agent systems.

Event-Driven Architecture

Build event-driven, distributed, scalable, and resilient AI agent systems.

## Extensions[#](#extensions "Link to this heading")

### Installation[#](#installation "Link to this heading")

First-part maintained extensions are available in the `autogen-ext` package.

```
pip install "autogen-ext"

```

Extras:

* `langchain` needed for [`LangChainToolAdapter`](#autogen_ext.tools.langchain.LangChainToolAdapter "autogen_ext.tools.langchain.LangChainToolAdapter")
* `azure` needed for [`ACADynamicSessionsCodeExecutor`](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor "autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor")
* `docker` needed for [`DockerCommandLineCodeExecutor`](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor "autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor")
* `openai` needed for [`OpenAIChatCompletionClient`](#autogen_ext.models.openai.OpenAIChatCompletionClient "autogen_ext.models.openai.OpenAIChatCompletionClient")

### Discover community projects[#](#discover-community-projects "Link to this heading")

Ecosystem

Find samples, services and other things that work with AutoGen

<https://github.com/topics/autogen>

Community Extensions

Find AutoGen extensions for 3rd party tools, components and services

<https://github.com/topics/autogen-extension>

Community Samples

Find community samples and examples of how to use AutoGen

<https://github.com/topics/autogen-sample>

#### List of community projects[#](#list-of-community-projects "Link to this heading")

| Name | Package | Description |
| --- | --- | --- |
| [autogen-watsonx-client](https://github.com/tsinggggg/autogen-watsonx-client) | [PyPi](https://pypi.org/project/autogen-watsonx-client/) | Model client for [IBM watsonx.ai](https://www.ibm.com/products/watsonx-ai) |
| [autogen-openaiext-client](https://github.com/vballoli/autogen-openaiext-client) | [PyPi](https://pypi.org/project/autogen-openaiext-client/) | Model client for other LLMs like Gemini, etc. through the OpenAI API |
| [autogen-ext-mcp](https://github.com/richard-gyiko/autogen-ext-mcp) | [PyPi](https://pypi.org/project/autogen-ext-mcp/) | Tool adapter for Model Context Protocol server tools |

### Creating your own extension[#](#creating-your-own-extension "Link to this heading")

With the new package structure in 0.4, it is easier than ever to create and publish your own extension to the AutoGen ecosystem. This page details some best practices so that your extension package integrates well with the AutoGen ecosystem.

#### Best practices[#](#best-practices "Link to this heading")

##### Naming[#](#naming "Link to this heading")

There is no requirement about naming. But prefixing the package name with `autogen-` makes it easier to find.

##### Common interfaces[#](#common-interfaces "Link to this heading")

Whenever possible, extensions should implement the provided interfaces from the `autogen_core` package. This will allow for a more consistent experience for users.

###### Dependency on AutoGen[#](#dependency-on-autogen "Link to this heading")

To ensure that the extension works with the version of AutoGen that it was designed for, it is recommended to specify the version of AutoGen the dependency section of the `pyproject.toml` with adequate constraints.

```
[project]
# ...
dependencies = [
    "autogen-core>=0.4,<0.5"
]

```

##### Usage of typing[#](#usage-of-typing "Link to this heading")

AutoGen embraces the use of type hints to provide a better development experience. Extensions should use type hints whenever possible.

#### Discovery[#](#discovery "Link to this heading")

To make it easier for users to find your extension, sample, service or package, you can [add the topic](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/classifying-your-repository-with-topics) [`autogen`](https://github.com/topics/autogen) to the GitHub repo.

More specific topics are also available:

* [`autogen-extension`](https://github.com/topics/autogen-extension) for extensions
* [`autogen-sample`](https://github.com/topics/autogen-sample) for samples

#### Changes from 0.2[#](#changes-from-0-2 "Link to this heading")

In AutoGen 0.2 it was common to merge 3rd party extensions and examples into the main repo. We are super appreciative of all of the users who have contributed to the ecosystem notebooks, modules and pages in 0.2. However, in general we are moving away from this model to allow for more flexibility and to reduce maintenance burden.

There is the `autogen-ext` package for 1st party supported extensions, but we want to be selective to manage maintenance load. If you would like to see if your extension makes sense to add into `autogen-ext`, please open an issue and let’s discuss. Otherwise, we encourage you to publish your extension as a separate package and follow the guidance under [discovery](#discovery) to make it easy for users to find.

### ACA Dynamic Sessions Code Executor[#](#aca-dynamic-sessions-code-executor "Link to this heading")

This guide will explain the Azure Container Apps dynamic sessions in Azure Container Apps and show you how to use the Azure Container Code Executor class.

The [Azure Container Apps dynamic sessions](https://learn.microsoft.com/en-us/azure/container-apps/sessions) is a component in the Azure Container Apps service. The environment is hosted on remote Azure instances and will not execute any code locally. The interpreter is capable of executing python code in a jupyter environment with a pre-installed base of commonly used packages. [Custom environments](https://learn.microsoft.com/en-us/azure/container-apps/sessions-custom-container) can be created by users for their applications. Files can additionally be [uploaded to, or downloaded from](https://learn.microsoft.com/en-us/azure/container-apps/sessions-code-interpreter#upload-a-file-to-a-session) each session.

The code interpreter can run multiple sessions of code, each of which are delineated by a session identifier string.

#### Create a Container Apps Session Pool[#](#create-a-container-apps-session-pool "Link to this heading")

In your Azure portal, create a new `Container App Session Pool` resource with the pool type set to `Python code interpreter` and note the `Pool management endpoint`. The format for the endpoint should be something like `https://{region}.dynamicsessions.io/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/sessionPools/{session_pool_name}`.

Alternatively, you can use the [Azure CLI to create a session pool.](https://learn.microsoft.com/en-us/azure/container-apps/sessions-code-interpreter#create-a-session-pool-with-azure-cli)

#### ACADynamicSessionsCodeExecutor[#](#acadynamicsessionscodeexecutor "Link to this heading")

The [`ACADynamicSessionsCodeExecutor`](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor "autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor") class is a python code executor that creates and executes arbitrary python code on a default Serverless code interpreter session. Its interface is as follows

##### Initialization[#](#initialization "Link to this heading")

First, you will need to find or create a credentialing object that implements the [`TokenProvider`](#autogen_ext.code_executors.azure.TokenProvider "autogen_ext.code_executors.azure.TokenProvider") interface. This is any object that implements the following function

```
def get_token(
    self, *scopes: str, claims: Optional[str] = None, tenant_id: Optional[str] = None, **kwargs: Any
) -> azure.core.credentials.AccessToken

```

An example of such an object is the [azure.identity.DefaultAzureCredential](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) class.

Lets start by installing that

```
# pip install azure.identity

```

Next, lets import all the necessary modules and classes for our code

```
import os
import tempfile

from anyio import open_file
from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.azure import ACADynamicSessionsCodeExecutor
from azure.identity import DefaultAzureCredential

```

Now, we create our Azure code executor and run some test code along with verification that it ran correctly. We’ll create the executor with a temporary working directory to ensure a clean environment as we show how to use each feature

```
cancellation_token = CancellationToken()
POOL_MANAGEMENT_ENDPOINT = "..."

with tempfile.TemporaryDirectory() as temp_dir:
    executor = ACADynamicSessionsCodeExecutor(
        pool_management_endpoint=POOL_MANAGEMENT_ENDPOINT, credential=DefaultAzureCredential(), work_dir=temp_dir
    )

    code_blocks = [CodeBlock(code="import sys; print('hello world!')", language="python")]
    code_result = await executor.execute_code_blocks(code_blocks, cancellation_token)
    assert code_result.exit_code == 0 and "hello world!" in code_result.output

```

Next, lets try uploading some files and verifying their integrity. All files uploaded to the Serverless code interpreter is uploaded into the `/mnt/data` directory. All downloadable files must also be placed in the directory. By default, the current working directory for the code executor is set to `/mnt/data`.

```
with tempfile.TemporaryDirectory() as temp_dir:
    test_file_1 = "test_upload_1.txt"
    test_file_1_contents = "test1 contents"
    test_file_2 = "test_upload_2.txt"
    test_file_2_contents = "test2 contents"

    async with await open_file(os.path.join(temp_dir, test_file_1), "w") as f:  # type: ignore[syntax]
        await f.write(test_file_1_contents)
    async with await open_file(os.path.join(temp_dir, test_file_2), "w") as f:  # type: ignore[syntax]
        await f.write(test_file_2_contents)

    assert os.path.isfile(os.path.join(temp_dir, test_file_1))
    assert os.path.isfile(os.path.join(temp_dir, test_file_2))

    executor = ACADynamicSessionsCodeExecutor(
        pool_management_endpoint=POOL_MANAGEMENT_ENDPOINT, credential=DefaultAzureCredential(), work_dir=temp_dir
    )
    await executor.upload_files([test_file_1, test_file_2], cancellation_token)

    file_list = await executor.get_file_list(cancellation_token)
    assert test_file_1 in file_list
    assert test_file_2 in file_list

    code_blocks = [
        CodeBlock(
            code=f"""
with open("{test_file_1}") as f:
  print(f.read())
with open("{test_file_2}") as f:
  print(f.read())
""",
            language="python",
        )
    ]
    code_result = await executor.execute_code_blocks(code_blocks, cancellation_token)
    assert code_result.exit_code == 0
    assert test_file_1_contents in code_result.output
    assert test_file_2_contents in code_result.output

```

Downloading files works in a similar way.

```
with tempfile.TemporaryDirectory() as temp_dir:
    test_file_1 = "test_upload_1.txt"
    test_file_1_contents = "test1 contents"
    test_file_2 = "test_upload_2.txt"
    test_file_2_contents = "test2 contents"

    assert not os.path.isfile(os.path.join(temp_dir, test_file_1))
    assert not os.path.isfile(os.path.join(temp_dir, test_file_2))

    executor = ACADynamicSessionsCodeExecutor(
        pool_management_endpoint=POOL_MANAGEMENT_ENDPOINT, credential=DefaultAzureCredential(), work_dir=temp_dir
    )

    code_blocks = [
        CodeBlock(
            code=f"""
with open("{test_file_1}", "w") as f:
  f.write("{test_file_1_contents}")
with open("{test_file_2}", "w") as f:
  f.write("{test_file_2_contents}")
""",
            language="python",
        ),
    ]
    code_result = await executor.execute_code_blocks(code_blocks, cancellation_token)
    assert code_result.exit_code == 0

    file_list = await executor.get_file_list(cancellation_token)
    assert test_file_1 in file_list
    assert test_file_2 in file_list

    await executor.download_files([test_file_1, test_file_2], cancellation_token)

    assert os.path.isfile(os.path.join(temp_dir, test_file_1))
    async with await open_file(os.path.join(temp_dir, test_file_1), "r") as f:  # type: ignore[syntax]
        content = await f.read()
        assert test_file_1_contents in content
    assert os.path.isfile(os.path.join(temp_dir, test_file_2))
    async with await open_file(os.path.join(temp_dir, test_file_2), "r") as f:  # type: ignore[syntax]
        content = await f.read()
        assert test_file_2_contents in content

```

##### New Sessions[#](#new-sessions "Link to this heading")

Every instance of the [`ACADynamicSessionsCodeExecutor`](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor "autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor") class will have a unique session ID. Every call to a particular code executor will be executed on the same session until the [`restart()`](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.restart "autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.restart") function is called on it. Previous sessions cannot be reused.

Here we’ll run some code on the code session, restart it, then verify that a new session has been opened.

```
executor = ACADynamicSessionsCodeExecutor(
    pool_management_endpoint=POOL_MANAGEMENT_ENDPOINT, credential=DefaultAzureCredential()
)

code_blocks = [CodeBlock(code="x = 'abcdefg'", language="python")]
code_result = await executor.execute_code_blocks(code_blocks, cancellation_token)
assert code_result.exit_code == 0

code_blocks = [CodeBlock(code="print(x)", language="python")]
code_result = await executor.execute_code_blocks(code_blocks, cancellation_token)
assert code_result.exit_code == 0 and "abcdefg" in code_result.output

await executor.restart()
code_blocks = [CodeBlock(code="print(x)", language="python")]
code_result = await executor.execute_code_blocks(code_blocks, cancellation_token)
assert code_result.exit_code != 0 and "NameError" in code_result.output

```

##### Available Packages[#](#available-packages "Link to this heading")

Each code execution instance is pre-installed with most of the commonly used packages. However, the list of available packages and versions are not available outside of the execution environment. The packages list on the environment can be retrieved by calling the [`get_available_packages()`](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.get_available_packages "autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.get_available_packages") function on the code executor.

```
print(executor.get_available_packages(cancellation_token))

```

AutoGen is designed to be extensible. The `autogen-ext` package contains the built-in component implementations maintained by the AutoGen project.

Examples of components include:

* `autogen_ext.agents.*` for agent implementations like [`MultimodalWebSurfer`](#autogen_ext.agents.web_surfer.MultimodalWebSurfer "autogen_ext.agents.web_surfer.MultimodalWebSurfer")
* `autogen_ext.models.*` for model clients like [`OpenAIChatCompletionClient`](#autogen_ext.models.openai.OpenAIChatCompletionClient "autogen_ext.models.openai.OpenAIChatCompletionClient") and [`SKChatCompletionAdapter`](#autogen_ext.models.semantic_kernel.SKChatCompletionAdapter "autogen_ext.models.semantic_kernel.SKChatCompletionAdapter") for connecting to hosted and local models.
* `autogen_ext.tools.*` for tools like GraphRAG [`LocalSearchTool`](#autogen_ext.tools.graphrag.LocalSearchTool "autogen_ext.tools.graphrag.LocalSearchTool") and [`mcp_server_tools()`](#autogen_ext.tools.mcp.mcp_server_tools "autogen_ext.tools.mcp.mcp_server_tools").
* `autogen_ext.executors.*` for executors like [`DockerCommandLineCodeExecutor`](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor "autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor") and [`ACADynamicSessionsCodeExecutor`](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor "autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor")
* `autogen_ext.runtimes.*` for agent runtimes like [`GrpcWorkerAgentRuntime`](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime "autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime")

See [API Reference](#document-reference/index) for the full list of components and their APIs.

We strongly encourage developers to build their own components and publish them as part of the ecosytem.

Discover

Discover community extensions and samples

<./discover.html>

Create your own

Create your own extension

<./create-your-own.html>

## AutoGen Studio[#](#autogen-studio "Link to this heading")

[![PyPI version](https://badge.fury.io/py/autogenstudio.svg)](https://badge.fury.io/py/autogenstudio)
[![Downloads](https://static.pepy.tech/badge/autogenstudio/week)](https://pepy.tech/project/autogenstudio)

AutoGen Studio is a low-code interface built to help you rapidly prototype AI agents, enhance them with tools, compose them into teams and interact with them to accomplish tasks. It is built on [AutoGen AgentChat](https://microsoft.github.io/autogen) - a high-level API for building multi-agent applications.

> See a video tutorial on AutoGen Studio v0.4 (02/25) - <https://youtu.be/oum6EI7wohM>

[![A Friendly Introduction to AutoGen Studio v0.4](https://img.youtube.com/vi/oum6EI7wohM/maxresdefault.jpg)](https://www.youtube.com/watch?v=oum6EI7wohM)

Code for AutoGen Studio is on GitHub at [microsoft/autogen](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-studio)

Caution

AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app. Developers are encouraged to use the AutoGen framework to build their own applications, implementing authentication, security and other features required for deployed applications.

### Capabilities - What Can You Do with AutoGen Studio?[#](#capabilities-what-can-you-do-with-autogen-studio "Link to this heading")

AutoGen Studio offers four main interfaces to help you build and manage multi-agent systems:

1. **Team Builder**

   * A visual interface for creating agent teams through declarative specification (JSON) or drag-and-drop
   * Supports configuration of all core components: teams, agents, tools, models, and termination conditions
   * Fully compatible with AgentChat’s component definitions
2. **Playground**

   * Interactive environment for testing and running agent teams
   * Features include:

     + Live message streaming between agents
     + Visual representation of message flow through a control transition graph
     + Interactive sessions with teams using UserProxyAgent
     + Full run control with the ability to pause or stop execution
3. **Gallery**

   * Central hub for discovering and importing community-created components
   * Enables easy integration of third-party components
4. **Deployment**

   * Export and run teams in python code
   * Setup and test endpoints based on a team configuration
   * Run teams in a docker container

#### Roadmap[#](#roadmap "Link to this heading")

Review project roadmap and issues [here](https://github.com/microsoft/autogen/issues/4006) .

### Contribution Guide[#](#contribution-guide "Link to this heading")

We welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:

* Review the overall AutoGen project [contribution guide](https://github.com/microsoft/autogen/blob/main/CONTRIBUTING.md)
* Please review the AutoGen Studio [roadmap](https://github.com/microsoft/autogen/issues/4006) to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with `help-wanted`
* Please use the tag [`proj-studio`](https://github.com/microsoft/autogen/issues?q=is%3Aissue%20state%3Aopen%20label%3Aproj-studio) tag for any issues, questions, and PRs related to Studio
* Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.
* Submit a pull request with your contribution!
* If you are modifying AutoGen Studio, it has its own devcontainer. See instructions in `.devcontainer/README.md` to use it

### A Note on Security[#](#a-note-on-security "Link to this heading")

AutoGen Studio is a research prototype and is **not meant to be used** in a production environment. Some baseline practices are encouraged e.g., using Docker code execution environment for your agents.

However, other considerations such as rigorous tests related to jailbreaking, ensuring LLMs only have access to the right keys of data given the end user’s permissions, and other security features are not implemented in AutoGen Studio.

If you are building a production application, please use the AutoGen framework and implement the necessary security features.

### Acknowledgements and Citation[#](#acknowledgements-and-citation "Link to this heading")

AutoGen Studio is based on the [AutoGen](https://microsoft.github.io/autogen) project. It was adapted from a research prototype built in October 2023 (original credits: Victor Dibia, Gagan Bansal, Adam Fourney, Piali Choudhury, Saleema Amershi, Ahmed Awadallah, Chi Wang).

If you use AutoGen Studio in your research, please cite the following paper:

```
@inproceedings{autogenstudio,
  title={AUTOGEN STUDIO: A No-Code Developer Tool for Building and Debugging Multi-Agent Systems},
  author={Dibia, Victor and Chen, Jingya and Bansal, Gagan and Syed, Suff and Fourney, Adam and Zhu, Erkang and Wang, Chi and Amershi, Saleema},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={72--79},
  year={2024}
}

```

### Next Steps[#](#next-steps "Link to this heading")

To begin, follow the [installation instructions](#document-user-guide/autogenstudio-user-guide/installation) to install AutoGen Studio.

#### Installation[#](#installation "Link to this heading")

There are two ways to install AutoGen Studio - from PyPi or from source. We **recommend installing from PyPi** unless you plan to modify the source code.

##### Create a Virtual Environment (Recommended)[#](#create-a-virtual-environment-recommended "Link to this heading")

We recommend using a virtual environment as this will ensure that the dependencies for AutoGen Studio are isolated from the rest of your system.

venv

Create and activate:

```
python3 -m venv .venv
source .venv/bin/activate

```

To deactivate later, run:

```
deactivate

```

conda

[Install Conda](https://docs.conda.io/projects/conda/en/stable/user-guide/install/index.html) if you have not already.

Create and activate:

```
conda create -n autogen python=3.10
conda activate autogen

```

To deactivate later, run:

```
conda deactivate

```

##### Install from PyPi (Recommended)[#](#install-from-pypi-recommended "Link to this heading")

You can install AutoGen Studio using pip, the Python package manager.

```
pip install -U autogenstudio

```

##### Install from source[#](#install-from-source "Link to this heading")

*Note: This approach requires some familiarity with building interfaces in React.*

You have two options for installing from source: manually or using a dev container.

###### A) Install from source manually[#](#a-install-from-source-manually "Link to this heading")

1. Ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed.
2. Clone the AutoGen Studio repository and install its Python dependencies using `pip install -e .`
3. Navigate to the `python/packages/autogen-studio/frontend` directory, install the dependencies, and build the UI:

```
npm install -g gatsby-cli
npm install --global yarn
cd frontend
yarn install
yarn build
# Windows users may need alternative commands to build the frontend:
gatsby clean && rmdir /s /q ..\\autogenstudio\\web\\ui 2>nul & (set \"PREFIX_PATH_VALUE=\" || ver>nul) && gatsby build --prefix-paths && xcopy /E /I /Y public ..\\autogenstudio\\web\\ui

```

###### B) Install from source using a dev container[#](#b-install-from-source-using-a-dev-container "Link to this heading")

1. Follow the [Dev Containers tutorial](https://code.visualstudio.com/docs/devcontainers/tutorial) to install VS Code, Docker and relevant extensions.
2. Clone the AutoGen Studio repository.
3. Open `python/packages/autogen-studio/`in VS Code. Click the blue button in bottom the corner or press F1 and select *“Dev Containers: Reopen in Container”*.
4. Build the UI:

```
cd frontend
yarn build

```

##### Running the Application[#](#running-the-application "Link to this heading")

Once installed, run the web UI by entering the following in your terminal:

```
autogenstudio ui --port 8081

```

This command will start the application on the specified port. Open your web browser and go to <http://localhost:8081/> to use AutoGen Studio.

AutoGen Studio also takes several parameters to customize the application:

* `--host <host>` argument to specify the host address. By default, it is set to `localhost`.
* `--appdir <appdir>` argument to specify the directory where the app files (e.g., database and generated user files) are stored. By default, it is set to the `.autogenstudio` directory in the user’s home directory.
* `--port <port>` argument to specify the port number. By default, it is set to `8080`.
* `--reload` argument to enable auto-reloading of the server when changes are made to the code. By default, it is set to `False`.
* `--database-uri` argument to specify the database URI. Example values include `sqlite:///database.sqlite` for SQLite and `postgresql+psycopg://user:password@localhost/dbname` for PostgreSQL. If this is not specified, the database URL defaults to a `database.sqlite` file in the `--appdir` directory.
* `--upgrade-database` argument to upgrade the database schema to the latest version. By default, it is set to `False`.

Now that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills.

#### Usage[#](#usage "Link to this heading")

AutoGen Studio (AGS) provides a Team Builder interface where developers can define multiple components and behaviors. Users can create teams, add agents to teams, attach tools and models to agents, and define team termination conditions.
After defining a team, users can test directly in the team builder view or attach it to a session for use in the Playground view.

> See a video tutorial on AutoGen Studio v0.4 (02/25) - <https://youtu.be/oum6EI7wohM>

[![A Friendly Introduction to AutoGen Studio v0.4](https://img.youtube.com/vi/oum6EI7wohM/maxresdefault.jpg)](https://www.youtube.com/watch?v=oum6EI7wohM)

##### Setting Up an API Key[#](#setting-up-an-api-key "Link to this heading")

Most of your agents will require an API key. You can set up an environment variable `OPENAI_API_KEY` (assuming you are using OpenAI models) and AutoGen will automatically use this for any OpenAI model clients you specify for your agents or teams. Alternatively you can specify the api key as part of the team or agent configuration.

See the section below on how to build an agent team either using the visual builder or by directly editing the JSON configuration.

##### Building an Agent Team[#](#building-an-agent-team "Link to this heading")

##### Declarative Specification of Componenents[#](#declarative-specification-of-componenents "Link to this heading")

AutoGen Studio is built on the declarative specification behaviors of AutoGen AgentChat. This allows users to define teams, agents, models, tools, and termination conditions in Python and then dump them into a JSON file for use in AutoGen Studio.

Here’s an example of an agent team and how it is converted to a JSON file:

```
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.conditions import  TextMentionTermination

agent = AssistantAgent(
        name="weather_agent",
        model_client=OpenAIChatCompletionClient(
            model="gpt-4o-mini",
        ),
    )

agent_team = RoundRobinGroupChat([agent], termination_condition=TextMentionTermination("TERMINATE"))
config = agent_team.dump_component()
print(config.model_dump_json())

```

```
{
  "provider": "autogen_agentchat.teams.RoundRobinGroupChat",
  "component_type": "team",
  "version": 1,
  "component_version": 1,
  "description": "A team that runs a group chat with participants taking turns in a round-robin fashion\n    to publish a message to all.",
  "label": "RoundRobinGroupChat",
  "config": {
    "participants": [
      {
        "provider": "autogen_agentchat.agents.AssistantAgent",
        "component_type": "agent",
        "version": 1,
        "component_version": 1,
        "description": "An agent that provides assistance with tool use.",
        "label": "AssistantAgent",
        "config": {
          "name": "weather_agent",
          "model_client": {
            "provider": "autogen_ext.models.openai.OpenAIChatCompletionClient",
            "component_type": "model",
            "version": 1,
            "component_version": 1,
            "description": "Chat completion client for OpenAI hosted models.",
            "label": "OpenAIChatCompletionClient",
            "config": { "model": "gpt-4o-mini" }
          },
          "tools": [],
          "handoffs": [],
          "model_context": {
            "provider": "autogen_core.model_context.UnboundedChatCompletionContext",
            "component_type": "chat_completion_context",
            "version": 1,
            "component_version": 1,
            "description": "An unbounded chat completion context that keeps a view of the all the messages.",
            "label": "UnboundedChatCompletionContext",
            "config": {}
          },
          "description": "An agent that provides assistance with ability to use tools.",
          "system_message": "You are a helpful AI assistant. Solve tasks using your tools. Reply with TERMINATE when the task has been completed.",
          "model_client_stream": false,
          "reflect_on_tool_use": false,
          "tool_call_summary_format": "{result}"
        }
      }
    ],
    "termination_condition": {
      "provider": "autogen_agentchat.conditions.TextMentionTermination",
      "component_type": "termination",
      "version": 1,
      "component_version": 1,
      "description": "Terminate the conversation if a specific text is mentioned.",
      "label": "TextMentionTermination",
      "config": { "text": "TERMINATE" }
    }
  }
}

```

This example shows a team with a single agent, using the `RoundRobinGroupChat` type and a `TextMentionTermination` condition. You will also notice that the model client is an `OpenAIChatCompletionClient` model client where only the model name is specified. In this case, the API key is assumed to be set as an environment variable `OPENAI_API_KEY`. You can also specify the API key as part of the model client configuration.

To understand the full configuration of an model clients, you can refer to the [AutoGen Model Clients documentation](https://microsoft.github.io/autogen/dev/user-guide/core-user-guide/components/model-clients.html).

Note that you can similarly define your model client in Python and call `dump_component()` on it to get the JSON configuration and use it to update the model client section of your team or agent configuration.

Finally, you can use the `load_component()` method to load a team configuration from a JSON file:

```

import json
from autogen_agentchat.teams import BaseGroupChat
team_config = json.load(open("team.json"))
team = BaseGroupChat.load_component(team_config)

```

##### Gallery - Sharing and Reusing Components[#](#gallery-sharing-and-reusing-components "Link to this heading")

AGS provides a Gallery view, where a gallery is a collection of components - teams, agents, models, tools, and terminations - that can be shared and reused across projects.

Users can create a local gallery or import a gallery (from a URL, a JSON file import or simply by copying and pasting the JSON). At any given time, users can select any of the current Gallery items as a **default gallery**. This **default gallery** will be used to populate the Team Builder sidebar with components.

* Create new galleries via Gallery -> New Gallery
* Edit gallery JSON as needed
* Set a **default** gallery (click pin icon in sidebar) to make components available in Team Builder.

##### Interactively Running Teams[#](#interactively-running-teams "Link to this heading")

The AutoGen Studio Playground enables users to:

* Test teams on specific tasks
* Review generated artifacts (images, code, text)
* Monitor team “inner monologue” during task execution
* View performance metrics (turn count, token usage)
* Track agent actions (tool usage, code execution results)

##### Importing and Reusing Team Configurations[#](#importing-and-reusing-team-configurations "Link to this heading")

AutoGen Studio’s Gallery view offers a default component collection and supports importing external configurations:

* Create/Import galleries through Gallery -> New Gallery -> Import
* Set default galleries via sidebar pin icon
* Access components in Team Builder through Sidebar -> From Gallery

###### Python Integration[#](#python-integration "Link to this heading")

Team configurations can be integrated into Python applications using the `TeamManager` class:

```
from autogenstudio.teammanager import TeamManager

tm = TeamManager()
result_stream = tm.run(task="What is the weather in New York?", team_config="team.json") # or tm.run_stream(..)

```

To export team configurations, use the export button in Team Builder to generate a JSON file for Python application use.

#### Experimental Features[#](#experimental-features "Link to this heading")

##### Authentication[#](#authentication "Link to this heading")

AutoGen Studio offers an experimental authentication feature to enable personalized experiences (multiple users). Currently, only GitHub authentication is supported. You can extend the base authentication class to add support for other authentication methods.

By default authenticatio is disabled and only enabled when you pass in the `--auth-config` argument when running the application.

###### Enable GitHub Authentication[#](#enable-github-authentication "Link to this heading")

To enable GitHub authentication, create a `auth.yaml` file in your app directory:

```
type: github
jwt_secret: "your-secret-key" # keep secure!
token_expiry_minutes: 60
github:
  client_id: "your-github-client-id"
  client_secret: "your-github-client-secret"
  callback_url: "http://localhost:8081/api/auth/callback"
  scopes: ["user:email"]

```

Note

**JWT Secret**

* Generate a strong, unique JWT secret (at least 32 random bytes). You can run `openssl rand -hex 32` to generate a secure random key.
* Never commit your JWT secret to version control
* In production, store secrets in environment variables or secure secret management services
* Regularly rotate your JWT secret to limit the impact of potential breaches

**Callback URL**

* The callback URL is the URL that GitHub will redirect to after the user has authenticated. It should match the URL you set in your GitHub OAuth application settings.
* Ensure that the callback URL is accessible from the internet if you are running AutoGen Studio on a remote server.

Please see the documentation on [GitHub OAuth](https://docs.github.com/en/apps/oauth-apps/building-oauth-apps/authenticating-to-the-rest-api-with-an-oauth-app) for more details on obtaining the `client_id` and `client_secret`.

To pass in this configuration you can use the `--auth-config` argument when running the application:

```
autogenstudio ui --auth-config /path/to/auth.yaml

```

Or set the environment variable:

```
export AUTOGENSTUDIO_AUTH_CONFIG="/path/to/auth.yaml"

```

Note

* Authentication is currently experimental and may change in future releases
* User data is stored in your configured database
* When enabled, all API endpoints require authentication except for the authentication endpoints
* WebSocket connections require the token to be passed as a query parameter (`?token=your-jwt-token`)

#### FAQ[#](#faq "Link to this heading")

##### Q: How do I specify the directory where files(e.g. database) are stored?[#](#q-how-do-i-specify-the-directory-where-files-e-g-database-are-stored "Link to this heading")

A: You can specify the directory where files are stored by setting the `--appdir` argument when running the application. For example, `autogenstudio ui --appdir /path/to/folder`. This will store the database (default) and other files in the specified directory e.g. `/path/to/folder/database.sqlite`.

##### Q: Can I use other models with AutoGen Studio?[#](#q-can-i-use-other-models-with-autogen-studio "Link to this heading")

Yes. AutoGen standardizes on the openai model api format, and you can use any api server that offers an openai compliant endpoint.

AutoGen Studio is based on declaritive specifications which applies to models as well. Agents can include a model\_client field which specifies the model endpoint details including `model`, `api_key`, `base_url`, `model type`. Note, you can define your [model client](https://microsoft.github.io/autogen/dev/user-guide/core-user-guide/components/model-clients.html) in python and dump it to a json file for use in AutoGen Studio.

In the following sample, we will define an OpenAI, AzureOpenAI and a local model client in python and dump them to a json file.

```
from autogen_ext.models.openai import AzureOpenAIChatCompletionClient, OpenAIChatCompletionClient
from autogen_ext.models.anthropic import AnthropicChatCompletionClient
from autogen_core.models import ModelInfo

model_client=OpenAIChatCompletionClient(
            model="gpt-4o-mini",
        )
print(model_client.dump_component().model_dump_json())

az_model_client = AzureOpenAIChatCompletionClient(
    azure_deployment="{your-azure-deployment}",
    model="gpt-4o",
    api_version="2024-06-01",
    azure_endpoint="https://{your-custom-endpoint}.openai.azure.com/",
    api_key="sk-...",
)
print(az_model_client.dump_component().model_dump_json())

anthropic_client = AnthropicChatCompletionClient(
        model="claude-3-sonnet-20240229",
        api_key="your-api-key",  # Optional if ANTHROPIC_API_KEY is set in environment
    )
print(anthropic_client.dump_component().model_dump_json())

mistral_vllm_model = OpenAIChatCompletionClient(
        model="TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
        base_url="http://localhost:1234/v1",
        model_info=ModelInfo(vision=False, function_calling=True, json_output=False, family="unknown", structured_output=True),
    )
print(mistral_vllm_model.dump_component().model_dump_json())

```

OpenAI

```
{
  "provider": "autogen_ext.models.openai.OpenAIChatCompletionClient",
  "component_type": "model",
  "version": 1,
  "component_version": 1,
  "description": "Chat completion client for OpenAI hosted models.",
  "label": "OpenAIChatCompletionClient",
  "config": { "model": "gpt-4o-mini" }
}

```

Azure OpenAI

```
{
  "provider": "autogen_ext.models.openai.AzureOpenAIChatCompletionClient",
  "component_type": "model",
  "version": 1,
  "component_version": 1,
  "description": "Chat completion client for Azure OpenAI hosted models.",
  "label": "AzureOpenAIChatCompletionClient",
  "config": {
    "model": "gpt-4o",
    "api_key": "sk-...",
    "azure_endpoint": "https://{your-custom-endpoint}.openai.azure.com/",
    "azure_deployment": "{your-azure-deployment}",
    "api_version": "2024-06-01"
  }
}

```

Anthropic

```
{
  "provider": "autogen_ext.models.anthropic.AnthropicChatCompletionClient",
  "component_type": "model",
  "version": 1,
  "component_version": 1,
  "description": "Chat completion client for Anthropic's Claude models.",
  "label": "AnthropicChatCompletionClient",
  "config": {
    "model": "claude-3-sonnet-20240229",
    "max_tokens": 4096,
    "temperature": 1.0,
    "api_key": "your-api-key"
  }
}

```

Have a local model server like Ollama, vLLM or LMStudio that provide an OpenAI compliant endpoint? You can use that as well.

```
{
  "provider": "autogen_ext.models.openai.OpenAIChatCompletionClient",
  "component_type": "model",
  "version": 1,
  "component_version": 1,
  "description": "Chat completion client for OpenAI hosted models.",
  "label": "OpenAIChatCompletionClient",
  "config": {
    "model": "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
    "model_info": {
      "vision": false,
      "function_calling": true,
      "json_output": false,
      "family": "unknown",
      "structured_output": true
    },
    "base_url": "http://localhost:1234/v1"
  }
}

```

Caution

It is important that you add the `model_info` field to the model client specification for custom models. This is used by the framework instantiate and use the model correctly. Also, the `AssistantAgent` and many other agents in AgentChat require the model to have the `function_calling` capability.

##### Q: The server starts but I can’t access the UI[#](#q-the-server-starts-but-i-can-t-access-the-ui "Link to this heading")

A: If you are running the server on a remote machine (or a local machine that fails to resolve localhost correctly), you may need to specify the host address. By default, the host address is set to `localhost`. You can specify the host address using the `--host <host>` argument. For example, to start the server on port 8081 and local address such that it is accessible from other machines on the network, you can run the following command:

```
autogenstudio ui --port 8081 --host 0.0.0.0

```

##### Q: How do I use AutoGen Studio with a different database?[#](#q-how-do-i-use-autogen-studio-with-a-different-database "Link to this heading")

A: By default, AutoGen Studio uses SQLite as the database. However, it uses the SQLModel library, which supports multiple database backends. You can use any database supported by SQLModel, such as PostgreSQL or MySQL. To use a different database, you need to specify the connection string for the database using the `--database-uri` argument when running the application. Example connection strings include:

* SQLite: `sqlite:///database.sqlite`
* PostgreSQL: `postgresql+psycopg://user:password@localhost/dbname`
* MySQL: `mysql+pymysql://user:password@localhost/dbname`
* AzureSQL: `mssql+pyodbc:///?odbc_connect=DRIVER%3D%7BODBC+Driver+17+for+SQL+Server%7D%3BSERVER%3Dtcp%3Aservername.database.windows.net%2C1433%3BDATABASE%3Ddatabasename%3BUID%3Dusername%3BPWD%3Dpassword123%3BEncrypt%3Dyes%3BTrustServerCertificate%3Dno%3BConnection+Timeout%3D30%3B`

You can then run the application with the specified database URI. For example, to use PostgreSQL, you can run the following command:

```
autogenstudio ui --database-uri postgresql+psycopg://user:password@localhost/dbname

```

> **Note:** Make sure to install the appropriate database drivers for your chosen database:
>
> * PostgreSQL: `pip install psycopg2` or `pip install psycopg2-binary`
> * MySQL: `pip install pymysql`
> * SQL Server/Azure SQL: `pip install pyodbc`
> * Oracle: `pip install cx_oracle`

##### Q: Can I export my agent workflows for use in a python app?[#](#q-can-i-export-my-agent-workflows-for-use-in-a-python-app "Link to this heading")

Yes. In the Team Builder view, you select a team and download its specification. This file can be imported in a python application using the `TeamManager` class. For example:

```

from autogenstudio.teammanager import TeamManager

tm = TeamManager()
result_stream =  tm.run(task="What is the weather in New York?", team_config="team.json") # or wm.run_stream(..)

```

You can also load the team specification as an AgentChat object using the `load_component` method.

```

import json
from autogen_agentchat.teams import BaseGroupChat
team_config = json.load(open("team.json"))
team = BaseGroupChat.load_component(team_config)

```

##### Q: Can I run AutoGen Studio in a Docker container?[#](#q-can-i-run-autogen-studio-in-a-docker-container "Link to this heading")

A: Yes, you can run AutoGen Studio in a Docker container. You can build the Docker image using the provided [Dockerfile](https://github.com/microsoft/autogen/blob/autogenstudio/samples/apps/autogen-studio/Dockerfile) and run the container using the following commands:

```
FROM python:3.10-slim

WORKDIR /code

RUN pip install -U gunicorn autogenstudio

RUN useradd -m -u 1000 user
USER user
ENV HOME=/home/user \
    PATH=/home/user/.local/bin:$PATH \
    AUTOGENSTUDIO_APPDIR=/home/user/app

WORKDIR $HOME/app

COPY --chown=user . $HOME/app

CMD gunicorn -w $((2 * $(getconf _NPROCESSORS_ONLN) + 1)) --timeout 12600 -k uvicorn.workers.UvicornWorker autogenstudio.web.app:app --bind "0.0.0.0:8081"

```

Using Gunicorn as the application server for improved performance is recommended. To run AutoGen Studio with Gunicorn, you can use the following command:

```
gunicorn -w $((2 * $(getconf _NPROCESSORS_ONLN) + 1)) --timeout 12600 -k uvicorn.workers.UvicornWorker autogenstudio.web.app:app --bind

```

## API Reference[#](#api-reference "Link to this heading")

### autogen\_agentchat[#](#module-autogen_agentchat "Link to this heading")

This module provides the main entry point for the autogen\_agentchat package.
It includes logger names for trace and event logs, and retrieves the package version.

EVENT\_LOGGER\_NAME *= 'autogen\_agentchat.events'*[#](#autogen_agentchat.EVENT_LOGGER_NAME "Link to this definition")
:   Logger name for event logs.

TRACE\_LOGGER\_NAME *= 'autogen\_agentchat'*[#](#autogen_agentchat.TRACE_LOGGER_NAME "Link to this definition")
:   Logger name for trace logs.

### autogen\_agentchat.messages[#](#module-autogen_agentchat.messages "Link to this heading")

This module defines various message types used for agent-to-agent communication.
Each message type inherits either from the BaseChatMessage class or BaseAgentEvent
class and includes specific fields relevant to the type of message being sent.

AgentEvent[#](#autogen_agentchat.messages.AgentEvent "Link to this definition")
:   Events emitted by agents and teams when they work, not used for agent-to-agent communication.

    alias of `Annotated`[[`ToolCallRequestEvent`](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [`ToolCallExecutionEvent`](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [`MemoryQueryEvent`](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [`UserInputRequestedEvent`](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [`ModelClientStreamingChunkEvent`](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [`ThoughtEvent`](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator=’type’)]

*pydantic model* BaseMessage[#](#autogen_agentchat.messages.BaseMessage "Link to this definition")
:   Bases: `BaseModel`, [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC "(in Python v3.13)")

    Base class for all message types.

    Show JSON schema

    ```
    {
       "title": "BaseMessage",
       "description": "Base class for all message types.",
       "type": "object",
       "properties": {
          "source": {
             "title": "Source",
             "type": "string"
          },
          "models_usage": {
             "anyOf": [
                {
                   "$ref": "#/$defs/RequestUsage"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "metadata": {
             "additionalProperties": {
                "type": "string"
             },
             "default": {},
             "title": "Metadata",
             "type": "object"
          }
       },
       "$defs": {
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          }
       },
       "required": [
          "source"
       ]
    }

    ```

    Fields:
    :   * [`metadata (Dict[str, str])`](#autogen_agentchat.messages.BaseMessage.metadata "autogen_agentchat.messages.BaseMessage.metadata")
        * [`models_usage (autogen_core.models._types.RequestUsage | None)`](#autogen_agentchat.messages.BaseMessage.models_usage "autogen_agentchat.messages.BaseMessage.models_usage")
        * [`source (str)`](#autogen_agentchat.messages.BaseMessage.source "autogen_agentchat.messages.BaseMessage.source")

    *field* metadata*: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]* *= {}*[#](#autogen_agentchat.messages.BaseMessage.metadata "Link to this definition")
    :   Additional metadata about the message.

    *field* models\_usage*: [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_agentchat.messages.BaseMessage.models_usage "Link to this definition")
    :   The model client usage incurred when producing this message.

    *field* source*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_agentchat.messages.BaseMessage.source "Link to this definition")
    :   The name of the agent that sent this message.

ChatMessage[#](#autogen_agentchat.messages.ChatMessage "Link to this definition")
:   Messages for agent-to-agent communication only.

    alias of `Annotated`[[`TextMessage`](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [`MultiModalMessage`](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [`StopMessage`](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [`ToolCallSummaryMessage`](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [`HandoffMessage`](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator=’type’)]

*pydantic model* HandoffMessage[#](#autogen_agentchat.messages.HandoffMessage "Link to this definition")
:   Bases: `BaseChatMessage`

    A message requesting handoff of a conversation to another agent.

    Show JSON schema

    ```
    {
       "title": "HandoffMessage",
       "description": "A message requesting handoff of a conversation to another agent.",
       "type": "object",
       "properties": {
          "source": {
             "title": "Source",
             "type": "string"
          },
          "models_usage": {
             "anyOf": [
                {
                   "$ref": "#/$defs/RequestUsage"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "metadata": {
             "additionalProperties": {
                "type": "string"
             },
             "default": {},
             "title": "Metadata",
             "type": "object"
          },
          "target": {
             "title": "Target",
             "type": "string"
          },
          "content": {
             "title": "Content",
             "type": "string"
          },
          "context": {
             "default": [],
             "items": {
                "discriminator": {
                   "mapping": {
                      "AssistantMessage": "#/$defs/AssistantMessage",
                      "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                      "SystemMessage": "#/$defs/SystemMessage",
                      "UserMessage": "#/$defs/UserMessage"
                   },
                   "propertyName": "type"
                },
                "oneOf": [
                   {
                      "$ref": "#/$defs/SystemMessage"
                   },
                   {
                      "$ref": "#/$defs/UserMessage"
                   },
                   {
                      "$ref": "#/$defs/AssistantMessage"
                   },
                   {
                      "$ref": "#/$defs/FunctionExecutionResultMessage"
                   }
                ]
             },
             "title": "Context",
             "type": "array"
          },
          "type": {
             "const": "HandoffMessage",
             "default": "HandoffMessage",
             "title": "Type",
             "type": "string"
          }
       },
       "$defs": {
          "AssistantMessage": {
             "description": "Assistant message are sampled from the language model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "$ref": "#/$defs/FunctionCall"
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "thought": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Thought"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "AssistantMessage",
                   "default": "AssistantMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "AssistantMessage",
             "type": "object"
          },
          "FunctionCall": {
             "properties": {
                "id": {
                   "title": "Id",
                   "type": "string"
                },
                "arguments": {
                   "title": "Arguments",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                }
             },
             "required": [
                "id",
                "arguments",
                "name"
             ],
             "title": "FunctionCall",
             "type": "object"
          },
          "FunctionExecutionResult": {
             "description": "Function execution result contains the output of a function call.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                },
                "call_id": {
                   "title": "Call Id",
                   "type": "string"
                },
                "is_error": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Is Error"
                }
             },
             "required": [
                "content",
                "name",
                "call_id"
             ],
             "title": "FunctionExecutionResult",
             "type": "object"
          },
          "FunctionExecutionResultMessage": {
             "description": "Function execution result message contains the output of multiple function calls.",
             "properties": {
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionExecutionResult"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "FunctionExecutionResultMessage",
                   "default": "FunctionExecutionResultMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "FunctionExecutionResultMessage",
             "type": "object"
          },
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          },
          "SystemMessage": {
             "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "SystemMessage",
                   "default": "SystemMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "SystemMessage",
             "type": "object"
          },
          "UserMessage": {
             "description": "User message contains input from end users, or a catch-all for data provided to the model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "anyOf": [
                               {
                                  "type": "string"
                               },
                               {}
                            ]
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "UserMessage",
                   "default": "UserMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "UserMessage",
             "type": "object"
          }
       },
       "required": [
          "source",
          "target",
          "content"
       ]
    }

    ```

    Fields:
    :   * [`content (str)`](#autogen_agentchat.messages.HandoffMessage.content "autogen_agentchat.messages.HandoffMessage.content")
        * [`context (List[Annotated[autogen_core.models._types.SystemMessage | autogen_core.models._types.UserMessage | autogen_core.models._types.AssistantMessage | autogen_core.models._types.FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]])`](#autogen_agentchat.messages.HandoffMessage.context "autogen_agentchat.messages.HandoffMessage.context")
        * [`target (str)`](#autogen_agentchat.messages.HandoffMessage.target "autogen_agentchat.messages.HandoffMessage.target")
        * [`type (Literal['HandoffMessage'])`](#autogen_agentchat.messages.HandoffMessage.type "autogen_agentchat.messages.HandoffMessage.type")

    *field* content*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_agentchat.messages.HandoffMessage.content "Link to this definition")
    :   The handoff message to the target agent.

    *field* context*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]* *= []*[#](#autogen_agentchat.messages.HandoffMessage.context "Link to this definition")
    :   The model context to be passed to the target agent.

    *field* target*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_agentchat.messages.HandoffMessage.target "Link to this definition")
    :   The name of the target agent to handoff to.

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['HandoffMessage']* *= 'HandoffMessage'*[#](#autogen_agentchat.messages.HandoffMessage.type "Link to this definition")

*pydantic model* MemoryQueryEvent[#](#autogen_agentchat.messages.MemoryQueryEvent "Link to this definition")
:   Bases: `BaseAgentEvent`

    An event signaling the results of memory queries.

    Show JSON schema

    ```
    {
       "title": "MemoryQueryEvent",
       "description": "An event signaling the results of memory queries.",
       "type": "object",
       "properties": {
          "source": {
             "title": "Source",
             "type": "string"
          },
          "models_usage": {
             "anyOf": [
                {
                   "$ref": "#/$defs/RequestUsage"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "metadata": {
             "additionalProperties": {
                "type": "string"
             },
             "default": {},
             "title": "Metadata",
             "type": "object"
          },
          "content": {
             "items": {
                "$ref": "#/$defs/MemoryContent"
             },
             "title": "Content",
             "type": "array"
          },
          "type": {
             "const": "MemoryQueryEvent",
             "default": "MemoryQueryEvent",
             "title": "Type",
             "type": "string"
          }
       },
       "$defs": {
          "MemoryContent": {
             "description": "A memory content item.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "format": "binary",
                         "type": "string"
                      },
                      {
                         "type": "object"
                      },
                      {}
                   ],
                   "title": "Content"
                },
                "mime_type": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/MemoryMimeType"
                      },
                      {
                         "type": "string"
                      }
                   ],
                   "title": "Mime Type"
                },
                "metadata": {
                   "anyOf": [
                      {
                         "type": "object"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Metadata"
                }
             },
             "required": [
                "content",
                "mime_type"
             ],
             "title": "MemoryContent",
             "type": "object"
          },
          "MemoryMimeType": {
             "description": "Supported MIME types for memory content.",
             "enum": [
                "text/plain",
                "application/json",
                "text/markdown",
                "image/*",
                "application/octet-stream"
             ],
             "title": "MemoryMimeType",
             "type": "string"
          },
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          }
       },
       "required": [
          "source",
          "content"
       ]
    }

    ```

    Fields:
    :   * [`content (List[autogen_core.memory._base_memory.MemoryContent])`](#autogen_agentchat.messages.MemoryQueryEvent.content "autogen_agentchat.messages.MemoryQueryEvent.content")
        * [`type (Literal['MemoryQueryEvent'])`](#autogen_agentchat.messages.MemoryQueryEvent.type "autogen_agentchat.messages.MemoryQueryEvent.type")

    *field* content*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[MemoryContent](#autogen_core.memory.MemoryContent "autogen_core.memory._base_memory.MemoryContent")]* *[Required]*[#](#autogen_agentchat.messages.MemoryQueryEvent.content "Link to this definition")
    :   The memory query results.

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['MemoryQueryEvent']* *= 'MemoryQueryEvent'*[#](#autogen_agentchat.messages.MemoryQueryEvent.type "Link to this definition")

*pydantic model* ModelClientStreamingChunkEvent[#](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "Link to this definition")
:   Bases: `BaseAgentEvent`

    An event signaling a text output chunk from a model client in streaming mode.

    Show JSON schema

    ```
    {
       "title": "ModelClientStreamingChunkEvent",
       "description": "An event signaling a text output chunk from a model client in streaming mode.",
       "type": "object",
       "properties": {
          "source": {
             "title": "Source",
             "type": "string"
          },
          "models_usage": {
             "anyOf": [
                {
                   "$ref": "#/$defs/RequestUsage"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "metadata": {
             "additionalProperties": {
                "type": "string"
             },
             "default": {},
             "title": "Metadata",
             "type": "object"
          },
          "content": {
             "title": "Content",
             "type": "string"
          },
          "type": {
             "const": "ModelClientStreamingChunkEvent",
             "default": "ModelClientStreamingChunkEvent",
             "title": "Type",
             "type": "string"
          }
       },
       "$defs": {
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          }
       },
       "required": [
          "source",
          "content"
       ]
    }

    ```

    Fields:
    :   * [`content (str)`](#autogen_agentchat.messages.ModelClientStreamingChunkEvent.content "autogen_agentchat.messages.ModelClientStreamingChunkEvent.content")
        * [`type (Literal['ModelClientStreamingChunkEvent'])`](#autogen_agentchat.messages.ModelClientStreamingChunkEvent.type "autogen_agentchat.messages.ModelClientStreamingChunkEvent.type")

    *field* content*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_agentchat.messages.ModelClientStreamingChunkEvent.content "Link to this definition")
    :   The partial text chunk.

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['ModelClientStreamingChunkEvent']* *= 'ModelClientStreamingChunkEvent'*[#](#autogen_agentchat.messages.ModelClientStreamingChunkEvent.type "Link to this definition")

*pydantic model* MultiModalMessage[#](#autogen_agentchat.messages.MultiModalMessage "Link to this definition")
:   Bases: `BaseChatMessage`

    A multimodal message.

    Show JSON schema

    ```
    {
       "title": "MultiModalMessage",
       "description": "A multimodal message.",
       "type": "object",
       "properties": {
          "source": {
             "title": "Source",
             "type": "string"
          },
          "models_usage": {
             "anyOf": [
                {
                   "$ref": "#/$defs/RequestUsage"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "metadata": {
             "additionalProperties": {
                "type": "string"
             },
             "default": {},
             "title": "Metadata",
             "type": "object"
          },
          "content": {
             "items": {
                "anyOf": [
                   {
                      "type": "string"
                   },
                   {}
                ]
             },
             "title": "Content",
             "type": "array"
          },
          "type": {
             "const": "MultiModalMessage",
             "default": "MultiModalMessage",
             "title": "Type",
             "type": "string"
          }
       },
       "$defs": {
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          }
       },
       "required": [
          "source",
          "content"
       ]
    }

    ```

    Fields:
    :   * [`content (List[str | autogen_core._image.Image])`](#autogen_agentchat.messages.MultiModalMessage.content "autogen_agentchat.messages.MultiModalMessage.content")
        * [`type (Literal['MultiModalMessage'])`](#autogen_agentchat.messages.MultiModalMessage.type "autogen_agentchat.messages.MultiModalMessage.type")

    *field* content*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Image](#autogen_core.Image "autogen_core._image.Image")]* *[Required]*[#](#autogen_agentchat.messages.MultiModalMessage.content "Link to this definition")
    :   The content of the message.

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['MultiModalMessage']* *= 'MultiModalMessage'*[#](#autogen_agentchat.messages.MultiModalMessage.type "Link to this definition")

*pydantic model* StopMessage[#](#autogen_agentchat.messages.StopMessage "Link to this definition")
:   Bases: `BaseChatMessage`

    A message requesting stop of a conversation.

    Show JSON schema

    ```
    {
       "title": "StopMessage",
       "description": "A message requesting stop of a conversation.",
       "type": "object",
       "properties": {
          "source": {
             "title": "Source",
             "type": "string"
          },
          "models_usage": {
             "anyOf": [
                {
                   "$ref": "#/$defs/RequestUsage"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "metadata": {
             "additionalProperties": {
                "type": "string"
             },
             "default": {},
             "title": "Metadata",
             "type": "object"
          },
          "content": {
             "title": "Content",
             "type": "string"
          },
          "type": {
             "const": "StopMessage",
             "default": "StopMessage",
             "title": "Type",
             "type": "string"
          }
       },
       "$defs": {
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          }
       },
       "required": [
          "source",
          "content"
       ]
    }

    ```

    Fields:
    :   * [`content (str)`](#autogen_agentchat.messages.StopMessage.content "autogen_agentchat.messages.StopMessage.content")
        * [`type (Literal['StopMessage'])`](#autogen_agentchat.messages.StopMessage.type "autogen_agentchat.messages.StopMessage.type")

    *field* content*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_agentchat.messages.StopMessage.content "Link to this definition")
    :   The content for the stop message.

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['StopMessage']* *= 'StopMessage'*[#](#autogen_agentchat.messages.StopMessage.type "Link to this definition")

*pydantic model* TextMessage[#](#autogen_agentchat.messages.TextMessage "Link to this definition")
:   Bases: `BaseChatMessage`

    A text message.

    Show JSON schema

    ```
    {
       "title": "TextMessage",
       "description": "A text message.",
       "type": "object",
       "properties": {
          "source": {
             "title": "Source",
             "type": "string"
          },
          "models_usage": {
             "anyOf": [
                {
                   "$ref": "#/$defs/RequestUsage"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "metadata": {
             "additionalProperties": {
                "type": "string"
             },
             "default": {},
             "title": "Metadata",
             "type": "object"
          },
          "content": {
             "title": "Content",
             "type": "string"
          },
          "type": {
             "const": "TextMessage",
             "default": "TextMessage",
             "title": "Type",
             "type": "string"
          }
       },
       "$defs": {
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          }
       },
       "required": [
          "source",
          "content"
       ]
    }

    ```

    Fields:
    :   * [`content (str)`](#autogen_agentchat.messages.TextMessage.content "autogen_agentchat.messages.TextMessage.content")
        * [`type (Literal['TextMessage'])`](#autogen_agentchat.messages.TextMessage.type "autogen_agentchat.messages.TextMessage.type")

    *field* content*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_agentchat.messages.TextMessage.content "Link to this definition")
    :   The content of the message.

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['TextMessage']* *= 'TextMessage'*[#](#autogen_agentchat.messages.TextMessage.type "Link to this definition")

*pydantic model* ThoughtEvent[#](#autogen_agentchat.messages.ThoughtEvent "Link to this definition")
:   Bases: `BaseAgentEvent`

    An event signaling the thought process of an agent.
    It is used to communicate the reasoning tokens generated by a reasoning model,
    or the extra text content generated by a function call.

    Show JSON schema

    ```
    {
       "title": "ThoughtEvent",
       "description": "An event signaling the thought process of an agent.\nIt is used to communicate the reasoning tokens generated by a reasoning model,\nor the extra text content generated by a function call.",
       "type": "object",
       "properties": {
          "source": {
             "title": "Source",
             "type": "string"
          },
          "models_usage": {
             "anyOf": [
                {
                   "$ref": "#/$defs/RequestUsage"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "metadata": {
             "additionalProperties": {
                "type": "string"
             },
             "default": {},
             "title": "Metadata",
             "type": "object"
          },
          "content": {
             "title": "Content",
             "type": "string"
          },
          "type": {
             "const": "ThoughtEvent",
             "default": "ThoughtEvent",
             "title": "Type",
             "type": "string"
          }
       },
       "$defs": {
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          }
       },
       "required": [
          "source",
          "content"
       ]
    }

    ```

    Fields:
    :   * [`content (str)`](#autogen_agentchat.messages.ThoughtEvent.content "autogen_agentchat.messages.ThoughtEvent.content")
        * [`type (Literal['ThoughtEvent'])`](#autogen_agentchat.messages.ThoughtEvent.type "autogen_agentchat.messages.ThoughtEvent.type")

    *field* content*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_agentchat.messages.ThoughtEvent.content "Link to this definition")
    :   The thought process.

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['ThoughtEvent']* *= 'ThoughtEvent'*[#](#autogen_agentchat.messages.ThoughtEvent.type "Link to this definition")

*pydantic model* ToolCallExecutionEvent[#](#autogen_agentchat.messages.ToolCallExecutionEvent "Link to this definition")
:   Bases: `BaseAgentEvent`

    An event signaling the execution of tool calls.

    Show JSON schema

    ```
    {
       "title": "ToolCallExecutionEvent",
       "description": "An event signaling the execution of tool calls.",
       "type": "object",
       "properties": {
          "source": {
             "title": "Source",
             "type": "string"
          },
          "models_usage": {
             "anyOf": [
                {
                   "$ref": "#/$defs/RequestUsage"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "metadata": {
             "additionalProperties": {
                "type": "string"
             },
             "default": {},
             "title": "Metadata",
             "type": "object"
          },
          "content": {
             "items": {
                "$ref": "#/$defs/FunctionExecutionResult"
             },
             "title": "Content",
             "type": "array"
          },
          "type": {
             "const": "ToolCallExecutionEvent",
             "default": "ToolCallExecutionEvent",
             "title": "Type",
             "type": "string"
          }
       },
       "$defs": {
          "FunctionExecutionResult": {
             "description": "Function execution result contains the output of a function call.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                },
                "call_id": {
                   "title": "Call Id",
                   "type": "string"
                },
                "is_error": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Is Error"
                }
             },
             "required": [
                "content",
                "name",
                "call_id"
             ],
             "title": "FunctionExecutionResult",
             "type": "object"
          },
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          }
       },
       "required": [
          "source",
          "content"
       ]
    }

    ```

    Fields:
    :   * [`content (List[autogen_core.models._types.FunctionExecutionResult])`](#autogen_agentchat.messages.ToolCallExecutionEvent.content "autogen_agentchat.messages.ToolCallExecutionEvent.content")
        * [`type (Literal['ToolCallExecutionEvent'])`](#autogen_agentchat.messages.ToolCallExecutionEvent.type "autogen_agentchat.messages.ToolCallExecutionEvent.type")

    *field* content*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[FunctionExecutionResult](#autogen_core.models.FunctionExecutionResult "autogen_core.models._types.FunctionExecutionResult")]* *[Required]*[#](#autogen_agentchat.messages.ToolCallExecutionEvent.content "Link to this definition")
    :   The tool call results.

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['ToolCallExecutionEvent']* *= 'ToolCallExecutionEvent'*[#](#autogen_agentchat.messages.ToolCallExecutionEvent.type "Link to this definition")

*pydantic model* ToolCallRequestEvent[#](#autogen_agentchat.messages.ToolCallRequestEvent "Link to this definition")
:   Bases: `BaseAgentEvent`

    An event signaling a request to use tools.

    Show JSON schema

    ```
    {
       "title": "ToolCallRequestEvent",
       "description": "An event signaling a request to use tools.",
       "type": "object",
       "properties": {
          "source": {
             "title": "Source",
             "type": "string"
          },
          "models_usage": {
             "anyOf": [
                {
                   "$ref": "#/$defs/RequestUsage"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "metadata": {
             "additionalProperties": {
                "type": "string"
             },
             "default": {},
             "title": "Metadata",
             "type": "object"
          },
          "content": {
             "items": {
                "$ref": "#/$defs/FunctionCall"
             },
             "title": "Content",
             "type": "array"
          },
          "type": {
             "const": "ToolCallRequestEvent",
             "default": "ToolCallRequestEvent",
             "title": "Type",
             "type": "string"
          }
       },
       "$defs": {
          "FunctionCall": {
             "properties": {
                "id": {
                   "title": "Id",
                   "type": "string"
                },
                "arguments": {
                   "title": "Arguments",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                }
             },
             "required": [
                "id",
                "arguments",
                "name"
             ],
             "title": "FunctionCall",
             "type": "object"
          },
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          }
       },
       "required": [
          "source",
          "content"
       ]
    }

    ```

    Fields:
    :   * [`content (List[autogen_core._types.FunctionCall])`](#autogen_agentchat.messages.ToolCallRequestEvent.content "autogen_agentchat.messages.ToolCallRequestEvent.content")
        * [`type (Literal['ToolCallRequestEvent'])`](#autogen_agentchat.messages.ToolCallRequestEvent.type "autogen_agentchat.messages.ToolCallRequestEvent.type")

    *field* content*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[FunctionCall](#autogen_core.FunctionCall "autogen_core._types.FunctionCall")]* *[Required]*[#](#autogen_agentchat.messages.ToolCallRequestEvent.content "Link to this definition")
    :   The tool calls.

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['ToolCallRequestEvent']* *= 'ToolCallRequestEvent'*[#](#autogen_agentchat.messages.ToolCallRequestEvent.type "Link to this definition")

*pydantic model* ToolCallSummaryMessage[#](#autogen_agentchat.messages.ToolCallSummaryMessage "Link to this definition")
:   Bases: `BaseChatMessage`

    A message signaling the summary of tool call results.

    Show JSON schema

    ```
    {
       "title": "ToolCallSummaryMessage",
       "description": "A message signaling the summary of tool call results.",
       "type": "object",
       "properties": {
          "source": {
             "title": "Source",
             "type": "string"
          },
          "models_usage": {
             "anyOf": [
                {
                   "$ref": "#/$defs/RequestUsage"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "metadata": {
             "additionalProperties": {
                "type": "string"
             },
             "default": {},
             "title": "Metadata",
             "type": "object"
          },
          "content": {
             "title": "Content",
             "type": "string"
          },
          "type": {
             "const": "ToolCallSummaryMessage",
             "default": "ToolCallSummaryMessage",
             "title": "Type",
             "type": "string"
          }
       },
       "$defs": {
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          }
       },
       "required": [
          "source",
          "content"
       ]
    }

    ```

    Fields:
    :   * [`content (str)`](#autogen_agentchat.messages.ToolCallSummaryMessage.content "autogen_agentchat.messages.ToolCallSummaryMessage.content")
        * [`type (Literal['ToolCallSummaryMessage'])`](#autogen_agentchat.messages.ToolCallSummaryMessage.type "autogen_agentchat.messages.ToolCallSummaryMessage.type")

    *field* content*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_agentchat.messages.ToolCallSummaryMessage.content "Link to this definition")
    :   Summary of the the tool call results.

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['ToolCallSummaryMessage']* *= 'ToolCallSummaryMessage'*[#](#autogen_agentchat.messages.ToolCallSummaryMessage.type "Link to this definition")

*pydantic model* UserInputRequestedEvent[#](#autogen_agentchat.messages.UserInputRequestedEvent "Link to this definition")
:   Bases: `BaseAgentEvent`

    An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.

    Show JSON schema

    ```
    {
       "title": "UserInputRequestedEvent",
       "description": "An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.",
       "type": "object",
       "properties": {
          "source": {
             "title": "Source",
             "type": "string"
          },
          "models_usage": {
             "anyOf": [
                {
                   "$ref": "#/$defs/RequestUsage"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "metadata": {
             "additionalProperties": {
                "type": "string"
             },
             "default": {},
             "title": "Metadata",
             "type": "object"
          },
          "request_id": {
             "title": "Request Id",
             "type": "string"
          },
          "content": {
             "const": "",
             "default": "",
             "title": "Content",
             "type": "string"
          },
          "type": {
             "const": "UserInputRequestedEvent",
             "default": "UserInputRequestedEvent",
             "title": "Type",
             "type": "string"
          }
       },
       "$defs": {
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          }
       },
       "required": [
          "source",
          "request_id"
       ]
    }

    ```

    Fields:
    :   * [`content (Literal[''])`](#autogen_agentchat.messages.UserInputRequestedEvent.content "autogen_agentchat.messages.UserInputRequestedEvent.content")
        * [`request_id (str)`](#autogen_agentchat.messages.UserInputRequestedEvent.request_id "autogen_agentchat.messages.UserInputRequestedEvent.request_id")
        * [`type (Literal['UserInputRequestedEvent'])`](#autogen_agentchat.messages.UserInputRequestedEvent.type "autogen_agentchat.messages.UserInputRequestedEvent.type")

    *field* content*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['']* *= ''*[#](#autogen_agentchat.messages.UserInputRequestedEvent.content "Link to this definition")
    :   Empty content for compat with consumers expecting a content field.

    *field* request\_id*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_agentchat.messages.UserInputRequestedEvent.request_id "Link to this definition")
    :   Identifier for the user input request.

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['UserInputRequestedEvent']* *= 'UserInputRequestedEvent'*[#](#autogen_agentchat.messages.UserInputRequestedEvent.type "Link to this definition")

### autogen\_agentchat.agents[#](#module-autogen_agentchat.agents "Link to this heading")

This module initializes various pre-defined agents provided by the package.
BaseChatAgent is the base class for all agents in AgentChat.

*class* AssistantAgent(*name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *model\_client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *\**, *tools: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[BaseTool](#autogen_core.tools.BaseTool "autogen_core.tools._base.BaseTool")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[...], [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[...], [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *handoffs: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Handoff](#autogen_agentchat.base.Handoff "autogen_agentchat.base._handoff.Handoff") | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *model\_context: [ChatCompletionContext](#autogen_core.model_context.ChatCompletionContext "autogen_core.model_context._chat_completion_context.ChatCompletionContext") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'An agent that provides assistance with ability to use tools.'*, *system\_message: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = 'You are a helpful AI assistant. Solve tasks using your tools. Reply with TERMINATE when the task has been completed.'*, *model\_client\_stream: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*, *reflect\_on\_tool\_use: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*, *tool\_call\_summary\_format: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = '{result}'*, *memory: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Memory](#autogen_core.memory.Memory "autogen_core.memory._base_memory.Memory")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_agentchat.agents.AssistantAgent "Link to this definition")
:   Bases: [`BaseChatAgent`](#autogen_agentchat.agents.BaseChatAgent "autogen_agentchat.agents._base_chat_agent.BaseChatAgent"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`AssistantAgentConfig`]

    An agent that provides assistance with tool use.

    The [`on_messages()`](#autogen_agentchat.agents.AssistantAgent.on_messages "autogen_agentchat.agents.AssistantAgent.on_messages") returns a [`Response`](#autogen_agentchat.base.Response "autogen_agentchat.base.Response")
    in which [`chat_message`](#autogen_agentchat.base.Response.chat_message "autogen_agentchat.base.Response.chat_message") is the final
    response message.

    The [`on_messages_stream()`](#autogen_agentchat.agents.AssistantAgent.on_messages_stream "autogen_agentchat.agents.AssistantAgent.on_messages_stream") creates an async generator that produces
    the inner messages as they are created, and the [`Response`](#autogen_agentchat.base.Response "autogen_agentchat.base.Response")
    object as the last item before closing the generator.

    Attention

    The caller must only pass the new messages to the agent on each call
    to the [`on_messages()`](#autogen_agentchat.agents.AssistantAgent.on_messages "autogen_agentchat.agents.AssistantAgent.on_messages") or [`on_messages_stream()`](#autogen_agentchat.agents.AssistantAgent.on_messages_stream "autogen_agentchat.agents.AssistantAgent.on_messages_stream") method.
    The agent maintains its state between calls to these methods.
    Do not pass the entire conversation history to the agent on each call.

    Warning

    The assistant agent is not thread-safe or coroutine-safe.
    It should not be shared between multiple tasks or coroutines, and it should
    not call its methods concurrently.

    The following diagram shows how the assistant agent works:

    ![_images/assistant-agent.svg](_images/assistant-agent.svg)

    Tool call behavior:

    * If the model returns no tool call, then the response is immediately returned as a [`TextMessage`](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") in [`chat_message`](#autogen_agentchat.base.Response.chat_message "autogen_agentchat.base.Response.chat_message").
    * When the model returns tool calls, they will be executed right away:
      :   + When reflect\_on\_tool\_use is False (default), the tool call results are returned as a [`ToolCallSummaryMessage`](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") in [`chat_message`](#autogen_agentchat.base.Response.chat_message "autogen_agentchat.base.Response.chat_message"). tool\_call\_summary\_format can be used to customize the tool call summary.
          + When reflect\_on\_tool\_use is True, the another model inference is made using the tool calls and results, and the text response is returned as a [`TextMessage`](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") in [`chat_message`](#autogen_agentchat.base.Response.chat_message "autogen_agentchat.base.Response.chat_message").
    * If the model returns multiple tool calls, they will be executed concurrently. To disable parallel tool calls you need to configure the model client. For example, set parallel\_tool\_calls=False for [`OpenAIChatCompletionClient`](#autogen_ext.models.openai.OpenAIChatCompletionClient "autogen_ext.models.openai.OpenAIChatCompletionClient") and [`AzureOpenAIChatCompletionClient`](#autogen_ext.models.openai.AzureOpenAIChatCompletionClient "autogen_ext.models.openai.AzureOpenAIChatCompletionClient").

    Tip

    By default, the tool call results are returned as response when tool calls are made.
    So it is recommended to pay attention to the formatting of the tools return values,
    especially if another agent is expecting them in a specific format.
    Use tool\_call\_summary\_format to customize the tool call summary, if needed.

    Hand off behavior:

    * If a handoff is triggered, a [`HandoffMessage`](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage") will be returned in [`chat_message`](#autogen_agentchat.base.Response.chat_message "autogen_agentchat.base.Response.chat_message").
    * If there are tool calls, they will also be executed right away before returning the handoff.
    * The tool calls and results are passed to the target agent through [`context`](#autogen_agentchat.messages.HandoffMessage.context "autogen_agentchat.messages.HandoffMessage.context").

    Note

    If multiple handoffs are detected, only the first handoff is executed.
    To avoid this, disable parallel tool calls in the model client configuration.

    Limit context size sent to the model:

    You can limit the number of messages sent to the model by setting
    the model\_context parameter to a [`BufferedChatCompletionContext`](#autogen_core.model_context.BufferedChatCompletionContext "autogen_core.model_context.BufferedChatCompletionContext").
    This will limit the number of recent messages sent to the model and can be useful
    when the model has a limit on the number of tokens it can process.
    You can also create your own model context by subclassing
    [`ChatCompletionContext`](#autogen_core.model_context.ChatCompletionContext "autogen_core.model_context.ChatCompletionContext").

    Streaming mode:

    The assistant agent can be used in streaming mode by setting model\_client\_stream=True.
    In this mode, the [`on_messages_stream()`](#autogen_agentchat.agents.AssistantAgent.on_messages_stream "autogen_agentchat.agents.AssistantAgent.on_messages_stream") and [`BaseChatAgent.run_stream()`](#autogen_agentchat.agents.BaseChatAgent.run_stream "autogen_agentchat.agents.BaseChatAgent.run_stream") methods will also yield
    [`ModelClientStreamingChunkEvent`](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent")
    messages as the model client produces chunks of response.
    The chunk messages will not be included in the final response’s inner messages.

    Parameters:
    :   * **name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The name of the agent.
        * **model\_client** ([*ChatCompletionClient*](#autogen_core.models.ChatCompletionClient "autogen_core.models.ChatCompletionClient")) – The model client to use for inference.
        * **tools** (*List**[*[*BaseTool*](#autogen_core.tools.BaseTool "autogen_core.tools.BaseTool")*[**Any**,* *Any**]* *|* *Callable**[**...**,* *Any**]* *|* *Callable**[**...**,* *Awaitable**[**Any**]**]**]* *|* *None**,* *optional*) – The tools to register with the agent.
        * **handoffs** (*List**[**HandoffBase* *|* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]* *|* *None**,* *optional*) – The handoff configurations for the agent,
          allowing it to transfer to other agents by responding with a `HandoffMessage`.
          The transfer is only executed when the team is in [`Swarm`](#autogen_agentchat.teams.Swarm "autogen_agentchat.teams.Swarm").
          If a handoff is a string, it should represent the target agent’s name.
        * **model\_context** ([*ChatCompletionContext*](#autogen_core.model_context.ChatCompletionContext "autogen_core.model_context.ChatCompletionContext") *|* *None**,* *optional*) – The model context for storing and retrieving `LLMMessage`. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset.
        * **description** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The description of the agent.
        * **system\_message** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable.
        * **model\_client\_stream** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – If True, the model client will be used in streaming mode.
          [`on_messages_stream()`](#autogen_agentchat.agents.AssistantAgent.on_messages_stream "autogen_agentchat.agents.AssistantAgent.on_messages_stream") and [`BaseChatAgent.run_stream()`](#autogen_agentchat.agents.BaseChatAgent.run_stream "autogen_agentchat.agents.BaseChatAgent.run_stream") methods will also yield [`ModelClientStreamingChunkEvent`](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent")
          messages as the model client produces chunks of response. Defaults to False.
        * **reflect\_on\_tool\_use** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – If True, the agent will make another model inference using the tool call and result
          to generate a response. If False, the tool call result will be returned as the response. Defaults to False.
        * **tool\_call\_summary\_format** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The format string used to create a tool call summary for every tool call result.
          Defaults to “{result}”.
          When reflect\_on\_tool\_use is False, a concatenation of all the tool call summaries, separated by a new line character (’n’)
          will be returned as the response.
          Available variables: {tool\_name}, {arguments}, {result}.
          For example, “{tool\_name}: {result}” will create a summary like “tool\_name: result”.
        * **memory** (*Sequence**[*[*Memory*](#autogen_core.memory.Memory "autogen_core.memory.Memory")*]* *|* *None**,* *optional*) – The memory store to use for the agent. Defaults to None.

    Raises:
    :   * [**ValueError**](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.13)") – If tool names are not unique.
        * [**ValueError**](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.13)") – If handoff names are not unique.
        * [**ValueError**](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.13)") – If handoff names are not unique from tool names.
        * [**ValueError**](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.13)") – If maximum number of tool iterations is less than 1.

    Examples

    **Example 1: basic agent**

    The following example demonstrates how to create an assistant agent with
    a model client and generate a response to a simple task.

    ```
    import asyncio
    from autogen_core import CancellationToken
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.messages import TextMessage

    async def main() -> None:
        model_client = OpenAIChatCompletionClient(
            model="gpt-4o",
            # api_key = "your_openai_api_key"
        )
        agent = AssistantAgent(name="assistant", model_client=model_client)

        response = await agent.on_messages(
            [TextMessage(content="What is the capital of France?", source="user")], CancellationToken()
        )
        print(response)

    asyncio.run(main())

    ```

    **Example 2: model client token streaming**

    This example demonstrates how to create an assistant agent with
    a model client and generate a token stream by setting model\_client\_stream=True.

    ```
    import asyncio
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.messages import TextMessage
    from autogen_core import CancellationToken

    async def main() -> None:
        model_client = OpenAIChatCompletionClient(
            model="gpt-4o",
            # api_key = "your_openai_api_key"
        )
        agent = AssistantAgent(
            name="assistant",
            model_client=model_client,
            model_client_stream=True,
        )

        stream = agent.on_messages_stream(
            [TextMessage(content="Name two cities in North America.", source="user")], CancellationToken()
        )
        async for message in stream:
            print(message)

    asyncio.run(main())

    ```

    ```
    source='assistant' models_usage=None content='Two' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' cities' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' in' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' North' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' America' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' are' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' New' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' York' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' City' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' in' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' the' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' United' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' States' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' and' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' Toronto' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' in' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' Canada' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content='.' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content=' TERMIN' type='ModelClientStreamingChunkEvent'
    source='assistant' models_usage=None content='ATE' type='ModelClientStreamingChunkEvent'
    Response(chat_message=TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), content='Two cities in North America are New York City in the United States and Toronto in Canada. TERMINATE', type='TextMessage'), inner_messages=[])

    ```

    **Example 3: agent with tools**

    The following example demonstrates how to create an assistant agent with
    a model client and a tool, generate a stream of messages for a task, and
    print the messages to the console using [`Console`](#autogen_agentchat.ui.Console "autogen_agentchat.ui.Console").

    The tool is a simple function that returns the current time.
    Under the hood, the function is wrapped in a [`FunctionTool`](#autogen_core.tools.FunctionTool "autogen_core.tools.FunctionTool")
    and used with the agent’s model client. The doc string of the function
    is used as the tool description, the function name is used as the tool name,
    and the function signature including the type hints is used as the tool arguments.

    ```
    import asyncio
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.messages import TextMessage
    from autogen_agentchat.ui import Console
    from autogen_core import CancellationToken

    async def get_current_time() -> str:
        return "The current time is 12:00 PM."

    async def main() -> None:
        model_client = OpenAIChatCompletionClient(
            model="gpt-4o",
            # api_key = "your_openai_api_key"
        )
        agent = AssistantAgent(name="assistant", model_client=model_client, tools=[get_current_time])

        await Console(
            agent.on_messages_stream(
                [TextMessage(content="What is the current time?", source="user")], CancellationToken()
            )
        )

    asyncio.run(main())

    ```

    **Example 4: agent with structured output and tool**

    The following example demonstrates how to create an assistant agent with
    a model client configured to use structured output and a tool.
    Note that you need to use [`FunctionTool`](#autogen_core.tools.FunctionTool "autogen_core.tools.FunctionTool") to create the tool
    and the strict=True is required for structured output mode.
    Because the model is configured to use structured output, the output
    reflection response will be a JSON formatted string.

    ```
    import asyncio
    from typing import Literal

    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.messages import TextMessage
    from autogen_agentchat.ui import Console
    from autogen_core import CancellationToken
    from autogen_core.tools import FunctionTool
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from pydantic import BaseModel

    # Define the structured output format.
    class AgentResponse(BaseModel):
        thoughts: str
        response: Literal["happy", "sad", "neutral"]

    # Define the function to be called as a tool.
    def sentiment_analysis(text: str) -> str:
        """Given a text, return the sentiment."""
        return "happy" if "happy" in text else "sad" if "sad" in text else "neutral"

    # Create a FunctionTool instance with `strict=True`,
    # which is required for structured output mode.
    tool = FunctionTool(sentiment_analysis, description="Sentiment Analysis", strict=True)

    # Create an OpenAIChatCompletionClient instance that uses the structured output format.
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        response_format=AgentResponse,  # type: ignore
    )

    # Create an AssistantAgent instance that uses the tool and model client.
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        tools=[tool],
        system_message="Use the tool to analyze sentiment.",
        reflect_on_tool_use=True,  # Use reflection to have the agent generate a formatted response.
    )

    async def main() -> None:
        stream = agent.on_messages_stream([TextMessage(content="I am happy today!", source="user")], CancellationToken())
        await Console(stream)

    asyncio.run(main())

    ```

    ```
    ---------- assistant ----------
    [FunctionCall(id='call_tIZjAVyKEDuijbBwLY6RHV2p', arguments='{"text":"I am happy today!"}', name='sentiment_analysis')]
    ---------- assistant ----------
    [FunctionExecutionResult(content='happy', call_id='call_tIZjAVyKEDuijbBwLY6RHV2p', is_error=False)]
    ---------- assistant ----------
    {"thoughts":"The user expresses a clear positive emotion by stating they are happy today, suggesting an upbeat mood.","response":"happy"}

    ```

    **Example 5: agent with bounded model context**

    The following example shows how to use a
    [`BufferedChatCompletionContext`](#autogen_core.model_context.BufferedChatCompletionContext "autogen_core.model_context.BufferedChatCompletionContext")
    that only keeps the last 2 messages (1 user + 1 assistant).
    Bounded model context is useful when the model has a limit on the
    number of tokens it can process.

    ```
    import asyncio

    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.messages import TextMessage
    from autogen_core import CancellationToken
    from autogen_core.model_context import BufferedChatCompletionContext
    from autogen_ext.models.openai import OpenAIChatCompletionClient

    async def main() -> None:
        # Create a model client.
        model_client = OpenAIChatCompletionClient(
            model="gpt-4o-mini",
            # api_key = "your_openai_api_key"
        )

        # Create a model context that only keeps the last 2 messages (1 user + 1 assistant).
        model_context = BufferedChatCompletionContext(buffer_size=2)

        # Create an AssistantAgent instance with the model client and context.
        agent = AssistantAgent(
            name="assistant",
            model_client=model_client,
            model_context=model_context,
            system_message="You are a helpful assistant.",
        )

        response = await agent.on_messages(
            [TextMessage(content="Name two cities in North America.", source="user")], CancellationToken()
        )
        print(response.chat_message.content)  # type: ignore

        response = await agent.on_messages(
            [TextMessage(content="My favorite color is blue.", source="user")], CancellationToken()
        )
        print(response.chat_message.content)  # type: ignore

        response = await agent.on_messages(
            [TextMessage(content="Did I ask you any question?", source="user")], CancellationToken()
        )
        print(response.chat_message.content)  # type: ignore

    asyncio.run(main())

    ```

    ```
    Two cities in North America are New York City and Toronto.
    That's great! Blue is often associated with calmness and serenity. Do you have a specific shade of blue that you like, or any particular reason why it's your favorite?
    No, you didn't ask a question. I apologize for any misunderstanding. If you have something specific you'd like to discuss or ask, feel free to let me know!

    ```

    **Example 6: agent with memory**

    The following example shows how to use a list-based memory with the assistant agent.
    The memory is preloaded with some initial content.
    Under the hood, the memory is used to update the model context
    before making an inference, using the [`update_context()`](#autogen_core.memory.Memory.update_context "autogen_core.memory.Memory.update_context") method.

    ```
    import asyncio

    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.messages import TextMessage
    from autogen_core import CancellationToken
    from autogen_core.memory import ListMemory, MemoryContent
    from autogen_ext.models.openai import OpenAIChatCompletionClient

    async def main() -> None:
        # Create a model client.
        model_client = OpenAIChatCompletionClient(
            model="gpt-4o-mini",
            # api_key = "your_openai_api_key"
        )

        # Create a list-based memory with some initial content.
        memory = ListMemory()
        await memory.add(MemoryContent(content="User likes pizza.", mime_type="text/plain"))
        await memory.add(MemoryContent(content="User dislikes cheese.", mime_type="text/plain"))

        # Create an AssistantAgent instance with the model client and memory.
        agent = AssistantAgent(
            name="assistant",
            model_client=model_client,
            memory=[memory],
            system_message="You are a helpful assistant.",
        )

        response = await agent.on_messages(
            [TextMessage(content="One idea for a dinner.", source="user")], CancellationToken()
        )
        print(response.chat_message.content)  # type: ignore

    asyncio.run(main())

    ```

    ```
    How about making a delicious pizza without cheese? You can create a flavorful veggie pizza with a variety of toppings. Here's a quick idea:

    **Veggie Tomato Sauce Pizza**
    - Start with a pizza crust (store-bought or homemade).
    - Spread a layer of marinara or tomato sauce evenly over the crust.
    - Top with your favorite vegetables like bell peppers, mushrooms, onions, olives, and spinach.
    - Add some protein if you’d like, such as grilled chicken or pepperoni (ensure it's cheese-free).
    - Sprinkle with herbs like oregano and basil, and maybe a drizzle of olive oil.
    - Bake according to the crust instructions until the edges are golden and the veggies are cooked.

    Serve it with a side salad or some garlic bread to complete the meal! Enjoy your dinner!

    ```

    **Example 7: agent with `o1-mini`**

    The following example shows how to use o1-mini model with the assistant agent.

    ```
    import asyncio
    from autogen_core import CancellationToken
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.messages import TextMessage

    async def main() -> None:
        model_client = OpenAIChatCompletionClient(
            model="o1-mini",
            # api_key = "your_openai_api_key"
        )
        # The system message is not supported by the o1 series model.
        agent = AssistantAgent(name="assistant", model_client=model_client, system_message=None)

        response = await agent.on_messages(
            [TextMessage(content="What is the capital of France?", source="user")], CancellationToken()
        )
        print(response)

    asyncio.run(main())

    ```

    Note

    The o1-preview and o1-mini models do not support system message and function calling.
    So the system\_message should be set to None and the tools and handoffs should not be set.
    See [o1 beta limitations](https://platform.openai.com/docs/guides/reasoning#beta-limitations) for more details.

    **Example 8: agent using reasoning model with custom model context.**

    The following example shows how to use a reasoning model (DeepSeek R1) with the assistant agent.
    The model context is used to filter out the thought field from the assistant message.

    ```
    import asyncio
    from typing import List

    from autogen_agentchat.agents import AssistantAgent
    from autogen_core.model_context import UnboundedChatCompletionContext
    from autogen_core.models import AssistantMessage, LLMMessage, ModelFamily
    from autogen_ext.models.ollama import OllamaChatCompletionClient

    class ReasoningModelContext(UnboundedChatCompletionContext):
        """A model context for reasoning models."""

        async def get_messages(self) -> List[LLMMessage]:
            messages = await super().get_messages()
            # Filter out thought field from AssistantMessage.
            messages_out: List[LLMMessage] = []
            for message in messages:
                if isinstance(message, AssistantMessage):
                    message.thought = None
                messages_out.append(message)
            return messages_out

    # Create an instance of the model client for DeepSeek R1 hosted locally on Ollama.
    model_client = OllamaChatCompletionClient(
        model="deepseek-r1:8b",
        model_info={
            "vision": False,
            "function_calling": False,
            "json_output": False,
            "family": ModelFamily.R1,
            "structured_output": True,
        },
    )

    agent = AssistantAgent(
        "reasoning_agent",
        model_client=model_client,
        model_context=ReasoningModelContext(),  # Use the custom model context.
    )

    async def run_reasoning_agent() -> None:
        result = await agent.run(task="What is the capital of France?")
        print(result)

    asyncio.run(run_reasoning_agent())

    ```

    component\_config\_schema[#](#autogen_agentchat.agents.AssistantAgent.component_config_schema "Link to this definition")
    :   alias of `AssistantAgentConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.agents.AssistantAgent'*[#](#autogen_agentchat.agents.AssistantAgent.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.agents.AssistantAgent.load_state "Link to this definition")
    :   Load the state of the assistant agent

    *async* on\_messages(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response")[#](#autogen_agentchat.agents.AssistantAgent.on_messages "Link to this definition")
    :   Handles incoming messages and returns a response.

        Note

        Agents are stateful and the messages passed to this method should
        be the new messages since the last call to this method. The agent
        should maintain its state between calls to this method. For example,
        if the agent needs to remember the previous messages to respond to
        the current message, it should store the previous messages in the
        agent state.

    *async* on\_messages\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_agentchat.agents.AssistantAgent.on_messages_stream "Link to this definition")
    :   Process the incoming messages with the assistant agent and yield events/responses as they happen.

    *async* on\_reset(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.agents.AssistantAgent.on_reset "Link to this definition")
    :   Reset the assistant agent to its initialization state.

    *property* produced\_message\_types*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]*[#](#autogen_agentchat.agents.AssistantAgent.produced_message_types "Link to this definition")
    :   The types of messages that the agent produces in the
        `Response.chat_message` field. They must be `ChatMessage` types.

    *async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_agentchat.agents.AssistantAgent.save_state "Link to this definition")
    :   Save the current state of the assistant agent.

*class* BaseChatAgent(*name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_agentchat.agents.BaseChatAgent "Link to this definition")
:   Bases: [`ChatAgent`](#autogen_agentchat.base.ChatAgent "autogen_agentchat.base._chat_agent.ChatAgent"), [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC "(in Python v3.13)"), [`ComponentBase`](#autogen_core.ComponentBase "autogen_core._component_config.ComponentBase")[`BaseModel`]

    Base class for a chat agent.

    This abstract class provides a base implementation for a `ChatAgent`.
    To create a new chat agent, subclass this class and implement the
    [`on_messages()`](#autogen_agentchat.agents.BaseChatAgent.on_messages "autogen_agentchat.agents.BaseChatAgent.on_messages"), [`on_reset()`](#autogen_agentchat.agents.BaseChatAgent.on_reset "autogen_agentchat.agents.BaseChatAgent.on_reset"), and [`produced_message_types`](#autogen_agentchat.agents.BaseChatAgent.produced_message_types "autogen_agentchat.agents.BaseChatAgent.produced_message_types").
    If streaming is required, also implement the [`on_messages_stream()`](#autogen_agentchat.agents.BaseChatAgent.on_messages_stream "autogen_agentchat.agents.BaseChatAgent.on_messages_stream") method.

    An agent is considered stateful and maintains its state between calls to
    the [`on_messages()`](#autogen_agentchat.agents.BaseChatAgent.on_messages "autogen_agentchat.agents.BaseChatAgent.on_messages") or [`on_messages_stream()`](#autogen_agentchat.agents.BaseChatAgent.on_messages_stream "autogen_agentchat.agents.BaseChatAgent.on_messages_stream") methods.
    The agent should store its state in the
    agent instance. The agent should also implement the [`on_reset()`](#autogen_agentchat.agents.BaseChatAgent.on_reset "autogen_agentchat.agents.BaseChatAgent.on_reset") method
    to reset the agent to its initialization state.

    Note

    The caller should only pass the new messages to the agent on each call
    to the [`on_messages()`](#autogen_agentchat.agents.BaseChatAgent.on_messages "autogen_agentchat.agents.BaseChatAgent.on_messages") or [`on_messages_stream()`](#autogen_agentchat.agents.BaseChatAgent.on_messages_stream "autogen_agentchat.agents.BaseChatAgent.on_messages_stream") method.
    Do not pass the entire conversation history to the agent on each call.
    This design principle must be followed when creating a new agent.

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.agents.BaseChatAgent.close "Link to this definition")
    :   Release any resources held by the agent. This is a no-op by default in the
        [`BaseChatAgent`](#autogen_agentchat.agents.BaseChatAgent "autogen_agentchat.agents.BaseChatAgent") class. Subclasses can override this method to
        implement custom close behavior.

    component\_type*: ClassVar[ComponentType]* *= 'agent'*[#](#autogen_agentchat.agents.BaseChatAgent.component_type "Link to this definition")
    :   The logical type of the component.

    *property* description*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_agentchat.agents.BaseChatAgent.description "Link to this definition")
    :   The description of the agent. This is used by team to
        make decisions about which agents to use. The description should
        describe the agent’s capabilities and how to interact with it.

    *async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.agents.BaseChatAgent.load_state "Link to this definition")
    :   Restore agent from saved state. Default implementation for stateless agents.

    *property* name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_agentchat.agents.BaseChatAgent.name "Link to this definition")
    :   The name of the agent. This is used by team to uniquely identify
        the agent. It should be unique within the team.

    *abstract async* on\_messages(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response")[#](#autogen_agentchat.agents.BaseChatAgent.on_messages "Link to this definition")
    :   Handles incoming messages and returns a response.

        Note

        Agents are stateful and the messages passed to this method should
        be the new messages since the last call to this method. The agent
        should maintain its state between calls to this method. For example,
        if the agent needs to remember the previous messages to respond to
        the current message, it should store the previous messages in the
        agent state.

    *async* on\_messages\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_agentchat.agents.BaseChatAgent.on_messages_stream "Link to this definition")
    :   Handles incoming messages and returns a stream of messages and
        and the final item is the response. The base implementation in
        [`BaseChatAgent`](#autogen_agentchat.agents.BaseChatAgent "autogen_agentchat.agents.BaseChatAgent") simply calls [`on_messages()`](#autogen_agentchat.agents.BaseChatAgent.on_messages "autogen_agentchat.agents.BaseChatAgent.on_messages") and yields
        the messages in the response.

        Note

        Agents are stateful and the messages passed to this method should
        be the new messages since the last call to this method. The agent
        should maintain its state between calls to this method. For example,
        if the agent needs to remember the previous messages to respond to
        the current message, it should store the previous messages in the
        agent state.

    *async* on\_pause(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.agents.BaseChatAgent.on_pause "Link to this definition")
    :   Called when the agent is paused while running in its [`on_messages()`](#autogen_agentchat.agents.BaseChatAgent.on_messages "autogen_agentchat.agents.BaseChatAgent.on_messages") or
        [`on_messages_stream()`](#autogen_agentchat.agents.BaseChatAgent.on_messages_stream "autogen_agentchat.agents.BaseChatAgent.on_messages_stream") method. This is a no-op by default in the
        [`BaseChatAgent`](#autogen_agentchat.agents.BaseChatAgent "autogen_agentchat.agents.BaseChatAgent") class. Subclasses can override this method to
        implement custom pause behavior.

    *abstract async* on\_reset(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.agents.BaseChatAgent.on_reset "Link to this definition")
    :   Resets the agent to its initialization state.

    *async* on\_resume(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.agents.BaseChatAgent.on_resume "Link to this definition")
    :   Called when the agent is resumed from a pause while running in
        its [`on_messages()`](#autogen_agentchat.agents.BaseChatAgent.on_messages "autogen_agentchat.agents.BaseChatAgent.on_messages") or [`on_messages_stream()`](#autogen_agentchat.agents.BaseChatAgent.on_messages_stream "autogen_agentchat.agents.BaseChatAgent.on_messages_stream") method.
        This is a no-op by default in the [`BaseChatAgent`](#autogen_agentchat.agents.BaseChatAgent "autogen_agentchat.agents.BaseChatAgent") class.
        Subclasses can override this method to implement custom resume behavior.

    *abstract property* produced\_message\_types*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]*[#](#autogen_agentchat.agents.BaseChatAgent.produced_message_types "Link to this definition")
    :   The types of messages that the agent produces in the
        `Response.chat_message` field. They must be `ChatMessage` types.

    *async* run(*\**, *task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [TaskResult](#autogen_agentchat.base.TaskResult "autogen_agentchat.base._task.TaskResult")[#](#autogen_agentchat.agents.BaseChatAgent.run "Link to this definition")
    :   Run the agent with the given task and return the result.

    *async* run\_stream(*\**, *task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [TaskResult](#autogen_agentchat.base.TaskResult "autogen_agentchat.base._task.TaskResult"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_agentchat.agents.BaseChatAgent.run_stream "Link to this definition")
    :   Run the agent with the given task and return a stream of messages
        and the final task result as the last item in the stream.

    *async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_agentchat.agents.BaseChatAgent.save_state "Link to this definition")
    :   Export state. Default implementation for stateless agents.

*class* CodeExecutorAgent(*name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *code\_executor: [CodeExecutor](#autogen_core.code_executor.CodeExecutor "autogen_core.code_executor._base.CodeExecutor")*, *\**, *description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'A computer terminal that performs no other action than running Python scripts (provided to it quoted in ```python code blocks), or sh shell scripts (provided to it quoted in ```sh code blocks).'*, *sources: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_agentchat.agents.CodeExecutorAgent "Link to this definition")
:   Bases: [`BaseChatAgent`](#autogen_agentchat.agents.BaseChatAgent "autogen_agentchat.agents._base_chat_agent.BaseChatAgent"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`CodeExecutorAgentConfig`]

    An agent that extracts and executes code snippets found in received messages and returns the output.

    It is typically used within a team with another agent that generates code snippets to be executed.

    Note

    Consider [`PythonCodeExecutionTool`](#autogen_ext.tools.code_execution.PythonCodeExecutionTool "autogen_ext.tools.code_execution.PythonCodeExecutionTool")
    as an alternative to this agent. The tool allows for executing Python code
    within a single agent, rather than sending it to a separate agent for execution.
    However, the model for the agent will have to generate properly escaped code
    string as a parameter to the tool.

    Parameters:
    :   * **name** – The name of the agent.
        * **code\_executor** – The CodeExecutor responsible for executing code received in messages ([`DockerCommandLineCodeExecutor`](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor "autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor") recommended. See example below)
        * **description** (*optional*) – The description of the agent.
        * **sources** (*optional*) – Check only messages from the specified agents for the code to execute.

    Note

    It is recommended that the CodeExecutorAgent agent uses a Docker container to execute code. This ensures that model-generated code is executed in an isolated environment. To use Docker, your environment must have Docker installed and running.
    Follow the installation instructions for [Docker](https://docs.docker.com/get-docker/).

    Note

    The code executor only processes code that is properly formatted in markdown code blocks using triple backticks.
    For example:

    ```
    ```python
    print("Hello World")
    ```

    # or

    ```sh
    echo "Hello World"
    ```

    ```

    In this example, we show how to set up a CodeExecutorAgent agent that uses the
    [`DockerCommandLineCodeExecutor`](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor "autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor")
    to execute code snippets in a Docker container. The work\_dir parameter indicates where all executed files are first saved locally before being executed in the Docker container.

    > ```
    > import asyncio
    > from autogen_agentchat.agents import CodeExecutorAgent
    > from autogen_agentchat.messages import TextMessage
    > from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
    > from autogen_core import CancellationToken
    >
    >
    > async def run_code_executor_agent() -> None:
    >     # Create a code executor agent that uses a Docker container to execute code.
    >     code_executor = DockerCommandLineCodeExecutor(work_dir="coding")
    >     await code_executor.start()
    >     code_executor_agent = CodeExecutorAgent("code_executor", code_executor=code_executor)
    >
    >     # Run the agent with a given code snippet.
    >     task = TextMessage(
    >         content='''Here is some code
    > ```python
    > print('Hello world')
    > ```
    > ''',
    >         source="user",
    >     )
    >     response = await code_executor_agent.on_messages([task], CancellationToken())
    >     print(response.chat_message)
    >
    >     # Stop the code executor.
    >     await code_executor.stop()
    >
    >
    > asyncio.run(run_code_executor_agent())
    >
    > ```

    *classmethod* \_from\_config(*config: CodeExecutorAgentConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.agents.CodeExecutorAgent._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → CodeExecutorAgentConfig[#](#autogen_agentchat.agents.CodeExecutorAgent._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.agents.CodeExecutorAgent.component_config_schema "Link to this definition")
    :   alias of `CodeExecutorAgentConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.agents.CodeExecutorAgent'*[#](#autogen_agentchat.agents.CodeExecutorAgent.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* on\_messages(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response")[#](#autogen_agentchat.agents.CodeExecutorAgent.on_messages "Link to this definition")
    :   Handles incoming messages and returns a response.

        Note

        Agents are stateful and the messages passed to this method should
        be the new messages since the last call to this method. The agent
        should maintain its state between calls to this method. For example,
        if the agent needs to remember the previous messages to respond to
        the current message, it should store the previous messages in the
        agent state.

    *async* on\_reset(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.agents.CodeExecutorAgent.on_reset "Link to this definition")
    :   It it’s a no-op as the code executor agent has no mutable state.

    *property* produced\_message\_types*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]*[#](#autogen_agentchat.agents.CodeExecutorAgent.produced_message_types "Link to this definition")
    :   The types of messages that the code executor agent produces.

*class* SocietyOfMindAgent(*name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *team: [Team](#autogen_agentchat.base.Team "autogen_agentchat.base._team.Team")*, *model\_client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *\**, *description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = DEFAULT\_DESCRIPTION*, *instruction: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = DEFAULT\_INSTRUCTION*, *response\_prompt: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = DEFAULT\_RESPONSE\_PROMPT*)[#](#autogen_agentchat.agents.SocietyOfMindAgent "Link to this definition")
:   Bases: [`BaseChatAgent`](#autogen_agentchat.agents.BaseChatAgent "autogen_agentchat.agents._base_chat_agent.BaseChatAgent"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`SocietyOfMindAgentConfig`]

    An agent that uses an inner team of agents to generate responses.

    Each time the agent’s [`on_messages()`](#autogen_agentchat.agents.SocietyOfMindAgent.on_messages "autogen_agentchat.agents.SocietyOfMindAgent.on_messages") or [`on_messages_stream()`](#autogen_agentchat.agents.SocietyOfMindAgent.on_messages_stream "autogen_agentchat.agents.SocietyOfMindAgent.on_messages_stream")
    method is called, it runs the inner team of agents and then uses the
    model client to generate a response based on the inner team’s messages.
    Once the response is generated, the agent resets the inner team by
    calling `Team.reset()`.

    Parameters:
    :   * **name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The name of the agent.
        * **team** ([*Team*](#autogen_agentchat.base.Team "autogen_agentchat.base.Team")) – The team of agents to use.
        * **model\_client** ([*ChatCompletionClient*](#autogen_core.models.ChatCompletionClient "autogen_core.models.ChatCompletionClient")) – The model client to use for preparing responses.
        * **description** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The description of the agent.
        * **instruction** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The instruction to use when generating a response using the inner team’s messages.
          Defaults to [`DEFAULT_INSTRUCTION`](#autogen_agentchat.agents.SocietyOfMindAgent.DEFAULT_INSTRUCTION "autogen_agentchat.agents.SocietyOfMindAgent.DEFAULT_INSTRUCTION"). It assumes the role of ‘system’.
        * **response\_prompt** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The response prompt to use when generating a response using the inner team’s messages.
          Defaults to [`DEFAULT_RESPONSE_PROMPT`](#autogen_agentchat.agents.SocietyOfMindAgent.DEFAULT_RESPONSE_PROMPT "autogen_agentchat.agents.SocietyOfMindAgent.DEFAULT_RESPONSE_PROMPT"). It assumes the role of ‘system’.

    Example:

    ```
    import asyncio
    from autogen_agentchat.ui import Console
    from autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_agentchat.teams import RoundRobinGroupChat
    from autogen_agentchat.conditions import TextMentionTermination

    async def main() -> None:
        model_client = OpenAIChatCompletionClient(model="gpt-4o")

        agent1 = AssistantAgent("assistant1", model_client=model_client, system_message="You are a writer, write well.")
        agent2 = AssistantAgent(
            "assistant2",
            model_client=model_client,
            system_message="You are an editor, provide critical feedback. Respond with 'APPROVE' if the text addresses all feedbacks.",
        )
        inner_termination = TextMentionTermination("APPROVE")
        inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination)

        society_of_mind_agent = SocietyOfMindAgent("society_of_mind", team=inner_team, model_client=model_client)

        agent3 = AssistantAgent(
            "assistant3", model_client=model_client, system_message="Translate the text to Spanish."
        )
        team = RoundRobinGroupChat([society_of_mind_agent, agent3], max_turns=2)

        stream = team.run_stream(task="Write a short story with a surprising ending.")
        await Console(stream)

    asyncio.run(main())

    ```

    DEFAULT\_DESCRIPTION *= 'An agent that uses an inner team of agents to generate responses.'*[#](#autogen_agentchat.agents.SocietyOfMindAgent.DEFAULT_DESCRIPTION "Link to this definition")
    :   The default description for a SocietyOfMindAgent.

        Type:
        :   [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")

    DEFAULT\_INSTRUCTION *= 'Earlier you were asked to fulfill a request. You and your team worked diligently to address that request. Here is a transcript of that conversation:'*[#](#autogen_agentchat.agents.SocietyOfMindAgent.DEFAULT_INSTRUCTION "Link to this definition")
    :   The default instruction to use when generating a response using the
        inner team’s messages. The instruction will be prepended to the inner team’s
        messages when generating a response using the model. It assumes the role of
        ‘system’.

        Type:
        :   [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")

    DEFAULT\_RESPONSE\_PROMPT *= 'Output a standalone response to the original request, without mentioning any of the intermediate discussion.'*[#](#autogen_agentchat.agents.SocietyOfMindAgent.DEFAULT_RESPONSE_PROMPT "Link to this definition")
    :   The default response prompt to use when generating a response using
        the inner team’s messages. It assumes the role of ‘system’.

        Type:
        :   [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")

    *classmethod* \_from\_config(*config: SocietyOfMindAgentConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.agents.SocietyOfMindAgent._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → SocietyOfMindAgentConfig[#](#autogen_agentchat.agents.SocietyOfMindAgent._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.agents.SocietyOfMindAgent.component_config_schema "Link to this definition")
    :   alias of `SocietyOfMindAgentConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.agents.SocietyOfMindAgent'*[#](#autogen_agentchat.agents.SocietyOfMindAgent.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.agents.SocietyOfMindAgent.load_state "Link to this definition")
    :   Restore agent from saved state. Default implementation for stateless agents.

    *async* on\_messages(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response")[#](#autogen_agentchat.agents.SocietyOfMindAgent.on_messages "Link to this definition")
    :   Handles incoming messages and returns a response.

        Note

        Agents are stateful and the messages passed to this method should
        be the new messages since the last call to this method. The agent
        should maintain its state between calls to this method. For example,
        if the agent needs to remember the previous messages to respond to
        the current message, it should store the previous messages in the
        agent state.

    *async* on\_messages\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_agentchat.agents.SocietyOfMindAgent.on_messages_stream "Link to this definition")
    :   Handles incoming messages and returns a stream of messages and
        and the final item is the response. The base implementation in
        [`BaseChatAgent`](#autogen_agentchat.agents.BaseChatAgent "autogen_agentchat.agents.BaseChatAgent") simply calls [`on_messages()`](#autogen_agentchat.agents.SocietyOfMindAgent.on_messages "autogen_agentchat.agents.SocietyOfMindAgent.on_messages") and yields
        the messages in the response.

        Note

        Agents are stateful and the messages passed to this method should
        be the new messages since the last call to this method. The agent
        should maintain its state between calls to this method. For example,
        if the agent needs to remember the previous messages to respond to
        the current message, it should store the previous messages in the
        agent state.

    *async* on\_reset(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.agents.SocietyOfMindAgent.on_reset "Link to this definition")
    :   Resets the agent to its initialization state.

    *property* produced\_message\_types*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]*[#](#autogen_agentchat.agents.SocietyOfMindAgent.produced_message_types "Link to this definition")
    :   The types of messages that the agent produces in the
        `Response.chat_message` field. They must be `ChatMessage` types.

    *async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_agentchat.agents.SocietyOfMindAgent.save_state "Link to this definition")
    :   Export state. Default implementation for stateless agents.

*class* UserProxyAgent(*name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *\**, *description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'A human user'*, *input\_func: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")], [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")], [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_agentchat.agents.UserProxyAgent "Link to this definition")
:   Bases: [`BaseChatAgent`](#autogen_agentchat.agents.BaseChatAgent "autogen_agentchat.agents._base_chat_agent.BaseChatAgent"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`UserProxyAgentConfig`]

    An agent that can represent a human user through an input function.

    This agent can be used to represent a human user in a chat system by providing a custom input function.

    Note

    Using [`UserProxyAgent`](#autogen_agentchat.agents.UserProxyAgent "autogen_agentchat.agents.UserProxyAgent") puts a running team in a temporary blocked
    state until the user responds. So it is important to time out the user input
    function and cancel using the [`CancellationToken`](#autogen_core.CancellationToken "autogen_core.CancellationToken") if the user does not respond.
    The input function should also handle exceptions and return a default response if needed.

    For typical use cases that involve
    slow human responses, it is recommended to use termination conditions
    such as [`HandoffTermination`](#autogen_agentchat.conditions.HandoffTermination "autogen_agentchat.conditions.HandoffTermination") or [`SourceMatchTermination`](#autogen_agentchat.conditions.SourceMatchTermination "autogen_agentchat.conditions.SourceMatchTermination")
    to stop the running team and return the control to the application.
    You can run the team again with the user input. This way, the state of the team
    can be saved and restored when the user responds.

    See [Human-in-the-loop](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.html) for more information.

    Parameters:
    :   * **name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The name of the agent.
        * **description** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – A description of the agent.
        * **input\_func** (*Optional**[**Callable**[**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]**]**,* *Callable**[**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Optional**[*[*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken")*]**]**,* *Awaitable**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]**]*) – A function that takes a prompt and returns a user input string.

    For examples of integrating with web and UI frameworks, see the following:

    * [FastAPI](https://github.com/microsoft/autogen/tree/main/python/samples/agentchat_fastapi)
    * [ChainLit](https://github.com/microsoft/autogen/tree/main/python/samples/agentchat_chainlit)

    Example

    Simple usage case:

    ```
    import asyncio
    from autogen_core import CancellationToken
    from autogen_agentchat.agents import UserProxyAgent
    from autogen_agentchat.messages import TextMessage

    async def simple_user_agent():
        agent = UserProxyAgent("user_proxy")
        response = await asyncio.create_task(
            agent.on_messages(
                [TextMessage(content="What is your name? ", source="user")],
                cancellation_token=CancellationToken(),
            )
        )
        print(f"Your name is {response.chat_message.content}")

    ```

    Example

    Cancellable usage case:

    ```
    import asyncio
    from typing import Any
    from autogen_core import CancellationToken
    from autogen_agentchat.agents import UserProxyAgent
    from autogen_agentchat.messages import TextMessage

    token = CancellationToken()
    agent = UserProxyAgent("user_proxy")

    async def timeout(delay: float):
        await asyncio.sleep(delay)

    def cancellation_callback(task: asyncio.Task[Any]):
        token.cancel()

    async def cancellable_user_agent():
        try:
            timeout_task = asyncio.create_task(timeout(3))
            timeout_task.add_done_callback(cancellation_callback)
            agent_task = asyncio.create_task(
                agent.on_messages(
                    [TextMessage(content="What is your name? ", source="user")],
                    cancellation_token=token,
                )
            )
            response = await agent_task
            print(f"Your name is {response.chat_message.content}")
        except Exception as e:
            print(f"Exception: {e}")
        except BaseException as e:
            print(f"BaseException: {e}")

    ```

    *class* InputRequestContext[#](#autogen_agentchat.agents.UserProxyAgent.InputRequestContext "Link to this definition")
    :   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

        *classmethod* request\_id() → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_agentchat.agents.UserProxyAgent.InputRequestContext.request_id "Link to this definition")

    *classmethod* \_from\_config(*config: UserProxyAgentConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.agents.UserProxyAgent._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → UserProxyAgentConfig[#](#autogen_agentchat.agents.UserProxyAgent._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.agents.UserProxyAgent.component_config_schema "Link to this definition")
    :   alias of `UserProxyAgentConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.agents.UserProxyAgent'*[#](#autogen_agentchat.agents.UserProxyAgent.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    component\_type*: ClassVar[ComponentType]* *= 'agent'*[#](#autogen_agentchat.agents.UserProxyAgent.component_type "Link to this definition")
    :   The logical type of the component.

    *async* on\_messages(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response")[#](#autogen_agentchat.agents.UserProxyAgent.on_messages "Link to this definition")
    :   Handles incoming messages and returns a response.

        Note

        Agents are stateful and the messages passed to this method should
        be the new messages since the last call to this method. The agent
        should maintain its state between calls to this method. For example,
        if the agent needs to remember the previous messages to respond to
        the current message, it should store the previous messages in the
        agent state.

    *async* on\_messages\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_agentchat.agents.UserProxyAgent.on_messages_stream "Link to this definition")
    :   Handle incoming messages by requesting user input.

    *async* on\_reset(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.agents.UserProxyAgent.on_reset "Link to this definition")
    :   Reset agent state.

    *property* produced\_message\_types*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]*[#](#autogen_agentchat.agents.UserProxyAgent.produced_message_types "Link to this definition")
    :   Message types this agent can produce.

### autogen\_agentchat.teams[#](#module-autogen_agentchat.teams "Link to this heading")

This module provides implementation of various pre-defined multi-agent teams.
Each team inherits from the BaseGroupChat class.

*class* BaseGroupChat(*participants: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[ChatAgent](#autogen_agentchat.base.ChatAgent "autogen_agentchat.base._chat_agent.ChatAgent")]*, *group\_chat\_manager\_name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *group\_chat\_manager\_class: [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[SequentialRoutedAgent]*, *termination\_condition: [TerminationCondition](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *max\_turns: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *runtime: [AgentRuntime](#autogen_core.AgentRuntime "autogen_core._agent_runtime.AgentRuntime") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_agentchat.teams.BaseGroupChat "Link to this definition")
:   Bases: [`Team`](#autogen_agentchat.base.Team "autogen_agentchat.base._team.Team"), [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC "(in Python v3.13)"), [`ComponentBase`](#autogen_core.ComponentBase "autogen_core._component_config.ComponentBase")[`BaseModel`]

    The base class for group chat teams.

    To implement a group chat team, first create a subclass of `BaseGroupChatManager` and then
    create a subclass of [`BaseGroupChat`](#autogen_agentchat.teams.BaseGroupChat "autogen_agentchat.teams.BaseGroupChat") that uses the group chat manager.

    component\_type*: ClassVar[ComponentType]* *= 'team'*[#](#autogen_agentchat.teams.BaseGroupChat.component_type "Link to this definition")
    :   The logical type of the component.

    *async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.teams.BaseGroupChat.load_state "Link to this definition")
    :   Load an external state and overwrite the current state of the group chat team.

        The state is loaded by calling the [`agent_load_state()`](#autogen_core.AgentRuntime.agent_load_state "autogen_core.AgentRuntime.agent_load_state") method
        on each participant and the group chat manager with their internal agent ID.
        See [`save_state()`](#autogen_agentchat.teams.BaseGroupChat.save_state "autogen_agentchat.teams.BaseGroupChat.save_state") for the expected format of the state.

    *async* pause() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.teams.BaseGroupChat.pause "Link to this definition")
    :   Pause its participants when the team is running by calling their
        [`on_pause()`](#autogen_agentchat.base.ChatAgent.on_pause "autogen_agentchat.base.ChatAgent.on_pause") method via direct RPC calls.

        Attention

        This is an experimental feature introduced in v0.4.9 and may subject
        to change or removal in the future.

        The team must be initialized before it can be paused.

        Different from termination, pausing the team does not cause the
        [`run()`](#autogen_agentchat.teams.BaseGroupChat.run "autogen_agentchat.teams.BaseGroupChat.run") or [`run_stream()`](#autogen_agentchat.teams.BaseGroupChat.run_stream "autogen_agentchat.teams.BaseGroupChat.run_stream") method to return. It calls the
        [`on_pause()`](#autogen_agentchat.base.ChatAgent.on_pause "autogen_agentchat.base.ChatAgent.on_pause") method on each
        participant, and if the participant does not implement the method, it
        will be a no-op.

        Note

        It is the responsibility of the agent class to handle the pause
        and ensure that the agent can be resumed later.
        Make sure to implement the [`on_pause()`](#autogen_agentchat.agents.BaseChatAgent.on_pause "autogen_agentchat.agents.BaseChatAgent.on_pause")
        method in your agent class for custom pause behavior.
        By default, the agent will not do anything when called.

        Raises:
        :   [**RuntimeError**](https://docs.python.org/3/library/exceptions.html#RuntimeError "(in Python v3.13)") – If the team has not been initialized. Exceptions from
            the participants when calling their implementations of
            [`on_pause`](#autogen_agentchat.base.ChatAgent.on_pause "autogen_agentchat.base.ChatAgent.on_pause") are
            propagated to this method and raised.

    *async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.teams.BaseGroupChat.reset "Link to this definition")
    :   Reset the team and its participants to their initial state.

        The team must be stopped before it can be reset.

        Raises:
        :   [**RuntimeError**](https://docs.python.org/3/library/exceptions.html#RuntimeError "(in Python v3.13)") – If the team has not been initialized or is currently running.

        Example using the [`RoundRobinGroupChat`](#autogen_agentchat.teams.RoundRobinGroupChat "autogen_agentchat.teams.RoundRobinGroupChat") team:

        ```
        import asyncio
        from autogen_agentchat.agents import AssistantAgent
        from autogen_agentchat.conditions import MaxMessageTermination
        from autogen_agentchat.teams import RoundRobinGroupChat
        from autogen_ext.models.openai import OpenAIChatCompletionClient

        async def main() -> None:
            model_client = OpenAIChatCompletionClient(model="gpt-4o")

            agent1 = AssistantAgent("Assistant1", model_client=model_client)
            agent2 = AssistantAgent("Assistant2", model_client=model_client)
            termination = MaxMessageTermination(3)
            team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
            stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
            async for message in stream:
                print(message)

            # Reset the team.
            await team.reset()
            stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
            async for message in stream:
                print(message)

        asyncio.run(main())

        ```

    *async* resume() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.teams.BaseGroupChat.resume "Link to this definition")
    :   Resume its participants when the team is running and paused by calling their
        [`on_resume()`](#autogen_agentchat.base.ChatAgent.on_resume "autogen_agentchat.base.ChatAgent.on_resume") method via direct RPC calls.

        Attention

        This is an experimental feature introduced in v0.4.9 and may subject
        to change or removal in the future.

        The team must be initialized before it can be resumed.

        Different from termination and restart with a new task, resuming the team
        does not cause the [`run()`](#autogen_agentchat.teams.BaseGroupChat.run "autogen_agentchat.teams.BaseGroupChat.run") or [`run_stream()`](#autogen_agentchat.teams.BaseGroupChat.run_stream "autogen_agentchat.teams.BaseGroupChat.run_stream") method to return.
        It calls the [`on_resume()`](#autogen_agentchat.base.ChatAgent.on_resume "autogen_agentchat.base.ChatAgent.on_resume") method on each
        participant, and if the participant does not implement the method, it
        will be a no-op.

        Note

        It is the responsibility of the agent class to handle the resume
        and ensure that the agent continues from where it was paused.
        Make sure to implement the [`on_resume()`](#autogen_agentchat.agents.BaseChatAgent.on_resume "autogen_agentchat.agents.BaseChatAgent.on_resume")
        method in your agent class for custom resume behavior.

        Raises:
        :   [**RuntimeError**](https://docs.python.org/3/library/exceptions.html#RuntimeError "(in Python v3.13)") – If the team has not been initialized. Exceptions from
            the participants when calling their implementations of [`on_resume`](#autogen_agentchat.base.ChatAgent.on_resume "autogen_agentchat.base.ChatAgent.on_resume")
            method are propagated to this method and raised.

    *async* run(*\**, *task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [TaskResult](#autogen_agentchat.base.TaskResult "autogen_agentchat.base._task.TaskResult")[#](#autogen_agentchat.teams.BaseGroupChat.run "Link to this definition")
    :   Run the team and return the result. The base implementation uses
        [`run_stream()`](#autogen_agentchat.teams.BaseGroupChat.run_stream "autogen_agentchat.teams.BaseGroupChat.run_stream") to run the team and then returns the final result.
        Once the team is stopped, the termination condition is reset.

        Parameters:
        :   * **task** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *|* *ChatMessage* *|* *Sequence**[**ChatMessage**]* *|* *None*) – The task to run the team with. Can be a string, a single `ChatMessage` , or a list of `ChatMessage`.
            * **cancellation\_token** ([*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken") *|* *None*) – The cancellation token to kill the task immediately.
              Setting the cancellation token potentially put the team in an inconsistent state,
              and it may not reset the termination condition.
              To gracefully stop the team, use [`ExternalTermination`](#autogen_agentchat.conditions.ExternalTermination "autogen_agentchat.conditions.ExternalTermination") instead.

        Returns:
        :   **result** – The result of the task as [`TaskResult`](#autogen_agentchat.base.TaskResult "autogen_agentchat.base.TaskResult"). The result contains the messages produced by the team and the stop reason.

        Example using the [`RoundRobinGroupChat`](#autogen_agentchat.teams.RoundRobinGroupChat "autogen_agentchat.teams.RoundRobinGroupChat") team:

        ```
        import asyncio
        from autogen_agentchat.agents import AssistantAgent
        from autogen_agentchat.conditions import MaxMessageTermination
        from autogen_agentchat.teams import RoundRobinGroupChat
        from autogen_ext.models.openai import OpenAIChatCompletionClient

        async def main() -> None:
            model_client = OpenAIChatCompletionClient(model="gpt-4o")

            agent1 = AssistantAgent("Assistant1", model_client=model_client)
            agent2 = AssistantAgent("Assistant2", model_client=model_client)
            termination = MaxMessageTermination(3)
            team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

            result = await team.run(task="Count from 1 to 10, respond one at a time.")
            print(result)

            # Run the team again without a task to continue the previous task.
            result = await team.run()
            print(result)

        asyncio.run(main())

        ```

        Example using the [`CancellationToken`](#autogen_core.CancellationToken "autogen_core.CancellationToken") to cancel the task:

        ```
        import asyncio
        from autogen_agentchat.agents import AssistantAgent
        from autogen_agentchat.conditions import MaxMessageTermination
        from autogen_agentchat.teams import RoundRobinGroupChat
        from autogen_core import CancellationToken
        from autogen_ext.models.openai import OpenAIChatCompletionClient

        async def main() -> None:
            model_client = OpenAIChatCompletionClient(model="gpt-4o")

            agent1 = AssistantAgent("Assistant1", model_client=model_client)
            agent2 = AssistantAgent("Assistant2", model_client=model_client)
            termination = MaxMessageTermination(3)
            team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

            cancellation_token = CancellationToken()

            # Create a task to run the team in the background.
            run_task = asyncio.create_task(
                team.run(
                    task="Count from 1 to 10, respond one at a time.",
                    cancellation_token=cancellation_token,
                )
            )

            # Wait for 1 second and then cancel the task.
            await asyncio.sleep(1)
            cancellation_token.cancel()

            # This will raise a cancellation error.
            await run_task

        asyncio.run(main())

        ```

    *async* run\_stream(*\**, *task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [TaskResult](#autogen_agentchat.base.TaskResult "autogen_agentchat.base._task.TaskResult"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_agentchat.teams.BaseGroupChat.run_stream "Link to this definition")
    :   Run the team and produces a stream of messages and the final result
        of the type [`TaskResult`](#autogen_agentchat.base.TaskResult "autogen_agentchat.base.TaskResult") as the last item in the stream. Once the
        team is stopped, the termination condition is reset.

        Note

        If an agent produces [`ModelClientStreamingChunkEvent`](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent"),
        the message will be yielded in the stream but it will not be included in the
        [`messages`](#autogen_agentchat.base.TaskResult.messages "autogen_agentchat.base.TaskResult.messages").

        Parameters:
        :   * **task** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *|* *ChatMessage* *|* *Sequence**[**ChatMessage**]* *|* *None*) – The task to run the team with. Can be a string, a single `ChatMessage` , or a list of `ChatMessage`.
            * **cancellation\_token** ([*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken") *|* *None*) – The cancellation token to kill the task immediately.
              Setting the cancellation token potentially put the team in an inconsistent state,
              and it may not reset the termination condition.
              To gracefully stop the team, use [`ExternalTermination`](#autogen_agentchat.conditions.ExternalTermination "autogen_agentchat.conditions.ExternalTermination") instead.

        Returns:
        :   **stream** – an [`AsyncGenerator`](https://docs.python.org/3/library/collections.abc.html#collections.abc.AsyncGenerator "(in Python v3.13)") that yields [`AgentEvent`](#autogen_agentchat.messages.AgentEvent "autogen_agentchat.messages.AgentEvent"), [`ChatMessage`](#autogen_agentchat.messages.ChatMessage "autogen_agentchat.messages.ChatMessage"), and the final result [`TaskResult`](#autogen_agentchat.base.TaskResult "autogen_agentchat.base.TaskResult") as the last item in the stream.

        Example using the [`RoundRobinGroupChat`](#autogen_agentchat.teams.RoundRobinGroupChat "autogen_agentchat.teams.RoundRobinGroupChat") team:

        ```
        import asyncio
        from autogen_agentchat.agents import AssistantAgent
        from autogen_agentchat.conditions import MaxMessageTermination
        from autogen_agentchat.teams import RoundRobinGroupChat
        from autogen_ext.models.openai import OpenAIChatCompletionClient

        async def main() -> None:
            model_client = OpenAIChatCompletionClient(model="gpt-4o")

            agent1 = AssistantAgent("Assistant1", model_client=model_client)
            agent2 = AssistantAgent("Assistant2", model_client=model_client)
            termination = MaxMessageTermination(3)
            team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

            stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
            async for message in stream:
                print(message)

            # Run the team again without a task to continue the previous task.
            stream = team.run_stream()
            async for message in stream:
                print(message)

        asyncio.run(main())

        ```

        Example using the [`CancellationToken`](#autogen_core.CancellationToken "autogen_core.CancellationToken") to cancel the task:

        ```
        import asyncio
        from autogen_agentchat.agents import AssistantAgent
        from autogen_agentchat.conditions import MaxMessageTermination
        from autogen_agentchat.ui import Console
        from autogen_agentchat.teams import RoundRobinGroupChat
        from autogen_core import CancellationToken
        from autogen_ext.models.openai import OpenAIChatCompletionClient

        async def main() -> None:
            model_client = OpenAIChatCompletionClient(model="gpt-4o")

            agent1 = AssistantAgent("Assistant1", model_client=model_client)
            agent2 = AssistantAgent("Assistant2", model_client=model_client)
            termination = MaxMessageTermination(3)
            team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

            cancellation_token = CancellationToken()

            # Create a task to run the team in the background.
            run_task = asyncio.create_task(
                Console(
                    team.run_stream(
                        task="Count from 1 to 10, respond one at a time.",
                        cancellation_token=cancellation_token,
                    )
                )
            )

            # Wait for 1 second and then cancel the task.
            await asyncio.sleep(1)
            cancellation_token.cancel()

            # This will raise a cancellation error.
            await run_task

        asyncio.run(main())

        ```

    *async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_agentchat.teams.BaseGroupChat.save_state "Link to this definition")
    :   Save the state of the group chat team.

        The state is saved by calling the [`agent_save_state()`](#autogen_core.AgentRuntime.agent_save_state "autogen_core.AgentRuntime.agent_save_state") method
        on each participant and the group chat manager with their internal agent ID.
        The state is returned as a nested dictionary: a dictionary with key agent\_states,
        which is a dictionary the agent names as keys and the state as values.

        ```
        {
            "agent_states": {
                "agent1": ...,
                "agent2": ...,
                "RoundRobinGroupChatManager": ...
            }
        }

        ```

        Note

        Starting v0.4.9, the state is using the agent name as the key instead of the agent ID,
        and the team\_id field is removed from the state. This is to allow the state to be
        portable across different teams and runtimes. States saved with the old format
        may not be compatible with the new format in the future.

        Caution

        When calling [`save_state()`](#autogen_agentchat.teams.BaseGroupChat.save_state "autogen_agentchat.teams.BaseGroupChat.save_state") on a team
        while it is running, the state may not be consistent and may result in an unexpected state.
        It is recommended to call this method when the team is not running or after it is stopped.

*class* MagenticOneGroupChat(*participants: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[ChatAgent](#autogen_agentchat.base.ChatAgent "autogen_agentchat.base._chat_agent.ChatAgent")]*, *model\_client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *\**, *termination\_condition: [TerminationCondition](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *max\_turns: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = 20*, *runtime: [AgentRuntime](#autogen_core.AgentRuntime "autogen_core._agent_runtime.AgentRuntime") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *max\_stalls: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 3*, *final\_answer\_prompt: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = ORCHESTRATOR\_FINAL\_ANSWER\_PROMPT*)[#](#autogen_agentchat.teams.MagenticOneGroupChat "Link to this definition")
:   Bases: [`BaseGroupChat`](#autogen_agentchat.teams.BaseGroupChat "autogen_agentchat.teams._group_chat._base_group_chat.BaseGroupChat"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`MagenticOneGroupChatConfig`]

    A team that runs a group chat with participants managed by the MagenticOneOrchestrator.

    The orchestrator handles the conversation flow, ensuring that the task is completed
    efficiently by managing the participants’ interactions.

    The orchestrator is based on the Magentic-One architecture, which is a generalist multi-agent system for solving complex tasks (see references below).

    Parameters:
    :   * **participants** (*List**[*[*ChatAgent*](#autogen_agentchat.base.ChatAgent "autogen_agentchat.base.ChatAgent")*]*) – The participants in the group chat.
        * **model\_client** ([*ChatCompletionClient*](#autogen_core.models.ChatCompletionClient "autogen_core.models.ChatCompletionClient")) – The model client used for generating responses.
        * **termination\_condition** ([*TerminationCondition*](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base.TerminationCondition")*,* *optional*) – The termination condition for the group chat. Defaults to None.
          Without a termination condition, the group chat will run based on the orchestrator logic or until the maximum number of turns is reached.
        * **max\_turns** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*,* *optional*) – The maximum number of turns in the group chat before stopping. Defaults to 20.
        * **max\_stalls** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*,* *optional*) – The maximum number of stalls allowed before re-planning. Defaults to 3.
        * **final\_answer\_prompt** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The LLM prompt used to generate the final answer or response from the team’s transcript. A default (sensible for GPT-4o class models) is provided.

    Raises:
    :   [**ValueError**](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.13)") – In orchestration logic if progress ledger does not have required keys or if next speaker is not valid.

    Examples:

    MagenticOneGroupChat with one assistant agent:

    > ```
    > import asyncio
    > from autogen_ext.models.openai import OpenAIChatCompletionClient
    > from autogen_agentchat.agents import AssistantAgent
    > from autogen_agentchat.teams import MagenticOneGroupChat
    > from autogen_agentchat.ui import Console
    >
    >
    > async def main() -> None:
    >     model_client = OpenAIChatCompletionClient(model="gpt-4o")
    >
    >     assistant = AssistantAgent(
    >         "Assistant",
    >         model_client=model_client,
    >     )
    >     team = MagenticOneGroupChat([assistant], model_client=model_client)
    >     await Console(team.run_stream(task="Provide a different proof to Fermat last theorem"))
    >
    >
    > asyncio.run(main())
    >
    > ```

    References

    If you use the MagenticOneGroupChat in your work, please cite the following paper:

    ```
    @article{fourney2024magentic,
        title={Magentic-one: A generalist multi-agent system for solving complex tasks},
        author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others},
        journal={arXiv preprint arXiv:2411.04468},
        year={2024}
    }

    ```

    *classmethod* \_from\_config(*config: MagenticOneGroupChatConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.teams.MagenticOneGroupChat._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → MagenticOneGroupChatConfig[#](#autogen_agentchat.teams.MagenticOneGroupChat._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.teams.MagenticOneGroupChat.component_config_schema "Link to this definition")
    :   alias of `MagenticOneGroupChatConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.teams.MagenticOneGroupChat'*[#](#autogen_agentchat.teams.MagenticOneGroupChat.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

*class* RoundRobinGroupChat(*participants: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[ChatAgent](#autogen_agentchat.base.ChatAgent "autogen_agentchat.base._chat_agent.ChatAgent")]*, *termination\_condition: [TerminationCondition](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *max\_turns: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *runtime: [AgentRuntime](#autogen_core.AgentRuntime "autogen_core._agent_runtime.AgentRuntime") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_agentchat.teams.RoundRobinGroupChat "Link to this definition")
:   Bases: [`BaseGroupChat`](#autogen_agentchat.teams.BaseGroupChat "autogen_agentchat.teams._group_chat._base_group_chat.BaseGroupChat"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`RoundRobinGroupChatConfig`]

    A team that runs a group chat with participants taking turns in a round-robin fashion
    to publish a message to all.

    If a single participant is in the team, the participant will be the only speaker.

    Parameters:
    :   * **participants** (*List**[*[*BaseChatAgent*](#autogen_agentchat.agents.BaseChatAgent "autogen_agentchat.agents.BaseChatAgent")*]*) – The participants in the group chat.
        * **termination\_condition** ([*TerminationCondition*](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base.TerminationCondition")*,* *optional*) – The termination condition for the group chat. Defaults to None.
          Without a termination condition, the group chat will run indefinitely.
        * **max\_turns** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*,* *optional*) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.

    Raises:
    :   [**ValueError**](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.13)") – If no participants are provided or if participant names are not unique.

    Examples:

    A team with one participant with tools:

    > ```
    > import asyncio
    > from autogen_ext.models.openai import OpenAIChatCompletionClient
    > from autogen_agentchat.agents import AssistantAgent
    > from autogen_agentchat.teams import RoundRobinGroupChat
    > from autogen_agentchat.conditions import TextMentionTermination
    > from autogen_agentchat.ui import Console
    >
    >
    > async def main() -> None:
    >     model_client = OpenAIChatCompletionClient(model="gpt-4o")
    >
    >     async def get_weather(location: str) -> str:
    >         return f"The weather in {location} is sunny."
    >
    >     assistant = AssistantAgent(
    >         "Assistant",
    >         model_client=model_client,
    >         tools=[get_weather],
    >     )
    >     termination = TextMentionTermination("TERMINATE")
    >     team = RoundRobinGroupChat([assistant], termination_condition=termination)
    >     await Console(team.run_stream(task="What's the weather in New York?"))
    >
    >
    > asyncio.run(main())
    >
    > ```

    A team with multiple participants:

    > ```
    > import asyncio
    > from autogen_ext.models.openai import OpenAIChatCompletionClient
    > from autogen_agentchat.agents import AssistantAgent
    > from autogen_agentchat.teams import RoundRobinGroupChat
    > from autogen_agentchat.conditions import TextMentionTermination
    > from autogen_agentchat.ui import Console
    >
    >
    > async def main() -> None:
    >     model_client = OpenAIChatCompletionClient(model="gpt-4o")
    >
    >     agent1 = AssistantAgent("Assistant1", model_client=model_client)
    >     agent2 = AssistantAgent("Assistant2", model_client=model_client)
    >     termination = TextMentionTermination("TERMINATE")
    >     team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    >     await Console(team.run_stream(task="Tell me some jokes."))
    >
    >
    > asyncio.run(main())
    >
    > ```

    *classmethod* \_from\_config(*config: RoundRobinGroupChatConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.teams.RoundRobinGroupChat._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → RoundRobinGroupChatConfig[#](#autogen_agentchat.teams.RoundRobinGroupChat._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.teams.RoundRobinGroupChat.component_config_schema "Link to this definition")
    :   alias of `RoundRobinGroupChatConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.teams.RoundRobinGroupChat'*[#](#autogen_agentchat.teams.RoundRobinGroupChat.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

*class* SelectorGroupChat(*participants: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[ChatAgent](#autogen_agentchat.base.ChatAgent "autogen_agentchat.base._chat_agent.ChatAgent")]*, *model\_client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *\**, *termination\_condition: [TerminationCondition](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *max\_turns: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *runtime: [AgentRuntime](#autogen_core.AgentRuntime "autogen_core._agent_runtime.AgentRuntime") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *selector\_prompt: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'You are in a role play game. The following roles are available:\n{roles}.\nRead the following conversation. Then select the next role from {participants} to play. Only return the role.\n\n{history}\n\nRead the above conversation. Then select the next role from {participants} to play. Only return the role.\n'*, *allow\_repeated\_speaker: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*, *max\_selector\_attempts: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 3*, *selector\_func: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]], [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *candidate\_func: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]], [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_agentchat.teams.SelectorGroupChat "Link to this definition")
:   Bases: [`BaseGroupChat`](#autogen_agentchat.teams.BaseGroupChat "autogen_agentchat.teams._group_chat._base_group_chat.BaseGroupChat"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`SelectorGroupChatConfig`]

    A group chat team that have participants takes turn to publish a message
    to all, using a ChatCompletion model to select the next speaker after each message.

    Parameters:
    :   * **participants** (*List**[*[*ChatAgent*](#autogen_agentchat.base.ChatAgent "autogen_agentchat.base.ChatAgent")*]*) – The participants in the group chat,
          must have unique names and at least two participants.
        * **model\_client** ([*ChatCompletionClient*](#autogen_core.models.ChatCompletionClient "autogen_core.models.ChatCompletionClient")) – The ChatCompletion model client used
          to select the next speaker.
        * **termination\_condition** ([*TerminationCondition*](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base.TerminationCondition")*,* *optional*) – The termination condition for the group chat. Defaults to None.
          Without a termination condition, the group chat will run indefinitely.
        * **max\_turns** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*,* *optional*) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.
        * **selector\_prompt** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The prompt template to use for selecting the next speaker.
          Available fields: ‘{roles}’, ‘{participants}’, and ‘{history}’.
        * **allow\_repeated\_speaker** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – Whether to include the previous speaker in the list of candidates to be selected for the next turn.
          Defaults to False. The model may still select the previous speaker – a warning will be logged if this happens.
        * **max\_selector\_attempts** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*,* *optional*) – The maximum number of attempts to select a speaker using the model. Defaults to 3.
          If the model fails to select a speaker after the maximum number of attempts, the previous speaker will be used if available,
          otherwise the first participant will be used.
        * **selector\_func** (*Callable**[**[**Sequence**[**AgentEvent* *|* *ChatMessage**]**]**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *|* *None**]**,* *optional*) – A custom selector
          function that takes the conversation history and returns the name of the next speaker.
          If provided, this function will be used to override the model to select the next speaker.
          If the function returns None, the model will be used to select the next speaker.
        * **candidate\_func** (*Callable**[**[**Sequence**[**AgentEvent* *|* *ChatMessage**]**]**,* *List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]**]**,* *optional*) – A custom function that takes the conversation history and returns a filtered list of candidates for the next speaker
          selection using model. If the function returns an empty list or None, SelectorGroupChat will raise a ValueError.
          This function is only used if selector\_func is not set. The allow\_repeated\_speaker will be ignored if set.

    Raises:
    :   [**ValueError**](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.13)") – If the number of participants is less than two or if the selector prompt is invalid.

    Examples:

    A team with multiple participants:

    > ```
    > import asyncio
    > from autogen_ext.models.openai import OpenAIChatCompletionClient
    > from autogen_agentchat.agents import AssistantAgent
    > from autogen_agentchat.teams import SelectorGroupChat
    > from autogen_agentchat.conditions import TextMentionTermination
    > from autogen_agentchat.ui import Console
    >
    >
    > async def main() -> None:
    >     model_client = OpenAIChatCompletionClient(model="gpt-4o")
    >
    >     async def lookup_hotel(location: str) -> str:
    >         return f"Here are some hotels in {location}: hotel1, hotel2, hotel3."
    >
    >     async def lookup_flight(origin: str, destination: str) -> str:
    >         return f"Here are some flights from {origin} to {destination}: flight1, flight2, flight3."
    >
    >     async def book_trip() -> str:
    >         return "Your trip is booked!"
    >
    >     travel_advisor = AssistantAgent(
    >         "Travel_Advisor",
    >         model_client,
    >         tools=[book_trip],
    >         description="Helps with travel planning.",
    >     )
    >     hotel_agent = AssistantAgent(
    >         "Hotel_Agent",
    >         model_client,
    >         tools=[lookup_hotel],
    >         description="Helps with hotel booking.",
    >     )
    >     flight_agent = AssistantAgent(
    >         "Flight_Agent",
    >         model_client,
    >         tools=[lookup_flight],
    >         description="Helps with flight booking.",
    >     )
    >     termination = TextMentionTermination("TERMINATE")
    >     team = SelectorGroupChat(
    >         [travel_advisor, hotel_agent, flight_agent],
    >         model_client=model_client,
    >         termination_condition=termination,
    >     )
    >     await Console(team.run_stream(task="Book a 3-day trip to new york."))
    >
    >
    > asyncio.run(main())
    >
    > ```

    A team with a custom selector function:

    > ```
    > import asyncio
    > from typing import Sequence
    > from autogen_ext.models.openai import OpenAIChatCompletionClient
    > from autogen_agentchat.agents import AssistantAgent
    > from autogen_agentchat.teams import SelectorGroupChat
    > from autogen_agentchat.conditions import TextMentionTermination
    > from autogen_agentchat.ui import Console
    > from autogen_agentchat.messages import AgentEvent, ChatMessage
    >
    >
    > async def main() -> None:
    >     model_client = OpenAIChatCompletionClient(model="gpt-4o")
    >
    >     def check_calculation(x: int, y: int, answer: int) -> str:
    >         if x + y == answer:
    >             return "Correct!"
    >         else:
    >             return "Incorrect!"
    >
    >     agent1 = AssistantAgent(
    >         "Agent1",
    >         model_client,
    >         description="For calculation",
    >         system_message="Calculate the sum of two numbers",
    >     )
    >     agent2 = AssistantAgent(
    >         "Agent2",
    >         model_client,
    >         tools=[check_calculation],
    >         description="For checking calculation",
    >         system_message="Check the answer and respond with 'Correct!' or 'Incorrect!'",
    >     )
    >
    >     def selector_func(messages: Sequence[AgentEvent | ChatMessage]) -> str | None:
    >         if len(messages) == 1 or messages[-1].content == "Incorrect!":
    >             return "Agent1"
    >         if messages[-1].source == "Agent1":
    >             return "Agent2"
    >         return None
    >
    >     termination = TextMentionTermination("Correct!")
    >     team = SelectorGroupChat(
    >         [agent1, agent2],
    >         model_client=model_client,
    >         selector_func=selector_func,
    >         termination_condition=termination,
    >     )
    >
    >     await Console(team.run_stream(task="What is 1 + 1?"))
    >
    >
    > asyncio.run(main())
    >
    > ```

    *classmethod* \_from\_config(*config: SelectorGroupChatConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.teams.SelectorGroupChat._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → SelectorGroupChatConfig[#](#autogen_agentchat.teams.SelectorGroupChat._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.teams.SelectorGroupChat.component_config_schema "Link to this definition")
    :   alias of `SelectorGroupChatConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.teams.SelectorGroupChat'*[#](#autogen_agentchat.teams.SelectorGroupChat.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

*class* Swarm(*participants: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[ChatAgent](#autogen_agentchat.base.ChatAgent "autogen_agentchat.base._chat_agent.ChatAgent")]*, *termination\_condition: [TerminationCondition](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *max\_turns: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *runtime: [AgentRuntime](#autogen_core.AgentRuntime "autogen_core._agent_runtime.AgentRuntime") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_agentchat.teams.Swarm "Link to this definition")
:   Bases: [`BaseGroupChat`](#autogen_agentchat.teams.BaseGroupChat "autogen_agentchat.teams._group_chat._base_group_chat.BaseGroupChat"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`SwarmConfig`]

    A group chat team that selects the next speaker based on handoff message only.

    The first participant in the list of participants is the initial speaker.
    The next speaker is selected based on the [`HandoffMessage`](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage") message
    sent by the current speaker. If no handoff message is sent, the current speaker
    continues to be the speaker.

    Parameters:
    :   * **participants** (*List**[*[*ChatAgent*](#autogen_agentchat.base.ChatAgent "autogen_agentchat.base.ChatAgent")*]*) – The agents participating in the group chat. The first agent in the list is the initial speaker.
        * **termination\_condition** ([*TerminationCondition*](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base.TerminationCondition")*,* *optional*) – The termination condition for the group chat. Defaults to None.
          Without a termination condition, the group chat will run indefinitely.
        * **max\_turns** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*,* *optional*) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.

    Basic example:

    > ```
    > import asyncio
    > from autogen_ext.models.openai import OpenAIChatCompletionClient
    > from autogen_agentchat.agents import AssistantAgent
    > from autogen_agentchat.teams import Swarm
    > from autogen_agentchat.conditions import MaxMessageTermination
    >
    >
    > async def main() -> None:
    >     model_client = OpenAIChatCompletionClient(model="gpt-4o")
    >
    >     agent1 = AssistantAgent(
    >         "Alice",
    >         model_client=model_client,
    >         handoffs=["Bob"],
    >         system_message="You are Alice and you only answer questions about yourself.",
    >     )
    >     agent2 = AssistantAgent(
    >         "Bob", model_client=model_client, system_message="You are Bob and your birthday is on 1st January."
    >     )
    >
    >     termination = MaxMessageTermination(3)
    >     team = Swarm([agent1, agent2], termination_condition=termination)
    >
    >     stream = team.run_stream(task="What is bob's birthday?")
    >     async for message in stream:
    >         print(message)
    >
    >
    > asyncio.run(main())
    >
    > ```

    Using the [`HandoffTermination`](#autogen_agentchat.conditions.HandoffTermination "autogen_agentchat.conditions.HandoffTermination") for human-in-the-loop handoff:

    > ```
    > import asyncio
    > from autogen_ext.models.openai import OpenAIChatCompletionClient
    > from autogen_agentchat.agents import AssistantAgent
    > from autogen_agentchat.teams import Swarm
    > from autogen_agentchat.conditions import HandoffTermination, MaxMessageTermination
    > from autogen_agentchat.ui import Console
    > from autogen_agentchat.messages import HandoffMessage
    >
    >
    > async def main() -> None:
    >     model_client = OpenAIChatCompletionClient(model="gpt-4o")
    >
    >     agent = AssistantAgent(
    >         "Alice",
    >         model_client=model_client,
    >         handoffs=["user"],
    >         system_message="You are Alice and you only answer questions about yourself, ask the user for help if needed.",
    >     )
    >     termination = HandoffTermination(target="user") | MaxMessageTermination(3)
    >     team = Swarm([agent], termination_condition=termination)
    >
    >     # Start the conversation.
    >     await Console(team.run_stream(task="What is bob's birthday?"))
    >
    >     # Resume with user feedback.
    >     await Console(
    >         team.run_stream(
    >             task=HandoffMessage(source="user", target="Alice", content="Bob's birthday is on 1st January.")
    >         )
    >     )
    >
    >
    > asyncio.run(main())
    >
    > ```

    *classmethod* \_from\_config(*config: SwarmConfig*) → [Swarm](#autogen_agentchat.teams.Swarm "autogen_agentchat.teams._group_chat._swarm_group_chat.Swarm")[#](#autogen_agentchat.teams.Swarm._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → SwarmConfig[#](#autogen_agentchat.teams.Swarm._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.teams.Swarm.component_config_schema "Link to this definition")
    :   alias of `SwarmConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.teams.Swarm'*[#](#autogen_agentchat.teams.Swarm.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

### autogen\_agentchat.base[#](#module-autogen_agentchat.base "Link to this heading")

*class* AndTerminationCondition(*\*conditions: [TerminationCondition](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition")*)[#](#autogen_agentchat.base.AndTerminationCondition "Link to this definition")
:   Bases: [`TerminationCondition`](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`AndTerminationConditionConfig`]

    component\_config\_schema[#](#autogen_agentchat.base.AndTerminationCondition.component_config_schema "Link to this definition")
    :   alias of `AndTerminationConditionConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.base.AndTerminationCondition'*[#](#autogen_agentchat.base.AndTerminationCondition.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    component\_type*: ClassVar[ComponentType]* *= 'termination'*[#](#autogen_agentchat.base.AndTerminationCondition.component_type "Link to this definition")
    :   The logical type of the component.

    *async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.base.AndTerminationCondition.reset "Link to this definition")
    :   Reset the termination condition.

    *property* terminated*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*[#](#autogen_agentchat.base.AndTerminationCondition.terminated "Link to this definition")
    :   Check if the termination condition has been reached

*class* ChatAgent(*\*args*, *\*\*kwargs*)[#](#autogen_agentchat.base.ChatAgent "Link to this definition")
:   Bases: [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC "(in Python v3.13)"), [`TaskRunner`](#autogen_agentchat.base.TaskRunner "autogen_agentchat.base._task.TaskRunner"), [`ComponentBase`](#autogen_core.ComponentBase "autogen_core._component_config.ComponentBase")[`BaseModel`]

    Protocol for a chat agent.

    *abstract async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.base.ChatAgent.close "Link to this definition")
    :   Release any resources held by the agent.

    component\_type*: ClassVar[ComponentType]* *= 'agent'*[#](#autogen_agentchat.base.ChatAgent.component_type "Link to this definition")
    :   The logical type of the component.

    *abstract property* description*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_agentchat.base.ChatAgent.description "Link to this definition")
    :   The description of the agent. This is used by team to
        make decisions about which agents to use. The description should
        describe the agent’s capabilities and how to interact with it.

    *abstract async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.base.ChatAgent.load_state "Link to this definition")
    :   Restore agent from saved state

    *abstract property* name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_agentchat.base.ChatAgent.name "Link to this definition")
    :   The name of the agent. This is used by team to uniquely identify
        the agent. It should be unique within the team.

    *abstract async* on\_messages(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response")[#](#autogen_agentchat.base.ChatAgent.on_messages "Link to this definition")
    :   Handles incoming messages and returns a response.

    *abstract* on\_messages\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_agentchat.base.ChatAgent.on_messages_stream "Link to this definition")
    :   Handles incoming messages and returns a stream of inner messages and
        and the final item is the response.

    *abstract async* on\_pause(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.base.ChatAgent.on_pause "Link to this definition")
    :   Called when the agent is paused. The agent may be running in [`on_messages()`](#autogen_agentchat.base.ChatAgent.on_messages "autogen_agentchat.base.ChatAgent.on_messages") or
        [`on_messages_stream()`](#autogen_agentchat.base.ChatAgent.on_messages_stream "autogen_agentchat.base.ChatAgent.on_messages_stream") when this method is called.

    *abstract async* on\_reset(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.base.ChatAgent.on_reset "Link to this definition")
    :   Resets the agent to its initialization state.

    *abstract async* on\_resume(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.base.ChatAgent.on_resume "Link to this definition")
    :   Called when the agent is resumed. The agent may be running in [`on_messages()`](#autogen_agentchat.base.ChatAgent.on_messages "autogen_agentchat.base.ChatAgent.on_messages") or
        [`on_messages_stream()`](#autogen_agentchat.base.ChatAgent.on_messages_stream "autogen_agentchat.base.ChatAgent.on_messages_stream") when this method is called.

    *abstract property* produced\_message\_types*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]*[#](#autogen_agentchat.base.ChatAgent.produced_message_types "Link to this definition")
    :   The types of messages that the agent produces in the
        [`Response.chat_message`](#autogen_agentchat.base.Response.chat_message "autogen_agentchat.base.Response.chat_message") field. They must be `ChatMessage` types.

    *abstract async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_agentchat.base.ChatAgent.save_state "Link to this definition")
    :   Save agent state for later restoration

*pydantic model* Handoff[#](#autogen_agentchat.base.Handoff "Link to this definition")
:   Bases: `BaseModel`

    Handoff configuration.

    Show JSON schema

    ```
    {
       "title": "Handoff",
       "description": "Handoff configuration.",
       "type": "object",
       "properties": {
          "target": {
             "title": "Target",
             "type": "string"
          },
          "description": {
             "default": "",
             "title": "Description",
             "type": "string"
          },
          "name": {
             "default": "",
             "title": "Name",
             "type": "string"
          },
          "message": {
             "default": "",
             "title": "Message",
             "type": "string"
          }
       },
       "required": [
          "target"
       ]
    }

    ```

    Fields:
    :   * `description (str)`
        * `message (str)`
        * `name (str)`
        * `target (str)`

    Validators:
    :   * `set_defaults` » `all fields`

    *field* description*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= ''*[#](#autogen_agentchat.base.Handoff.description "Link to this definition")
    :   The description of the handoff such as the condition under which it should happen and the target agent’s ability.
        If not provided, it is generated from the target agent’s name.

        Validated by:
        :   * `set_defaults`

    *field* message*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= ''*[#](#autogen_agentchat.base.Handoff.message "Link to this definition")
    :   The message to the target agent.
        If not provided, it is generated from the target agent’s name.

        Validated by:
        :   * `set_defaults`

    *field* name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= ''*[#](#autogen_agentchat.base.Handoff.name "Link to this definition")
    :   The name of this handoff configuration. If not provided, it is generated from the target agent’s name.

        Validated by:
        :   * `set_defaults`

    *field* target*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_agentchat.base.Handoff.target "Link to this definition")
    :   The name of the target agent to handoff to.

        Validated by:
        :   * `set_defaults`

    *validator* set\_defaults*»* *all fields*[#](#autogen_agentchat.base.Handoff.set_defaults "Link to this definition")

    *property* handoff\_tool*: [BaseTool](#autogen_core.tools.BaseTool "autogen_core.tools._base.BaseTool")[BaseModel, BaseModel]*[#](#autogen_agentchat.base.Handoff.handoff_tool "Link to this definition")
    :   Create a handoff tool from this handoff configuration.

*class* OrTerminationCondition(*\*conditions: [TerminationCondition](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition")*)[#](#autogen_agentchat.base.OrTerminationCondition "Link to this definition")
:   Bases: [`TerminationCondition`](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`OrTerminationConditionConfig`]

    component\_config\_schema[#](#autogen_agentchat.base.OrTerminationCondition.component_config_schema "Link to this definition")
    :   alias of `OrTerminationConditionConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.base.OrTerminationCondition'*[#](#autogen_agentchat.base.OrTerminationCondition.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    component\_type*: ClassVar[ComponentType]* *= 'termination'*[#](#autogen_agentchat.base.OrTerminationCondition.component_type "Link to this definition")
    :   The logical type of the component.

    *async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.base.OrTerminationCondition.reset "Link to this definition")
    :   Reset the termination condition.

    *property* terminated*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*[#](#autogen_agentchat.base.OrTerminationCondition.terminated "Link to this definition")
    :   Check if the termination condition has been reached

*class* Response(*\**, *chat\_message: [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]*, *inner\_messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_agentchat.base.Response "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    A response from calling [`ChatAgent.on_messages()`](#autogen_agentchat.base.ChatAgent.on_messages "autogen_agentchat.base.ChatAgent.on_messages").

    chat\_message*: [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]*[#](#autogen_agentchat.base.Response.chat_message "Link to this definition")
    :   A chat message produced by the agent as the response.

    inner\_messages*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_agentchat.base.Response.inner_messages "Link to this definition")
    :   Inner messages produced by the agent, they can be `AgentEvent`
        or `ChatMessage`.

*class* TaskResult(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *stop\_reason: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_agentchat.base.TaskResult "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    Result of running a task.

    messages*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*[#](#autogen_agentchat.base.TaskResult.messages "Link to this definition")
    :   Messages produced by the task.

    stop\_reason*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_agentchat.base.TaskResult.stop_reason "Link to this definition")
    :   The reason the task stopped.

*class* TaskRunner(*\*args*, *\*\*kwargs*)[#](#autogen_agentchat.base.TaskRunner "Link to this definition")
:   Bases: [`Protocol`](https://docs.python.org/3/library/typing.html#typing.Protocol "(in Python v3.13)")

    A task runner.

    *async* run(*\**, *task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [TaskResult](#autogen_agentchat.base.TaskResult "autogen_agentchat.base._task.TaskResult")[#](#autogen_agentchat.base.TaskRunner.run "Link to this definition")
    :   Run the task and return the result.

        The task can be a string, a single message, or a sequence of messages.

        The runner is stateful and a subsequent call to this method will continue
        from where the previous call left off. If the task is not specified,
        the runner will continue with the current task.

    run\_stream(*\**, *task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [TaskResult](#autogen_agentchat.base.TaskResult "autogen_agentchat.base._task.TaskResult"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_agentchat.base.TaskRunner.run_stream "Link to this definition")
    :   Run the task and produces a stream of messages and the final result
        [`TaskResult`](#autogen_agentchat.base.TaskResult "autogen_agentchat.base.TaskResult") as the last item in the stream.

        The task can be a string, a single message, or a sequence of messages.

        The runner is stateful and a subsequent call to this method will continue
        from where the previous call left off. If the task is not specified,
        the runner will continue with the current task.

*class* Team(*\*args*, *\*\*kwargs*)[#](#autogen_agentchat.base.Team "Link to this definition")
:   Bases: [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC "(in Python v3.13)"), [`TaskRunner`](#autogen_agentchat.base.TaskRunner "autogen_agentchat.base._task.TaskRunner"), [`ComponentBase`](#autogen_core.ComponentBase "autogen_core._component_config.ComponentBase")[`BaseModel`]

    component\_type*: ClassVar[ComponentType]* *= 'team'*[#](#autogen_agentchat.base.Team.component_type "Link to this definition")
    :   The logical type of the component.

    *abstract async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.base.Team.load_state "Link to this definition")
    :   Load the state of the team.

    *abstract async* pause() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.base.Team.pause "Link to this definition")
    :   Pause the team and all its participants. This is useful for
        pausing the [`autogen_agentchat.base.TaskRunner.run()`](#autogen_agentchat.base.TaskRunner.run "autogen_agentchat.base.TaskRunner.run") or
        [`autogen_agentchat.base.TaskRunner.run_stream()`](#autogen_agentchat.base.TaskRunner.run_stream "autogen_agentchat.base.TaskRunner.run_stream") methods from
        concurrently, while keeping them alive.

    *abstract async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.base.Team.reset "Link to this definition")
    :   Reset the team and all its participants to its initial state.

    *abstract async* resume() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.base.Team.resume "Link to this definition")
    :   Resume the team and all its participants from a pause after
        [`pause()`](#autogen_agentchat.base.Team.pause "autogen_agentchat.base.Team.pause") was called.

    *abstract async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_agentchat.base.Team.save_state "Link to this definition")
    :   Save the current state of the team.

*exception* TerminatedException[#](#autogen_agentchat.base.TerminatedException "Link to this definition")
:   Bases: [`BaseException`](https://docs.python.org/3/library/exceptions.html#BaseException "(in Python v3.13)")

*class* TerminationCondition[#](#autogen_agentchat.base.TerminationCondition "Link to this definition")
:   Bases: [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC "(in Python v3.13)"), [`ComponentBase`](#autogen_core.ComponentBase "autogen_core._component_config.ComponentBase")[`BaseModel`]

    A stateful condition that determines when a conversation should be terminated.

    A termination condition is a callable that takes a sequence of ChatMessage objects
    since the last time the condition was called, and returns a StopMessage if the
    conversation should be terminated, or None otherwise.
    Once a termination condition has been reached, it must be reset before it can be used again.

    Termination conditions can be combined using the AND and OR operators.

    Example

    ```
    import asyncio
    from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination

    async def main() -> None:
        # Terminate the conversation after 10 turns or if the text "TERMINATE" is mentioned.
        cond1 = MaxMessageTermination(10) | TextMentionTermination("TERMINATE")

        # Terminate the conversation after 10 turns and if the text "TERMINATE" is mentioned.
        cond2 = MaxMessageTermination(10) & TextMentionTermination("TERMINATE")

        # ...

        # Reset the termination condition.
        await cond1.reset()
        await cond2.reset()

    asyncio.run(main())

    ```

    component\_type*: ClassVar[ComponentType]* *= 'termination'*[#](#autogen_agentchat.base.TerminationCondition.component_type "Link to this definition")
    :   The logical type of the component.

    *abstract async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.base.TerminationCondition.reset "Link to this definition")
    :   Reset the termination condition.

    *abstract property* terminated*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*[#](#autogen_agentchat.base.TerminationCondition.terminated "Link to this definition")
    :   Check if the termination condition has been reached

### autogen\_agentchat.conditions[#](#module-autogen_agentchat.conditions "Link to this heading")

This module provides various termination conditions for controlling the behavior of
multi-agent teams.

*class* ExternalTermination[#](#autogen_agentchat.conditions.ExternalTermination "Link to this definition")
:   Bases: [`TerminationCondition`](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`ExternalTerminationConfig`]

    A termination condition that is externally controlled
    by calling the [`set()`](#autogen_agentchat.conditions.ExternalTermination.set "autogen_agentchat.conditions.ExternalTermination.set") method.

    Example:

    ```
    from autogen_agentchat.conditions import ExternalTermination

    termination = ExternalTermination()

    # Run the team in an asyncio task.
    ...

    # Set the termination condition externally
    termination.set()

    ```

    *classmethod* \_from\_config(*config: ExternalTerminationConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.conditions.ExternalTermination._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → ExternalTerminationConfig[#](#autogen_agentchat.conditions.ExternalTermination._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.conditions.ExternalTermination.component_config_schema "Link to this definition")
    :   alias of `ExternalTerminationConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.conditions.ExternalTermination'*[#](#autogen_agentchat.conditions.ExternalTermination.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.conditions.ExternalTermination.reset "Link to this definition")
    :   Reset the termination condition.

    set() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.conditions.ExternalTermination.set "Link to this definition")
    :   Set the termination condition to terminated.

    *property* terminated*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*[#](#autogen_agentchat.conditions.ExternalTermination.terminated "Link to this definition")
    :   Check if the termination condition has been reached

*class* FunctionCallTermination(*function\_name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_agentchat.conditions.FunctionCallTermination "Link to this definition")
:   Bases: [`TerminationCondition`](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`FunctionCallTerminationConfig`]

    Terminate the conversation if a [`FunctionExecutionResult`](#autogen_core.models.FunctionExecutionResult "autogen_core.models.FunctionExecutionResult")
    with a specific name was received.

    Parameters:
    :   **function\_name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The name of the function to look for in the messages.

    Raises:
    :   [**TerminatedException**](#autogen_agentchat.base.TerminatedException "autogen_agentchat.base.TerminatedException") – If the termination condition has already been reached.

    *classmethod* \_from\_config(*config: FunctionCallTerminationConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.conditions.FunctionCallTermination._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → FunctionCallTerminationConfig[#](#autogen_agentchat.conditions.FunctionCallTermination._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.conditions.FunctionCallTermination.component_config_schema "Link to this definition")
    :   alias of `FunctionCallTerminationConfig`

    *async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.conditions.FunctionCallTermination.reset "Link to this definition")
    :   Reset the termination condition.

    *property* terminated*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*[#](#autogen_agentchat.conditions.FunctionCallTermination.terminated "Link to this definition")
    :   Check if the termination condition has been reached

*class* HandoffTermination(*target: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_agentchat.conditions.HandoffTermination "Link to this definition")
:   Bases: [`TerminationCondition`](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`HandoffTerminationConfig`]

    Terminate the conversation if a [`HandoffMessage`](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage")
    with the given target is received.

    Parameters:
    :   **target** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The target of the handoff message.

    *classmethod* \_from\_config(*config: HandoffTerminationConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.conditions.HandoffTermination._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → HandoffTerminationConfig[#](#autogen_agentchat.conditions.HandoffTermination._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.conditions.HandoffTermination.component_config_schema "Link to this definition")
    :   alias of `HandoffTerminationConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.conditions.HandoffTermination'*[#](#autogen_agentchat.conditions.HandoffTermination.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.conditions.HandoffTermination.reset "Link to this definition")
    :   Reset the termination condition.

    *property* terminated*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*[#](#autogen_agentchat.conditions.HandoffTermination.terminated "Link to this definition")
    :   Check if the termination condition has been reached

*class* MaxMessageTermination(*max\_messages: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*, *include\_agent\_event: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*)[#](#autogen_agentchat.conditions.MaxMessageTermination "Link to this definition")
:   Bases: [`TerminationCondition`](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`MaxMessageTerminationConfig`]

    Terminate the conversation after a maximum number of messages have been exchanged.

    Parameters:
    :   * **max\_messages** – The maximum number of messages allowed in the conversation.
        * **include\_agent\_event** – If True, include [`AgentEvent`](#autogen_agentchat.messages.AgentEvent "autogen_agentchat.messages.AgentEvent") in the message count.
          Otherwise, only include [`ChatMessage`](#autogen_agentchat.messages.ChatMessage "autogen_agentchat.messages.ChatMessage"). Defaults to False.

    *classmethod* \_from\_config(*config: MaxMessageTerminationConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.conditions.MaxMessageTermination._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → MaxMessageTerminationConfig[#](#autogen_agentchat.conditions.MaxMessageTermination._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.conditions.MaxMessageTermination.component_config_schema "Link to this definition")
    :   alias of `MaxMessageTerminationConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.conditions.MaxMessageTermination'*[#](#autogen_agentchat.conditions.MaxMessageTermination.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.conditions.MaxMessageTermination.reset "Link to this definition")
    :   Reset the termination condition.

    *property* terminated*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*[#](#autogen_agentchat.conditions.MaxMessageTermination.terminated "Link to this definition")
    :   Check if the termination condition has been reached

*class* SourceMatchTermination(*sources: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*)[#](#autogen_agentchat.conditions.SourceMatchTermination "Link to this definition")
:   Bases: [`TerminationCondition`](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`SourceMatchTerminationConfig`]

    Terminate the conversation after a specific source responds.

    Parameters:
    :   **sources** (*List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]*) – List of source names to terminate the conversation.

    Raises:
    :   [**TerminatedException**](#autogen_agentchat.base.TerminatedException "autogen_agentchat.base.TerminatedException") – If the termination condition has already been reached.

    *classmethod* \_from\_config(*config: SourceMatchTerminationConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.conditions.SourceMatchTermination._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → SourceMatchTerminationConfig[#](#autogen_agentchat.conditions.SourceMatchTermination._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.conditions.SourceMatchTermination.component_config_schema "Link to this definition")
    :   alias of `SourceMatchTerminationConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.conditions.SourceMatchTermination'*[#](#autogen_agentchat.conditions.SourceMatchTermination.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.conditions.SourceMatchTermination.reset "Link to this definition")
    :   Reset the termination condition.

    *property* terminated*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*[#](#autogen_agentchat.conditions.SourceMatchTermination.terminated "Link to this definition")
    :   Check if the termination condition has been reached

*class* StopMessageTermination[#](#autogen_agentchat.conditions.StopMessageTermination "Link to this definition")
:   Bases: [`TerminationCondition`](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`StopMessageTerminationConfig`]

    Terminate the conversation if a StopMessage is received.

    *classmethod* \_from\_config(*config: StopMessageTerminationConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.conditions.StopMessageTermination._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → StopMessageTerminationConfig[#](#autogen_agentchat.conditions.StopMessageTermination._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.conditions.StopMessageTermination.component_config_schema "Link to this definition")
    :   alias of `StopMessageTerminationConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.conditions.StopMessageTermination'*[#](#autogen_agentchat.conditions.StopMessageTermination.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.conditions.StopMessageTermination.reset "Link to this definition")
    :   Reset the termination condition.

    *property* terminated*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*[#](#autogen_agentchat.conditions.StopMessageTermination.terminated "Link to this definition")
    :   Check if the termination condition has been reached

*class* TextMentionTermination(*text: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *sources: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_agentchat.conditions.TextMentionTermination "Link to this definition")
:   Bases: [`TerminationCondition`](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`TextMentionTerminationConfig`]

    Terminate the conversation if a specific text is mentioned.

    Parameters:
    :   * **text** – The text to look for in the messages.
        * **sources** – Check only messages of the specified agents for the text to look for.

    *classmethod* \_from\_config(*config: TextMentionTerminationConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.conditions.TextMentionTermination._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → TextMentionTerminationConfig[#](#autogen_agentchat.conditions.TextMentionTermination._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.conditions.TextMentionTermination.component_config_schema "Link to this definition")
    :   alias of `TextMentionTerminationConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.conditions.TextMentionTermination'*[#](#autogen_agentchat.conditions.TextMentionTermination.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.conditions.TextMentionTermination.reset "Link to this definition")
    :   Reset the termination condition.

    *property* terminated*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*[#](#autogen_agentchat.conditions.TextMentionTermination.terminated "Link to this definition")
    :   Check if the termination condition has been reached

*class* TextMessageTermination(*source: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_agentchat.conditions.TextMessageTermination "Link to this definition")
:   Bases: [`TerminationCondition`](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`TextMessageTerminationConfig`]

    Terminate the conversation if a [`TextMessage`](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") is received.

    This termination condition checks for TextMessage instances in the message sequence. When a TextMessage is found,
    it terminates the conversation if either:
    - No source was specified (terminates on any TextMessage)
    - The message source matches the specified source

    Parameters:
    :   **source** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *|* *None**,* *optional*) – The source name to match against incoming messages. If None, matches any source.
        Defaults to None.

    *classmethod* \_from\_config(*config: TextMessageTerminationConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.conditions.TextMessageTermination._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → TextMessageTerminationConfig[#](#autogen_agentchat.conditions.TextMessageTermination._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.conditions.TextMessageTermination.component_config_schema "Link to this definition")
    :   alias of `TextMessageTerminationConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.conditions.TextMessageTermination'*[#](#autogen_agentchat.conditions.TextMessageTermination.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.conditions.TextMessageTermination.reset "Link to this definition")
    :   Reset the termination condition.

    *property* terminated*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*[#](#autogen_agentchat.conditions.TextMessageTermination.terminated "Link to this definition")
    :   Check if the termination condition has been reached

*class* TimeoutTermination(*timeout\_seconds: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*)[#](#autogen_agentchat.conditions.TimeoutTermination "Link to this definition")
:   Bases: [`TerminationCondition`](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`TimeoutTerminationConfig`]

    Terminate the conversation after a specified duration has passed.

    Parameters:
    :   **timeout\_seconds** – The maximum duration in seconds before terminating the conversation.

    *classmethod* \_from\_config(*config: TimeoutTerminationConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.conditions.TimeoutTermination._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → TimeoutTerminationConfig[#](#autogen_agentchat.conditions.TimeoutTermination._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.conditions.TimeoutTermination.component_config_schema "Link to this definition")
    :   alias of `TimeoutTerminationConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.conditions.TimeoutTermination'*[#](#autogen_agentchat.conditions.TimeoutTermination.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.conditions.TimeoutTermination.reset "Link to this definition")
    :   Reset the termination condition.

    *property* terminated*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*[#](#autogen_agentchat.conditions.TimeoutTermination.terminated "Link to this definition")
    :   Check if the termination condition has been reached

*class* TokenUsageTermination(*max\_total\_token: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *max\_prompt\_token: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *max\_completion\_token: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_agentchat.conditions.TokenUsageTermination "Link to this definition")
:   Bases: [`TerminationCondition`](#autogen_agentchat.base.TerminationCondition "autogen_agentchat.base._termination.TerminationCondition"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`TokenUsageTerminationConfig`]

    Terminate the conversation if a token usage limit is reached.

    Parameters:
    :   * **max\_total\_token** – The maximum total number of tokens allowed in the conversation.
        * **max\_prompt\_token** – The maximum number of prompt tokens allowed in the conversation.
        * **max\_completion\_token** – The maximum number of completion tokens allowed in the conversation.

    Raises:
    :   [**ValueError**](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.13)") – If none of max\_total\_token, max\_prompt\_token, or max\_completion\_token is provided.

    *classmethod* \_from\_config(*config: TokenUsageTerminationConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_agentchat.conditions.TokenUsageTermination._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → TokenUsageTerminationConfig[#](#autogen_agentchat.conditions.TokenUsageTermination._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_agentchat.conditions.TokenUsageTermination.component_config_schema "Link to this definition")
    :   alias of `TokenUsageTerminationConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_agentchat.conditions.TokenUsageTermination'*[#](#autogen_agentchat.conditions.TokenUsageTermination.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.conditions.TokenUsageTermination.reset "Link to this definition")
    :   Reset the termination condition.

    *property* terminated*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*[#](#autogen_agentchat.conditions.TokenUsageTermination.terminated "Link to this definition")
    :   Check if the termination condition has been reached

### autogen\_agentchat.ui[#](#module-autogen_agentchat.ui "Link to this heading")

This module implements utility classes for formatting/printing agent messages.

*async* Console(*stream: [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | T, [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]*, *\**, *no\_inline\_images: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*, *output\_stats: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*, *user\_input\_manager: [UserInputManager](#autogen_agentchat.ui.UserInputManager "autogen_agentchat.ui._console.UserInputManager") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → T[#](#autogen_agentchat.ui.Console "Link to this definition")
:   Consumes the message stream from [`run_stream()`](#autogen_agentchat.base.TaskRunner.run_stream "autogen_agentchat.base.TaskRunner.run_stream")
    or [`on_messages_stream()`](#autogen_agentchat.base.ChatAgent.on_messages_stream "autogen_agentchat.base.ChatAgent.on_messages_stream") and renders the messages to the console.
    Returns the last processed TaskResult or Response.

    Note

    output\_stats is experimental and the stats may not be accurate.
    It will be improved in future releases.

    Parameters:
    :   * **stream** (*AsyncGenerator**[**AgentEvent* *|* *ChatMessage* *|* [*TaskResult*](#autogen_agentchat.base.TaskResult "autogen_agentchat.base.TaskResult")*,* *None**]* *|* *AsyncGenerator**[**AgentEvent* *|* *ChatMessage* *|* [*Response*](#autogen_agentchat.base.Response "autogen_agentchat.base.Response")*,* *None**]*) – Message stream to render.
          This can be from [`run_stream()`](#autogen_agentchat.base.TaskRunner.run_stream "autogen_agentchat.base.TaskRunner.run_stream") or [`on_messages_stream()`](#autogen_agentchat.base.ChatAgent.on_messages_stream "autogen_agentchat.base.ChatAgent.on_messages_stream").
        * **no\_inline\_images** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – If terminal is iTerm2 will render images inline. Use this to disable this behavior. Defaults to False.
        * **output\_stats** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – (Experimental) If True, will output a summary of the messages and inline token usage info. Defaults to False.

    Returns:
    :   **last\_processed** – A [`TaskResult`](#autogen_agentchat.base.TaskResult "autogen_agentchat.base.TaskResult") if the stream is from [`run_stream()`](#autogen_agentchat.base.TaskRunner.run_stream "autogen_agentchat.base.TaskRunner.run_stream")
        or a [`Response`](#autogen_agentchat.base.Response "autogen_agentchat.base.Response") if the stream is from [`on_messages_stream()`](#autogen_agentchat.base.ChatAgent.on_messages_stream "autogen_agentchat.base.ChatAgent.on_messages_stream").

*class* UserInputManager(*callback: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")], [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")], [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]]*)[#](#autogen_agentchat.ui.UserInputManager "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    get\_wrapped\_callback() → [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")], [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]][#](#autogen_agentchat.ui.UserInputManager.get_wrapped_callback "Link to this definition")

    notify\_event\_received(*request\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_agentchat.ui.UserInputManager.notify_event_received "Link to this definition")

### autogen\_agentchat.state[#](#module-autogen_agentchat.state "Link to this heading")

State management for agents, teams and termination conditions.

*pydantic model* AssistantAgentState[#](#autogen_agentchat.state.AssistantAgentState "Link to this definition")
:   Bases: [`BaseState`](#autogen_agentchat.state.BaseState "autogen_agentchat.state._states.BaseState")

    State for an assistant agent.

    Show JSON schema

    ```
    {
       "title": "AssistantAgentState",
       "description": "State for an assistant agent.",
       "type": "object",
       "properties": {
          "type": {
             "default": "AssistantAgentState",
             "title": "Type",
             "type": "string"
          },
          "version": {
             "default": "1.0.0",
             "title": "Version",
             "type": "string"
          },
          "llm_context": {
             "title": "Llm Context",
             "type": "object"
          }
       }
    }

    ```

    Fields:
    :   * `llm_context (Mapping[str, Any])`
        * `type (str)`

    *field* llm\_context*: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]* *[Optional]*[#](#autogen_agentchat.state.AssistantAgentState.llm_context "Link to this definition")

    *field* type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'AssistantAgentState'*[#](#autogen_agentchat.state.AssistantAgentState.type "Link to this definition")

*pydantic model* BaseGroupChatManagerState[#](#autogen_agentchat.state.BaseGroupChatManagerState "Link to this definition")
:   Bases: [`BaseState`](#autogen_agentchat.state.BaseState "autogen_agentchat.state._states.BaseState")

    Base state for all group chat managers.

    Show JSON schema

    ```
    {
       "title": "BaseGroupChatManagerState",
       "description": "Base state for all group chat managers.",
       "type": "object",
       "properties": {
          "type": {
             "default": "BaseGroupChatManagerState",
             "title": "Type",
             "type": "string"
          },
          "version": {
             "default": "1.0.0",
             "title": "Version",
             "type": "string"
          },
          "message_thread": {
             "items": {
                "discriminator": {
                   "mapping": {
                      "HandoffMessage": "#/$defs/HandoffMessage",
                      "MemoryQueryEvent": "#/$defs/MemoryQueryEvent",
                      "ModelClientStreamingChunkEvent": "#/$defs/ModelClientStreamingChunkEvent",
                      "MultiModalMessage": "#/$defs/MultiModalMessage",
                      "StopMessage": "#/$defs/StopMessage",
                      "TextMessage": "#/$defs/TextMessage",
                      "ThoughtEvent": "#/$defs/ThoughtEvent",
                      "ToolCallExecutionEvent": "#/$defs/ToolCallExecutionEvent",
                      "ToolCallRequestEvent": "#/$defs/ToolCallRequestEvent",
                      "ToolCallSummaryMessage": "#/$defs/ToolCallSummaryMessage",
                      "UserInputRequestedEvent": "#/$defs/UserInputRequestedEvent"
                   },
                   "propertyName": "type"
                },
                "oneOf": [
                   {
                      "$ref": "#/$defs/ToolCallRequestEvent"
                   },
                   {
                      "$ref": "#/$defs/ToolCallExecutionEvent"
                   },
                   {
                      "$ref": "#/$defs/MemoryQueryEvent"
                   },
                   {
                      "$ref": "#/$defs/UserInputRequestedEvent"
                   },
                   {
                      "$ref": "#/$defs/ModelClientStreamingChunkEvent"
                   },
                   {
                      "$ref": "#/$defs/ThoughtEvent"
                   },
                   {
                      "$ref": "#/$defs/TextMessage"
                   },
                   {
                      "$ref": "#/$defs/MultiModalMessage"
                   },
                   {
                      "$ref": "#/$defs/StopMessage"
                   },
                   {
                      "$ref": "#/$defs/ToolCallSummaryMessage"
                   },
                   {
                      "$ref": "#/$defs/HandoffMessage"
                   }
                ]
             },
             "title": "Message Thread",
             "type": "array"
          },
          "current_turn": {
             "default": 0,
             "title": "Current Turn",
             "type": "integer"
          }
       },
       "$defs": {
          "AssistantMessage": {
             "description": "Assistant message are sampled from the language model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "$ref": "#/$defs/FunctionCall"
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "thought": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Thought"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "AssistantMessage",
                   "default": "AssistantMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "AssistantMessage",
             "type": "object"
          },
          "FunctionCall": {
             "properties": {
                "id": {
                   "title": "Id",
                   "type": "string"
                },
                "arguments": {
                   "title": "Arguments",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                }
             },
             "required": [
                "id",
                "arguments",
                "name"
             ],
             "title": "FunctionCall",
             "type": "object"
          },
          "FunctionExecutionResult": {
             "description": "Function execution result contains the output of a function call.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                },
                "call_id": {
                   "title": "Call Id",
                   "type": "string"
                },
                "is_error": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Is Error"
                }
             },
             "required": [
                "content",
                "name",
                "call_id"
             ],
             "title": "FunctionExecutionResult",
             "type": "object"
          },
          "FunctionExecutionResultMessage": {
             "description": "Function execution result message contains the output of multiple function calls.",
             "properties": {
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionExecutionResult"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "FunctionExecutionResultMessage",
                   "default": "FunctionExecutionResultMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "FunctionExecutionResultMessage",
             "type": "object"
          },
          "HandoffMessage": {
             "description": "A message requesting handoff of a conversation to another agent.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "target": {
                   "title": "Target",
                   "type": "string"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "context": {
                   "default": [],
                   "items": {
                      "discriminator": {
                         "mapping": {
                            "AssistantMessage": "#/$defs/AssistantMessage",
                            "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                            "SystemMessage": "#/$defs/SystemMessage",
                            "UserMessage": "#/$defs/UserMessage"
                         },
                         "propertyName": "type"
                      },
                      "oneOf": [
                         {
                            "$ref": "#/$defs/SystemMessage"
                         },
                         {
                            "$ref": "#/$defs/UserMessage"
                         },
                         {
                            "$ref": "#/$defs/AssistantMessage"
                         },
                         {
                            "$ref": "#/$defs/FunctionExecutionResultMessage"
                         }
                      ]
                   },
                   "title": "Context",
                   "type": "array"
                },
                "type": {
                   "const": "HandoffMessage",
                   "default": "HandoffMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "target",
                "content"
             ],
             "title": "HandoffMessage",
             "type": "object"
          },
          "MemoryContent": {
             "description": "A memory content item.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "format": "binary",
                         "type": "string"
                      },
                      {
                         "type": "object"
                      },
                      {}
                   ],
                   "title": "Content"
                },
                "mime_type": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/MemoryMimeType"
                      },
                      {
                         "type": "string"
                      }
                   ],
                   "title": "Mime Type"
                },
                "metadata": {
                   "anyOf": [
                      {
                         "type": "object"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Metadata"
                }
             },
             "required": [
                "content",
                "mime_type"
             ],
             "title": "MemoryContent",
             "type": "object"
          },
          "MemoryMimeType": {
             "description": "Supported MIME types for memory content.",
             "enum": [
                "text/plain",
                "application/json",
                "text/markdown",
                "image/*",
                "application/octet-stream"
             ],
             "title": "MemoryMimeType",
             "type": "string"
          },
          "MemoryQueryEvent": {
             "description": "An event signaling the results of memory queries.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/MemoryContent"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "MemoryQueryEvent",
                   "default": "MemoryQueryEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "MemoryQueryEvent",
             "type": "object"
          },
          "ModelClientStreamingChunkEvent": {
             "description": "An event signaling a text output chunk from a model client in streaming mode.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ModelClientStreamingChunkEvent",
                   "default": "ModelClientStreamingChunkEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ModelClientStreamingChunkEvent",
             "type": "object"
          },
          "MultiModalMessage": {
             "description": "A multimodal message.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "anyOf": [
                         {
                            "type": "string"
                         },
                         {}
                      ]
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "MultiModalMessage",
                   "default": "MultiModalMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "MultiModalMessage",
             "type": "object"
          },
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          },
          "StopMessage": {
             "description": "A message requesting stop of a conversation.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "StopMessage",
                   "default": "StopMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "StopMessage",
             "type": "object"
          },
          "SystemMessage": {
             "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "SystemMessage",
                   "default": "SystemMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "SystemMessage",
             "type": "object"
          },
          "TextMessage": {
             "description": "A text message.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "TextMessage",
                   "default": "TextMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "TextMessage",
             "type": "object"
          },
          "ThoughtEvent": {
             "description": "An event signaling the thought process of an agent.\nIt is used to communicate the reasoning tokens generated by a reasoning model,\nor the extra text content generated by a function call.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ThoughtEvent",
                   "default": "ThoughtEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ThoughtEvent",
             "type": "object"
          },
          "ToolCallExecutionEvent": {
             "description": "An event signaling the execution of tool calls.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionExecutionResult"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "ToolCallExecutionEvent",
                   "default": "ToolCallExecutionEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallExecutionEvent",
             "type": "object"
          },
          "ToolCallRequestEvent": {
             "description": "An event signaling a request to use tools.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionCall"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "ToolCallRequestEvent",
                   "default": "ToolCallRequestEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallRequestEvent",
             "type": "object"
          },
          "ToolCallSummaryMessage": {
             "description": "A message signaling the summary of tool call results.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ToolCallSummaryMessage",
                   "default": "ToolCallSummaryMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallSummaryMessage",
             "type": "object"
          },
          "UserInputRequestedEvent": {
             "description": "An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "request_id": {
                   "title": "Request Id",
                   "type": "string"
                },
                "content": {
                   "const": "",
                   "default": "",
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "UserInputRequestedEvent",
                   "default": "UserInputRequestedEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "request_id"
             ],
             "title": "UserInputRequestedEvent",
             "type": "object"
          },
          "UserMessage": {
             "description": "User message contains input from end users, or a catch-all for data provided to the model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "anyOf": [
                               {
                                  "type": "string"
                               },
                               {}
                            ]
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "UserMessage",
                   "default": "UserMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "UserMessage",
             "type": "object"
          }
       }
    }

    ```

    Fields:
    :   * `current_turn (int)`
        * `message_thread (List[autogen_agentchat.messages.ToolCallRequestEvent | autogen_agentchat.messages.ToolCallExecutionEvent | autogen_agentchat.messages.MemoryQueryEvent | autogen_agentchat.messages.UserInputRequestedEvent | autogen_agentchat.messages.ModelClientStreamingChunkEvent | autogen_agentchat.messages.ThoughtEvent | autogen_agentchat.messages.TextMessage | autogen_agentchat.messages.MultiModalMessage | autogen_agentchat.messages.StopMessage | autogen_agentchat.messages.ToolCallSummaryMessage | autogen_agentchat.messages.HandoffMessage])`
        * `type (str)`

    *field* current\_turn*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")* *= 0*[#](#autogen_agentchat.state.BaseGroupChatManagerState.current_turn "Link to this definition")

    *field* message\_thread*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')], FieldInfo(annotation=NoneType, required=True, discriminator='type')]]* *[Optional]*[#](#autogen_agentchat.state.BaseGroupChatManagerState.message_thread "Link to this definition")

    *field* type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'BaseGroupChatManagerState'*[#](#autogen_agentchat.state.BaseGroupChatManagerState.type "Link to this definition")

*pydantic model* BaseState[#](#autogen_agentchat.state.BaseState "Link to this definition")
:   Bases: `BaseModel`

    Base class for all saveable state

    Show JSON schema

    ```
    {
       "title": "BaseState",
       "description": "Base class for all saveable state",
       "type": "object",
       "properties": {
          "type": {
             "default": "BaseState",
             "title": "Type",
             "type": "string"
          },
          "version": {
             "default": "1.0.0",
             "title": "Version",
             "type": "string"
          }
       }
    }

    ```

    Fields:
    :   * `type (str)`
        * `version (str)`

    *field* type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'BaseState'*[#](#autogen_agentchat.state.BaseState.type "Link to this definition")

    *field* version*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= '1.0.0'*[#](#autogen_agentchat.state.BaseState.version "Link to this definition")

*pydantic model* ChatAgentContainerState[#](#autogen_agentchat.state.ChatAgentContainerState "Link to this definition")
:   Bases: [`BaseState`](#autogen_agentchat.state.BaseState "autogen_agentchat.state._states.BaseState")

    State for a container of chat agents.

    Show JSON schema

    ```
    {
       "title": "ChatAgentContainerState",
       "description": "State for a container of chat agents.",
       "type": "object",
       "properties": {
          "type": {
             "default": "ChatAgentContainerState",
             "title": "Type",
             "type": "string"
          },
          "version": {
             "default": "1.0.0",
             "title": "Version",
             "type": "string"
          },
          "agent_state": {
             "title": "Agent State",
             "type": "object"
          },
          "message_buffer": {
             "items": {
                "discriminator": {
                   "mapping": {
                      "HandoffMessage": "#/$defs/HandoffMessage",
                      "MultiModalMessage": "#/$defs/MultiModalMessage",
                      "StopMessage": "#/$defs/StopMessage",
                      "TextMessage": "#/$defs/TextMessage",
                      "ToolCallSummaryMessage": "#/$defs/ToolCallSummaryMessage"
                   },
                   "propertyName": "type"
                },
                "oneOf": [
                   {
                      "$ref": "#/$defs/TextMessage"
                   },
                   {
                      "$ref": "#/$defs/MultiModalMessage"
                   },
                   {
                      "$ref": "#/$defs/StopMessage"
                   },
                   {
                      "$ref": "#/$defs/ToolCallSummaryMessage"
                   },
                   {
                      "$ref": "#/$defs/HandoffMessage"
                   }
                ]
             },
             "title": "Message Buffer",
             "type": "array"
          }
       },
       "$defs": {
          "AssistantMessage": {
             "description": "Assistant message are sampled from the language model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "$ref": "#/$defs/FunctionCall"
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "thought": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Thought"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "AssistantMessage",
                   "default": "AssistantMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "AssistantMessage",
             "type": "object"
          },
          "FunctionCall": {
             "properties": {
                "id": {
                   "title": "Id",
                   "type": "string"
                },
                "arguments": {
                   "title": "Arguments",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                }
             },
             "required": [
                "id",
                "arguments",
                "name"
             ],
             "title": "FunctionCall",
             "type": "object"
          },
          "FunctionExecutionResult": {
             "description": "Function execution result contains the output of a function call.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                },
                "call_id": {
                   "title": "Call Id",
                   "type": "string"
                },
                "is_error": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Is Error"
                }
             },
             "required": [
                "content",
                "name",
                "call_id"
             ],
             "title": "FunctionExecutionResult",
             "type": "object"
          },
          "FunctionExecutionResultMessage": {
             "description": "Function execution result message contains the output of multiple function calls.",
             "properties": {
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionExecutionResult"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "FunctionExecutionResultMessage",
                   "default": "FunctionExecutionResultMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "FunctionExecutionResultMessage",
             "type": "object"
          },
          "HandoffMessage": {
             "description": "A message requesting handoff of a conversation to another agent.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "target": {
                   "title": "Target",
                   "type": "string"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "context": {
                   "default": [],
                   "items": {
                      "discriminator": {
                         "mapping": {
                            "AssistantMessage": "#/$defs/AssistantMessage",
                            "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                            "SystemMessage": "#/$defs/SystemMessage",
                            "UserMessage": "#/$defs/UserMessage"
                         },
                         "propertyName": "type"
                      },
                      "oneOf": [
                         {
                            "$ref": "#/$defs/SystemMessage"
                         },
                         {
                            "$ref": "#/$defs/UserMessage"
                         },
                         {
                            "$ref": "#/$defs/AssistantMessage"
                         },
                         {
                            "$ref": "#/$defs/FunctionExecutionResultMessage"
                         }
                      ]
                   },
                   "title": "Context",
                   "type": "array"
                },
                "type": {
                   "const": "HandoffMessage",
                   "default": "HandoffMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "target",
                "content"
             ],
             "title": "HandoffMessage",
             "type": "object"
          },
          "MultiModalMessage": {
             "description": "A multimodal message.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "anyOf": [
                         {
                            "type": "string"
                         },
                         {}
                      ]
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "MultiModalMessage",
                   "default": "MultiModalMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "MultiModalMessage",
             "type": "object"
          },
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          },
          "StopMessage": {
             "description": "A message requesting stop of a conversation.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "StopMessage",
                   "default": "StopMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "StopMessage",
             "type": "object"
          },
          "SystemMessage": {
             "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "SystemMessage",
                   "default": "SystemMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "SystemMessage",
             "type": "object"
          },
          "TextMessage": {
             "description": "A text message.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "TextMessage",
                   "default": "TextMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "TextMessage",
             "type": "object"
          },
          "ToolCallSummaryMessage": {
             "description": "A message signaling the summary of tool call results.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ToolCallSummaryMessage",
                   "default": "ToolCallSummaryMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallSummaryMessage",
             "type": "object"
          },
          "UserMessage": {
             "description": "User message contains input from end users, or a catch-all for data provided to the model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "anyOf": [
                               {
                                  "type": "string"
                               },
                               {}
                            ]
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "UserMessage",
                   "default": "UserMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "UserMessage",
             "type": "object"
          }
       }
    }

    ```

    Fields:
    :   * `agent_state (Mapping[str, Any])`
        * `message_buffer (List[Annotated[autogen_agentchat.messages.TextMessage | autogen_agentchat.messages.MultiModalMessage | autogen_agentchat.messages.StopMessage | autogen_agentchat.messages.ToolCallSummaryMessage | autogen_agentchat.messages.HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]])`
        * `type (str)`

    *field* agent\_state*: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]* *[Optional]*[#](#autogen_agentchat.state.ChatAgentContainerState.agent_state "Link to this definition")

    *field* message\_buffer*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]* *[Optional]*[#](#autogen_agentchat.state.ChatAgentContainerState.message_buffer "Link to this definition")

    *field* type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'ChatAgentContainerState'*[#](#autogen_agentchat.state.ChatAgentContainerState.type "Link to this definition")

*pydantic model* MagenticOneOrchestratorState[#](#autogen_agentchat.state.MagenticOneOrchestratorState "Link to this definition")
:   Bases: [`BaseGroupChatManagerState`](#autogen_agentchat.state.BaseGroupChatManagerState "autogen_agentchat.state._states.BaseGroupChatManagerState")

    State for `MagneticOneGroupChat` orchestrator.

    Show JSON schema

    ```
    {
       "title": "MagenticOneOrchestratorState",
       "description": "State for :class:`~autogen_agentchat.teams.MagneticOneGroupChat` orchestrator.",
       "type": "object",
       "properties": {
          "type": {
             "default": "MagenticOneOrchestratorState",
             "title": "Type",
             "type": "string"
          },
          "version": {
             "default": "1.0.0",
             "title": "Version",
             "type": "string"
          },
          "message_thread": {
             "items": {
                "discriminator": {
                   "mapping": {
                      "HandoffMessage": "#/$defs/HandoffMessage",
                      "MemoryQueryEvent": "#/$defs/MemoryQueryEvent",
                      "ModelClientStreamingChunkEvent": "#/$defs/ModelClientStreamingChunkEvent",
                      "MultiModalMessage": "#/$defs/MultiModalMessage",
                      "StopMessage": "#/$defs/StopMessage",
                      "TextMessage": "#/$defs/TextMessage",
                      "ThoughtEvent": "#/$defs/ThoughtEvent",
                      "ToolCallExecutionEvent": "#/$defs/ToolCallExecutionEvent",
                      "ToolCallRequestEvent": "#/$defs/ToolCallRequestEvent",
                      "ToolCallSummaryMessage": "#/$defs/ToolCallSummaryMessage",
                      "UserInputRequestedEvent": "#/$defs/UserInputRequestedEvent"
                   },
                   "propertyName": "type"
                },
                "oneOf": [
                   {
                      "$ref": "#/$defs/ToolCallRequestEvent"
                   },
                   {
                      "$ref": "#/$defs/ToolCallExecutionEvent"
                   },
                   {
                      "$ref": "#/$defs/MemoryQueryEvent"
                   },
                   {
                      "$ref": "#/$defs/UserInputRequestedEvent"
                   },
                   {
                      "$ref": "#/$defs/ModelClientStreamingChunkEvent"
                   },
                   {
                      "$ref": "#/$defs/ThoughtEvent"
                   },
                   {
                      "$ref": "#/$defs/TextMessage"
                   },
                   {
                      "$ref": "#/$defs/MultiModalMessage"
                   },
                   {
                      "$ref": "#/$defs/StopMessage"
                   },
                   {
                      "$ref": "#/$defs/ToolCallSummaryMessage"
                   },
                   {
                      "$ref": "#/$defs/HandoffMessage"
                   }
                ]
             },
             "title": "Message Thread",
             "type": "array"
          },
          "current_turn": {
             "default": 0,
             "title": "Current Turn",
             "type": "integer"
          },
          "task": {
             "default": "",
             "title": "Task",
             "type": "string"
          },
          "facts": {
             "default": "",
             "title": "Facts",
             "type": "string"
          },
          "plan": {
             "default": "",
             "title": "Plan",
             "type": "string"
          },
          "n_rounds": {
             "default": 0,
             "title": "N Rounds",
             "type": "integer"
          },
          "n_stalls": {
             "default": 0,
             "title": "N Stalls",
             "type": "integer"
          }
       },
       "$defs": {
          "AssistantMessage": {
             "description": "Assistant message are sampled from the language model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "$ref": "#/$defs/FunctionCall"
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "thought": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Thought"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "AssistantMessage",
                   "default": "AssistantMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "AssistantMessage",
             "type": "object"
          },
          "FunctionCall": {
             "properties": {
                "id": {
                   "title": "Id",
                   "type": "string"
                },
                "arguments": {
                   "title": "Arguments",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                }
             },
             "required": [
                "id",
                "arguments",
                "name"
             ],
             "title": "FunctionCall",
             "type": "object"
          },
          "FunctionExecutionResult": {
             "description": "Function execution result contains the output of a function call.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                },
                "call_id": {
                   "title": "Call Id",
                   "type": "string"
                },
                "is_error": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Is Error"
                }
             },
             "required": [
                "content",
                "name",
                "call_id"
             ],
             "title": "FunctionExecutionResult",
             "type": "object"
          },
          "FunctionExecutionResultMessage": {
             "description": "Function execution result message contains the output of multiple function calls.",
             "properties": {
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionExecutionResult"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "FunctionExecutionResultMessage",
                   "default": "FunctionExecutionResultMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "FunctionExecutionResultMessage",
             "type": "object"
          },
          "HandoffMessage": {
             "description": "A message requesting handoff of a conversation to another agent.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "target": {
                   "title": "Target",
                   "type": "string"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "context": {
                   "default": [],
                   "items": {
                      "discriminator": {
                         "mapping": {
                            "AssistantMessage": "#/$defs/AssistantMessage",
                            "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                            "SystemMessage": "#/$defs/SystemMessage",
                            "UserMessage": "#/$defs/UserMessage"
                         },
                         "propertyName": "type"
                      },
                      "oneOf": [
                         {
                            "$ref": "#/$defs/SystemMessage"
                         },
                         {
                            "$ref": "#/$defs/UserMessage"
                         },
                         {
                            "$ref": "#/$defs/AssistantMessage"
                         },
                         {
                            "$ref": "#/$defs/FunctionExecutionResultMessage"
                         }
                      ]
                   },
                   "title": "Context",
                   "type": "array"
                },
                "type": {
                   "const": "HandoffMessage",
                   "default": "HandoffMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "target",
                "content"
             ],
             "title": "HandoffMessage",
             "type": "object"
          },
          "MemoryContent": {
             "description": "A memory content item.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "format": "binary",
                         "type": "string"
                      },
                      {
                         "type": "object"
                      },
                      {}
                   ],
                   "title": "Content"
                },
                "mime_type": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/MemoryMimeType"
                      },
                      {
                         "type": "string"
                      }
                   ],
                   "title": "Mime Type"
                },
                "metadata": {
                   "anyOf": [
                      {
                         "type": "object"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Metadata"
                }
             },
             "required": [
                "content",
                "mime_type"
             ],
             "title": "MemoryContent",
             "type": "object"
          },
          "MemoryMimeType": {
             "description": "Supported MIME types for memory content.",
             "enum": [
                "text/plain",
                "application/json",
                "text/markdown",
                "image/*",
                "application/octet-stream"
             ],
             "title": "MemoryMimeType",
             "type": "string"
          },
          "MemoryQueryEvent": {
             "description": "An event signaling the results of memory queries.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/MemoryContent"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "MemoryQueryEvent",
                   "default": "MemoryQueryEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "MemoryQueryEvent",
             "type": "object"
          },
          "ModelClientStreamingChunkEvent": {
             "description": "An event signaling a text output chunk from a model client in streaming mode.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ModelClientStreamingChunkEvent",
                   "default": "ModelClientStreamingChunkEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ModelClientStreamingChunkEvent",
             "type": "object"
          },
          "MultiModalMessage": {
             "description": "A multimodal message.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "anyOf": [
                         {
                            "type": "string"
                         },
                         {}
                      ]
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "MultiModalMessage",
                   "default": "MultiModalMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "MultiModalMessage",
             "type": "object"
          },
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          },
          "StopMessage": {
             "description": "A message requesting stop of a conversation.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "StopMessage",
                   "default": "StopMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "StopMessage",
             "type": "object"
          },
          "SystemMessage": {
             "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "SystemMessage",
                   "default": "SystemMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "SystemMessage",
             "type": "object"
          },
          "TextMessage": {
             "description": "A text message.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "TextMessage",
                   "default": "TextMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "TextMessage",
             "type": "object"
          },
          "ThoughtEvent": {
             "description": "An event signaling the thought process of an agent.\nIt is used to communicate the reasoning tokens generated by a reasoning model,\nor the extra text content generated by a function call.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ThoughtEvent",
                   "default": "ThoughtEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ThoughtEvent",
             "type": "object"
          },
          "ToolCallExecutionEvent": {
             "description": "An event signaling the execution of tool calls.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionExecutionResult"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "ToolCallExecutionEvent",
                   "default": "ToolCallExecutionEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallExecutionEvent",
             "type": "object"
          },
          "ToolCallRequestEvent": {
             "description": "An event signaling a request to use tools.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionCall"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "ToolCallRequestEvent",
                   "default": "ToolCallRequestEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallRequestEvent",
             "type": "object"
          },
          "ToolCallSummaryMessage": {
             "description": "A message signaling the summary of tool call results.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ToolCallSummaryMessage",
                   "default": "ToolCallSummaryMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallSummaryMessage",
             "type": "object"
          },
          "UserInputRequestedEvent": {
             "description": "An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "request_id": {
                   "title": "Request Id",
                   "type": "string"
                },
                "content": {
                   "const": "",
                   "default": "",
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "UserInputRequestedEvent",
                   "default": "UserInputRequestedEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "request_id"
             ],
             "title": "UserInputRequestedEvent",
             "type": "object"
          },
          "UserMessage": {
             "description": "User message contains input from end users, or a catch-all for data provided to the model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "anyOf": [
                               {
                                  "type": "string"
                               },
                               {}
                            ]
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "UserMessage",
                   "default": "UserMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "UserMessage",
             "type": "object"
          }
       }
    }

    ```

    Fields:
    :   * `facts (str)`
        * `n_rounds (int)`
        * `n_stalls (int)`
        * `plan (str)`
        * `task (str)`
        * `type (str)`

    *field* facts*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= ''*[#](#autogen_agentchat.state.MagenticOneOrchestratorState.facts "Link to this definition")

    *field* n\_rounds*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")* *= 0*[#](#autogen_agentchat.state.MagenticOneOrchestratorState.n_rounds "Link to this definition")

    *field* n\_stalls*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")* *= 0*[#](#autogen_agentchat.state.MagenticOneOrchestratorState.n_stalls "Link to this definition")

    *field* plan*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= ''*[#](#autogen_agentchat.state.MagenticOneOrchestratorState.plan "Link to this definition")

    *field* task*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= ''*[#](#autogen_agentchat.state.MagenticOneOrchestratorState.task "Link to this definition")

    *field* type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'MagenticOneOrchestratorState'*[#](#autogen_agentchat.state.MagenticOneOrchestratorState.type "Link to this definition")

*pydantic model* RoundRobinManagerState[#](#autogen_agentchat.state.RoundRobinManagerState "Link to this definition")
:   Bases: [`BaseGroupChatManagerState`](#autogen_agentchat.state.BaseGroupChatManagerState "autogen_agentchat.state._states.BaseGroupChatManagerState")

    State for [`RoundRobinGroupChat`](#autogen_agentchat.teams.RoundRobinGroupChat "autogen_agentchat.teams.RoundRobinGroupChat") manager.

    Show JSON schema

    ```
    {
       "title": "RoundRobinManagerState",
       "description": "State for :class:`~autogen_agentchat.teams.RoundRobinGroupChat` manager.",
       "type": "object",
       "properties": {
          "type": {
             "default": "RoundRobinManagerState",
             "title": "Type",
             "type": "string"
          },
          "version": {
             "default": "1.0.0",
             "title": "Version",
             "type": "string"
          },
          "message_thread": {
             "items": {
                "discriminator": {
                   "mapping": {
                      "HandoffMessage": "#/$defs/HandoffMessage",
                      "MemoryQueryEvent": "#/$defs/MemoryQueryEvent",
                      "ModelClientStreamingChunkEvent": "#/$defs/ModelClientStreamingChunkEvent",
                      "MultiModalMessage": "#/$defs/MultiModalMessage",
                      "StopMessage": "#/$defs/StopMessage",
                      "TextMessage": "#/$defs/TextMessage",
                      "ThoughtEvent": "#/$defs/ThoughtEvent",
                      "ToolCallExecutionEvent": "#/$defs/ToolCallExecutionEvent",
                      "ToolCallRequestEvent": "#/$defs/ToolCallRequestEvent",
                      "ToolCallSummaryMessage": "#/$defs/ToolCallSummaryMessage",
                      "UserInputRequestedEvent": "#/$defs/UserInputRequestedEvent"
                   },
                   "propertyName": "type"
                },
                "oneOf": [
                   {
                      "$ref": "#/$defs/ToolCallRequestEvent"
                   },
                   {
                      "$ref": "#/$defs/ToolCallExecutionEvent"
                   },
                   {
                      "$ref": "#/$defs/MemoryQueryEvent"
                   },
                   {
                      "$ref": "#/$defs/UserInputRequestedEvent"
                   },
                   {
                      "$ref": "#/$defs/ModelClientStreamingChunkEvent"
                   },
                   {
                      "$ref": "#/$defs/ThoughtEvent"
                   },
                   {
                      "$ref": "#/$defs/TextMessage"
                   },
                   {
                      "$ref": "#/$defs/MultiModalMessage"
                   },
                   {
                      "$ref": "#/$defs/StopMessage"
                   },
                   {
                      "$ref": "#/$defs/ToolCallSummaryMessage"
                   },
                   {
                      "$ref": "#/$defs/HandoffMessage"
                   }
                ]
             },
             "title": "Message Thread",
             "type": "array"
          },
          "current_turn": {
             "default": 0,
             "title": "Current Turn",
             "type": "integer"
          },
          "next_speaker_index": {
             "default": 0,
             "title": "Next Speaker Index",
             "type": "integer"
          }
       },
       "$defs": {
          "AssistantMessage": {
             "description": "Assistant message are sampled from the language model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "$ref": "#/$defs/FunctionCall"
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "thought": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Thought"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "AssistantMessage",
                   "default": "AssistantMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "AssistantMessage",
             "type": "object"
          },
          "FunctionCall": {
             "properties": {
                "id": {
                   "title": "Id",
                   "type": "string"
                },
                "arguments": {
                   "title": "Arguments",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                }
             },
             "required": [
                "id",
                "arguments",
                "name"
             ],
             "title": "FunctionCall",
             "type": "object"
          },
          "FunctionExecutionResult": {
             "description": "Function execution result contains the output of a function call.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                },
                "call_id": {
                   "title": "Call Id",
                   "type": "string"
                },
                "is_error": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Is Error"
                }
             },
             "required": [
                "content",
                "name",
                "call_id"
             ],
             "title": "FunctionExecutionResult",
             "type": "object"
          },
          "FunctionExecutionResultMessage": {
             "description": "Function execution result message contains the output of multiple function calls.",
             "properties": {
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionExecutionResult"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "FunctionExecutionResultMessage",
                   "default": "FunctionExecutionResultMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "FunctionExecutionResultMessage",
             "type": "object"
          },
          "HandoffMessage": {
             "description": "A message requesting handoff of a conversation to another agent.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "target": {
                   "title": "Target",
                   "type": "string"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "context": {
                   "default": [],
                   "items": {
                      "discriminator": {
                         "mapping": {
                            "AssistantMessage": "#/$defs/AssistantMessage",
                            "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                            "SystemMessage": "#/$defs/SystemMessage",
                            "UserMessage": "#/$defs/UserMessage"
                         },
                         "propertyName": "type"
                      },
                      "oneOf": [
                         {
                            "$ref": "#/$defs/SystemMessage"
                         },
                         {
                            "$ref": "#/$defs/UserMessage"
                         },
                         {
                            "$ref": "#/$defs/AssistantMessage"
                         },
                         {
                            "$ref": "#/$defs/FunctionExecutionResultMessage"
                         }
                      ]
                   },
                   "title": "Context",
                   "type": "array"
                },
                "type": {
                   "const": "HandoffMessage",
                   "default": "HandoffMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "target",
                "content"
             ],
             "title": "HandoffMessage",
             "type": "object"
          },
          "MemoryContent": {
             "description": "A memory content item.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "format": "binary",
                         "type": "string"
                      },
                      {
                         "type": "object"
                      },
                      {}
                   ],
                   "title": "Content"
                },
                "mime_type": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/MemoryMimeType"
                      },
                      {
                         "type": "string"
                      }
                   ],
                   "title": "Mime Type"
                },
                "metadata": {
                   "anyOf": [
                      {
                         "type": "object"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Metadata"
                }
             },
             "required": [
                "content",
                "mime_type"
             ],
             "title": "MemoryContent",
             "type": "object"
          },
          "MemoryMimeType": {
             "description": "Supported MIME types for memory content.",
             "enum": [
                "text/plain",
                "application/json",
                "text/markdown",
                "image/*",
                "application/octet-stream"
             ],
             "title": "MemoryMimeType",
             "type": "string"
          },
          "MemoryQueryEvent": {
             "description": "An event signaling the results of memory queries.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/MemoryContent"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "MemoryQueryEvent",
                   "default": "MemoryQueryEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "MemoryQueryEvent",
             "type": "object"
          },
          "ModelClientStreamingChunkEvent": {
             "description": "An event signaling a text output chunk from a model client in streaming mode.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ModelClientStreamingChunkEvent",
                   "default": "ModelClientStreamingChunkEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ModelClientStreamingChunkEvent",
             "type": "object"
          },
          "MultiModalMessage": {
             "description": "A multimodal message.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "anyOf": [
                         {
                            "type": "string"
                         },
                         {}
                      ]
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "MultiModalMessage",
                   "default": "MultiModalMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "MultiModalMessage",
             "type": "object"
          },
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          },
          "StopMessage": {
             "description": "A message requesting stop of a conversation.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "StopMessage",
                   "default": "StopMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "StopMessage",
             "type": "object"
          },
          "SystemMessage": {
             "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "SystemMessage",
                   "default": "SystemMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "SystemMessage",
             "type": "object"
          },
          "TextMessage": {
             "description": "A text message.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "TextMessage",
                   "default": "TextMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "TextMessage",
             "type": "object"
          },
          "ThoughtEvent": {
             "description": "An event signaling the thought process of an agent.\nIt is used to communicate the reasoning tokens generated by a reasoning model,\nor the extra text content generated by a function call.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ThoughtEvent",
                   "default": "ThoughtEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ThoughtEvent",
             "type": "object"
          },
          "ToolCallExecutionEvent": {
             "description": "An event signaling the execution of tool calls.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionExecutionResult"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "ToolCallExecutionEvent",
                   "default": "ToolCallExecutionEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallExecutionEvent",
             "type": "object"
          },
          "ToolCallRequestEvent": {
             "description": "An event signaling a request to use tools.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionCall"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "ToolCallRequestEvent",
                   "default": "ToolCallRequestEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallRequestEvent",
             "type": "object"
          },
          "ToolCallSummaryMessage": {
             "description": "A message signaling the summary of tool call results.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ToolCallSummaryMessage",
                   "default": "ToolCallSummaryMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallSummaryMessage",
             "type": "object"
          },
          "UserInputRequestedEvent": {
             "description": "An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "request_id": {
                   "title": "Request Id",
                   "type": "string"
                },
                "content": {
                   "const": "",
                   "default": "",
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "UserInputRequestedEvent",
                   "default": "UserInputRequestedEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "request_id"
             ],
             "title": "UserInputRequestedEvent",
             "type": "object"
          },
          "UserMessage": {
             "description": "User message contains input from end users, or a catch-all for data provided to the model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "anyOf": [
                               {
                                  "type": "string"
                               },
                               {}
                            ]
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "UserMessage",
                   "default": "UserMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "UserMessage",
             "type": "object"
          }
       }
    }

    ```

    Fields:
    :   * `next_speaker_index (int)`
        * `type (str)`

    *field* next\_speaker\_index*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")* *= 0*[#](#autogen_agentchat.state.RoundRobinManagerState.next_speaker_index "Link to this definition")

    *field* type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'RoundRobinManagerState'*[#](#autogen_agentchat.state.RoundRobinManagerState.type "Link to this definition")

*pydantic model* SelectorManagerState[#](#autogen_agentchat.state.SelectorManagerState "Link to this definition")
:   Bases: [`BaseGroupChatManagerState`](#autogen_agentchat.state.BaseGroupChatManagerState "autogen_agentchat.state._states.BaseGroupChatManagerState")

    State for [`SelectorGroupChat`](#autogen_agentchat.teams.SelectorGroupChat "autogen_agentchat.teams.SelectorGroupChat") manager.

    Show JSON schema

    ```
    {
       "title": "SelectorManagerState",
       "description": "State for :class:`~autogen_agentchat.teams.SelectorGroupChat` manager.",
       "type": "object",
       "properties": {
          "type": {
             "default": "SelectorManagerState",
             "title": "Type",
             "type": "string"
          },
          "version": {
             "default": "1.0.0",
             "title": "Version",
             "type": "string"
          },
          "message_thread": {
             "items": {
                "discriminator": {
                   "mapping": {
                      "HandoffMessage": "#/$defs/HandoffMessage",
                      "MemoryQueryEvent": "#/$defs/MemoryQueryEvent",
                      "ModelClientStreamingChunkEvent": "#/$defs/ModelClientStreamingChunkEvent",
                      "MultiModalMessage": "#/$defs/MultiModalMessage",
                      "StopMessage": "#/$defs/StopMessage",
                      "TextMessage": "#/$defs/TextMessage",
                      "ThoughtEvent": "#/$defs/ThoughtEvent",
                      "ToolCallExecutionEvent": "#/$defs/ToolCallExecutionEvent",
                      "ToolCallRequestEvent": "#/$defs/ToolCallRequestEvent",
                      "ToolCallSummaryMessage": "#/$defs/ToolCallSummaryMessage",
                      "UserInputRequestedEvent": "#/$defs/UserInputRequestedEvent"
                   },
                   "propertyName": "type"
                },
                "oneOf": [
                   {
                      "$ref": "#/$defs/ToolCallRequestEvent"
                   },
                   {
                      "$ref": "#/$defs/ToolCallExecutionEvent"
                   },
                   {
                      "$ref": "#/$defs/MemoryQueryEvent"
                   },
                   {
                      "$ref": "#/$defs/UserInputRequestedEvent"
                   },
                   {
                      "$ref": "#/$defs/ModelClientStreamingChunkEvent"
                   },
                   {
                      "$ref": "#/$defs/ThoughtEvent"
                   },
                   {
                      "$ref": "#/$defs/TextMessage"
                   },
                   {
                      "$ref": "#/$defs/MultiModalMessage"
                   },
                   {
                      "$ref": "#/$defs/StopMessage"
                   },
                   {
                      "$ref": "#/$defs/ToolCallSummaryMessage"
                   },
                   {
                      "$ref": "#/$defs/HandoffMessage"
                   }
                ]
             },
             "title": "Message Thread",
             "type": "array"
          },
          "current_turn": {
             "default": 0,
             "title": "Current Turn",
             "type": "integer"
          },
          "previous_speaker": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Previous Speaker"
          }
       },
       "$defs": {
          "AssistantMessage": {
             "description": "Assistant message are sampled from the language model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "$ref": "#/$defs/FunctionCall"
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "thought": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Thought"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "AssistantMessage",
                   "default": "AssistantMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "AssistantMessage",
             "type": "object"
          },
          "FunctionCall": {
             "properties": {
                "id": {
                   "title": "Id",
                   "type": "string"
                },
                "arguments": {
                   "title": "Arguments",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                }
             },
             "required": [
                "id",
                "arguments",
                "name"
             ],
             "title": "FunctionCall",
             "type": "object"
          },
          "FunctionExecutionResult": {
             "description": "Function execution result contains the output of a function call.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                },
                "call_id": {
                   "title": "Call Id",
                   "type": "string"
                },
                "is_error": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Is Error"
                }
             },
             "required": [
                "content",
                "name",
                "call_id"
             ],
             "title": "FunctionExecutionResult",
             "type": "object"
          },
          "FunctionExecutionResultMessage": {
             "description": "Function execution result message contains the output of multiple function calls.",
             "properties": {
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionExecutionResult"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "FunctionExecutionResultMessage",
                   "default": "FunctionExecutionResultMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "FunctionExecutionResultMessage",
             "type": "object"
          },
          "HandoffMessage": {
             "description": "A message requesting handoff of a conversation to another agent.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "target": {
                   "title": "Target",
                   "type": "string"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "context": {
                   "default": [],
                   "items": {
                      "discriminator": {
                         "mapping": {
                            "AssistantMessage": "#/$defs/AssistantMessage",
                            "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                            "SystemMessage": "#/$defs/SystemMessage",
                            "UserMessage": "#/$defs/UserMessage"
                         },
                         "propertyName": "type"
                      },
                      "oneOf": [
                         {
                            "$ref": "#/$defs/SystemMessage"
                         },
                         {
                            "$ref": "#/$defs/UserMessage"
                         },
                         {
                            "$ref": "#/$defs/AssistantMessage"
                         },
                         {
                            "$ref": "#/$defs/FunctionExecutionResultMessage"
                         }
                      ]
                   },
                   "title": "Context",
                   "type": "array"
                },
                "type": {
                   "const": "HandoffMessage",
                   "default": "HandoffMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "target",
                "content"
             ],
             "title": "HandoffMessage",
             "type": "object"
          },
          "MemoryContent": {
             "description": "A memory content item.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "format": "binary",
                         "type": "string"
                      },
                      {
                         "type": "object"
                      },
                      {}
                   ],
                   "title": "Content"
                },
                "mime_type": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/MemoryMimeType"
                      },
                      {
                         "type": "string"
                      }
                   ],
                   "title": "Mime Type"
                },
                "metadata": {
                   "anyOf": [
                      {
                         "type": "object"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Metadata"
                }
             },
             "required": [
                "content",
                "mime_type"
             ],
             "title": "MemoryContent",
             "type": "object"
          },
          "MemoryMimeType": {
             "description": "Supported MIME types for memory content.",
             "enum": [
                "text/plain",
                "application/json",
                "text/markdown",
                "image/*",
                "application/octet-stream"
             ],
             "title": "MemoryMimeType",
             "type": "string"
          },
          "MemoryQueryEvent": {
             "description": "An event signaling the results of memory queries.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/MemoryContent"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "MemoryQueryEvent",
                   "default": "MemoryQueryEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "MemoryQueryEvent",
             "type": "object"
          },
          "ModelClientStreamingChunkEvent": {
             "description": "An event signaling a text output chunk from a model client in streaming mode.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ModelClientStreamingChunkEvent",
                   "default": "ModelClientStreamingChunkEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ModelClientStreamingChunkEvent",
             "type": "object"
          },
          "MultiModalMessage": {
             "description": "A multimodal message.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "anyOf": [
                         {
                            "type": "string"
                         },
                         {}
                      ]
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "MultiModalMessage",
                   "default": "MultiModalMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "MultiModalMessage",
             "type": "object"
          },
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          },
          "StopMessage": {
             "description": "A message requesting stop of a conversation.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "StopMessage",
                   "default": "StopMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "StopMessage",
             "type": "object"
          },
          "SystemMessage": {
             "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "SystemMessage",
                   "default": "SystemMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "SystemMessage",
             "type": "object"
          },
          "TextMessage": {
             "description": "A text message.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "TextMessage",
                   "default": "TextMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "TextMessage",
             "type": "object"
          },
          "ThoughtEvent": {
             "description": "An event signaling the thought process of an agent.\nIt is used to communicate the reasoning tokens generated by a reasoning model,\nor the extra text content generated by a function call.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ThoughtEvent",
                   "default": "ThoughtEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ThoughtEvent",
             "type": "object"
          },
          "ToolCallExecutionEvent": {
             "description": "An event signaling the execution of tool calls.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionExecutionResult"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "ToolCallExecutionEvent",
                   "default": "ToolCallExecutionEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallExecutionEvent",
             "type": "object"
          },
          "ToolCallRequestEvent": {
             "description": "An event signaling a request to use tools.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionCall"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "ToolCallRequestEvent",
                   "default": "ToolCallRequestEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallRequestEvent",
             "type": "object"
          },
          "ToolCallSummaryMessage": {
             "description": "A message signaling the summary of tool call results.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ToolCallSummaryMessage",
                   "default": "ToolCallSummaryMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallSummaryMessage",
             "type": "object"
          },
          "UserInputRequestedEvent": {
             "description": "An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "request_id": {
                   "title": "Request Id",
                   "type": "string"
                },
                "content": {
                   "const": "",
                   "default": "",
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "UserInputRequestedEvent",
                   "default": "UserInputRequestedEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "request_id"
             ],
             "title": "UserInputRequestedEvent",
             "type": "object"
          },
          "UserMessage": {
             "description": "User message contains input from end users, or a catch-all for data provided to the model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "anyOf": [
                               {
                                  "type": "string"
                               },
                               {}
                            ]
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "UserMessage",
                   "default": "UserMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "UserMessage",
             "type": "object"
          }
       }
    }

    ```

    Fields:
    :   * `previous_speaker (str | None)`
        * `type (str)`

    *field* previous\_speaker*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_agentchat.state.SelectorManagerState.previous_speaker "Link to this definition")

    *field* type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'SelectorManagerState'*[#](#autogen_agentchat.state.SelectorManagerState.type "Link to this definition")

*pydantic model* SocietyOfMindAgentState[#](#autogen_agentchat.state.SocietyOfMindAgentState "Link to this definition")
:   Bases: [`BaseState`](#autogen_agentchat.state.BaseState "autogen_agentchat.state._states.BaseState")

    State for a Society of Mind agent.

    Show JSON schema

    ```
    {
       "title": "SocietyOfMindAgentState",
       "description": "State for a Society of Mind agent.",
       "type": "object",
       "properties": {
          "type": {
             "default": "SocietyOfMindAgentState",
             "title": "Type",
             "type": "string"
          },
          "version": {
             "default": "1.0.0",
             "title": "Version",
             "type": "string"
          },
          "inner_team_state": {
             "title": "Inner Team State",
             "type": "object"
          }
       }
    }

    ```

    Fields:
    :   * `inner_team_state (Mapping[str, Any])`
        * `type (str)`

    *field* inner\_team\_state*: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]* *[Optional]*[#](#autogen_agentchat.state.SocietyOfMindAgentState.inner_team_state "Link to this definition")

    *field* type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'SocietyOfMindAgentState'*[#](#autogen_agentchat.state.SocietyOfMindAgentState.type "Link to this definition")

*pydantic model* SwarmManagerState[#](#autogen_agentchat.state.SwarmManagerState "Link to this definition")
:   Bases: [`BaseGroupChatManagerState`](#autogen_agentchat.state.BaseGroupChatManagerState "autogen_agentchat.state._states.BaseGroupChatManagerState")

    State for [`Swarm`](#autogen_agentchat.teams.Swarm "autogen_agentchat.teams.Swarm") manager.

    Show JSON schema

    ```
    {
       "title": "SwarmManagerState",
       "description": "State for :class:`~autogen_agentchat.teams.Swarm` manager.",
       "type": "object",
       "properties": {
          "type": {
             "default": "SwarmManagerState",
             "title": "Type",
             "type": "string"
          },
          "version": {
             "default": "1.0.0",
             "title": "Version",
             "type": "string"
          },
          "message_thread": {
             "items": {
                "discriminator": {
                   "mapping": {
                      "HandoffMessage": "#/$defs/HandoffMessage",
                      "MemoryQueryEvent": "#/$defs/MemoryQueryEvent",
                      "ModelClientStreamingChunkEvent": "#/$defs/ModelClientStreamingChunkEvent",
                      "MultiModalMessage": "#/$defs/MultiModalMessage",
                      "StopMessage": "#/$defs/StopMessage",
                      "TextMessage": "#/$defs/TextMessage",
                      "ThoughtEvent": "#/$defs/ThoughtEvent",
                      "ToolCallExecutionEvent": "#/$defs/ToolCallExecutionEvent",
                      "ToolCallRequestEvent": "#/$defs/ToolCallRequestEvent",
                      "ToolCallSummaryMessage": "#/$defs/ToolCallSummaryMessage",
                      "UserInputRequestedEvent": "#/$defs/UserInputRequestedEvent"
                   },
                   "propertyName": "type"
                },
                "oneOf": [
                   {
                      "$ref": "#/$defs/ToolCallRequestEvent"
                   },
                   {
                      "$ref": "#/$defs/ToolCallExecutionEvent"
                   },
                   {
                      "$ref": "#/$defs/MemoryQueryEvent"
                   },
                   {
                      "$ref": "#/$defs/UserInputRequestedEvent"
                   },
                   {
                      "$ref": "#/$defs/ModelClientStreamingChunkEvent"
                   },
                   {
                      "$ref": "#/$defs/ThoughtEvent"
                   },
                   {
                      "$ref": "#/$defs/TextMessage"
                   },
                   {
                      "$ref": "#/$defs/MultiModalMessage"
                   },
                   {
                      "$ref": "#/$defs/StopMessage"
                   },
                   {
                      "$ref": "#/$defs/ToolCallSummaryMessage"
                   },
                   {
                      "$ref": "#/$defs/HandoffMessage"
                   }
                ]
             },
             "title": "Message Thread",
             "type": "array"
          },
          "current_turn": {
             "default": 0,
             "title": "Current Turn",
             "type": "integer"
          },
          "current_speaker": {
             "default": "",
             "title": "Current Speaker",
             "type": "string"
          }
       },
       "$defs": {
          "AssistantMessage": {
             "description": "Assistant message are sampled from the language model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "$ref": "#/$defs/FunctionCall"
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "thought": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Thought"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "AssistantMessage",
                   "default": "AssistantMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "AssistantMessage",
             "type": "object"
          },
          "FunctionCall": {
             "properties": {
                "id": {
                   "title": "Id",
                   "type": "string"
                },
                "arguments": {
                   "title": "Arguments",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                }
             },
             "required": [
                "id",
                "arguments",
                "name"
             ],
             "title": "FunctionCall",
             "type": "object"
          },
          "FunctionExecutionResult": {
             "description": "Function execution result contains the output of a function call.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                },
                "call_id": {
                   "title": "Call Id",
                   "type": "string"
                },
                "is_error": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Is Error"
                }
             },
             "required": [
                "content",
                "name",
                "call_id"
             ],
             "title": "FunctionExecutionResult",
             "type": "object"
          },
          "FunctionExecutionResultMessage": {
             "description": "Function execution result message contains the output of multiple function calls.",
             "properties": {
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionExecutionResult"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "FunctionExecutionResultMessage",
                   "default": "FunctionExecutionResultMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "FunctionExecutionResultMessage",
             "type": "object"
          },
          "HandoffMessage": {
             "description": "A message requesting handoff of a conversation to another agent.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "target": {
                   "title": "Target",
                   "type": "string"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "context": {
                   "default": [],
                   "items": {
                      "discriminator": {
                         "mapping": {
                            "AssistantMessage": "#/$defs/AssistantMessage",
                            "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                            "SystemMessage": "#/$defs/SystemMessage",
                            "UserMessage": "#/$defs/UserMessage"
                         },
                         "propertyName": "type"
                      },
                      "oneOf": [
                         {
                            "$ref": "#/$defs/SystemMessage"
                         },
                         {
                            "$ref": "#/$defs/UserMessage"
                         },
                         {
                            "$ref": "#/$defs/AssistantMessage"
                         },
                         {
                            "$ref": "#/$defs/FunctionExecutionResultMessage"
                         }
                      ]
                   },
                   "title": "Context",
                   "type": "array"
                },
                "type": {
                   "const": "HandoffMessage",
                   "default": "HandoffMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "target",
                "content"
             ],
             "title": "HandoffMessage",
             "type": "object"
          },
          "MemoryContent": {
             "description": "A memory content item.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "format": "binary",
                         "type": "string"
                      },
                      {
                         "type": "object"
                      },
                      {}
                   ],
                   "title": "Content"
                },
                "mime_type": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/MemoryMimeType"
                      },
                      {
                         "type": "string"
                      }
                   ],
                   "title": "Mime Type"
                },
                "metadata": {
                   "anyOf": [
                      {
                         "type": "object"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Metadata"
                }
             },
             "required": [
                "content",
                "mime_type"
             ],
             "title": "MemoryContent",
             "type": "object"
          },
          "MemoryMimeType": {
             "description": "Supported MIME types for memory content.",
             "enum": [
                "text/plain",
                "application/json",
                "text/markdown",
                "image/*",
                "application/octet-stream"
             ],
             "title": "MemoryMimeType",
             "type": "string"
          },
          "MemoryQueryEvent": {
             "description": "An event signaling the results of memory queries.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/MemoryContent"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "MemoryQueryEvent",
                   "default": "MemoryQueryEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "MemoryQueryEvent",
             "type": "object"
          },
          "ModelClientStreamingChunkEvent": {
             "description": "An event signaling a text output chunk from a model client in streaming mode.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ModelClientStreamingChunkEvent",
                   "default": "ModelClientStreamingChunkEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ModelClientStreamingChunkEvent",
             "type": "object"
          },
          "MultiModalMessage": {
             "description": "A multimodal message.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "anyOf": [
                         {
                            "type": "string"
                         },
                         {}
                      ]
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "MultiModalMessage",
                   "default": "MultiModalMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "MultiModalMessage",
             "type": "object"
          },
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          },
          "StopMessage": {
             "description": "A message requesting stop of a conversation.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "StopMessage",
                   "default": "StopMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "StopMessage",
             "type": "object"
          },
          "SystemMessage": {
             "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "SystemMessage",
                   "default": "SystemMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "SystemMessage",
             "type": "object"
          },
          "TextMessage": {
             "description": "A text message.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "TextMessage",
                   "default": "TextMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "TextMessage",
             "type": "object"
          },
          "ThoughtEvent": {
             "description": "An event signaling the thought process of an agent.\nIt is used to communicate the reasoning tokens generated by a reasoning model,\nor the extra text content generated by a function call.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ThoughtEvent",
                   "default": "ThoughtEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ThoughtEvent",
             "type": "object"
          },
          "ToolCallExecutionEvent": {
             "description": "An event signaling the execution of tool calls.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionExecutionResult"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "ToolCallExecutionEvent",
                   "default": "ToolCallExecutionEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallExecutionEvent",
             "type": "object"
          },
          "ToolCallRequestEvent": {
             "description": "An event signaling a request to use tools.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionCall"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "ToolCallRequestEvent",
                   "default": "ToolCallRequestEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallRequestEvent",
             "type": "object"
          },
          "ToolCallSummaryMessage": {
             "description": "A message signaling the summary of tool call results.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "ToolCallSummaryMessage",
                   "default": "ToolCallSummaryMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "content"
             ],
             "title": "ToolCallSummaryMessage",
             "type": "object"
          },
          "UserInputRequestedEvent": {
             "description": "An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.",
             "properties": {
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "models_usage": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/RequestUsage"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "metadata": {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "default": {},
                   "title": "Metadata",
                   "type": "object"
                },
                "request_id": {
                   "title": "Request Id",
                   "type": "string"
                },
                "content": {
                   "const": "",
                   "default": "",
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "UserInputRequestedEvent",
                   "default": "UserInputRequestedEvent",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "source",
                "request_id"
             ],
             "title": "UserInputRequestedEvent",
             "type": "object"
          },
          "UserMessage": {
             "description": "User message contains input from end users, or a catch-all for data provided to the model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "anyOf": [
                               {
                                  "type": "string"
                               },
                               {}
                            ]
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "UserMessage",
                   "default": "UserMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "UserMessage",
             "type": "object"
          }
       }
    }

    ```

    Fields:
    :   * `current_speaker (str)`
        * `type (str)`

    *field* current\_speaker*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= ''*[#](#autogen_agentchat.state.SwarmManagerState.current_speaker "Link to this definition")

    *field* type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'SwarmManagerState'*[#](#autogen_agentchat.state.SwarmManagerState.type "Link to this definition")

*pydantic model* TeamState[#](#autogen_agentchat.state.TeamState "Link to this definition")
:   Bases: [`BaseState`](#autogen_agentchat.state.BaseState "autogen_agentchat.state._states.BaseState")

    State for a team of agents.

    Show JSON schema

    ```
    {
       "title": "TeamState",
       "description": "State for a team of agents.",
       "type": "object",
       "properties": {
          "type": {
             "default": "TeamState",
             "title": "Type",
             "type": "string"
          },
          "version": {
             "default": "1.0.0",
             "title": "Version",
             "type": "string"
          },
          "agent_states": {
             "title": "Agent States",
             "type": "object"
          }
       }
    }

    ```

    Fields:
    :   * `agent_states (Mapping[str, Any])`
        * `type (str)`

    *field* agent\_states*: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]* *[Optional]*[#](#autogen_agentchat.state.TeamState.agent_states "Link to this definition")

    *field* type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'TeamState'*[#](#autogen_agentchat.state.TeamState.type "Link to this definition")

### autogen\_core[#](#module-autogen_core "Link to this heading")

*class* Agent(*\*args*, *\*\*kwargs*)[#](#autogen_core.Agent "Link to this definition")
:   Bases: [`Protocol`](https://docs.python.org/3/library/typing.html#typing.Protocol "(in Python v3.13)")

    *property* metadata*: [AgentMetadata](#autogen_core.AgentMetadata "autogen_core._agent_metadata.AgentMetadata")*[#](#autogen_core.Agent.metadata "Link to this definition")
    :   Metadata of the agent.

    *property* id*: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*[#](#autogen_core.Agent.id "Link to this definition")
    :   ID of the agent.

    *async* on\_message(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *ctx: [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_core.Agent.on_message "Link to this definition")
    :   Message handler for the agent. This should only be called by the runtime, not by other agents.

        Parameters:
        :   * **message** (*Any*) – Received message. Type is one of the types in subscriptions.
            * **ctx** ([*MessageContext*](#autogen_core.MessageContext "autogen_core.MessageContext")) – Context of the message.

        Returns:
        :   **Any** – Response to the message. Can be None.

        Raises:
        :   * [**CancelledError**](https://docs.python.org/3/library/asyncio-exceptions.html#asyncio.CancelledError "(in Python v3.13)") – If the message was cancelled.
            * [**CantHandleException**](#autogen_core.exceptions.CantHandleException "autogen_core.exceptions.CantHandleException") – If the agent cannot handle the message.

    *async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.Agent.save_state "Link to this definition")
    :   Save the state of the agent. The result must be JSON serializable.

    *async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.Agent.load_state "Link to this definition")
    :   Load in the state of the agent obtained from save\_state.

        Parameters:
        :   **state** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]*) – State of the agent. Must be JSON serializable.

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.Agent.close "Link to this definition")
    :   Called when the runtime is closed

*class* AgentId(*type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType")*, *key: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_core.AgentId "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    Agent ID uniquely identifies an agent instance within an agent runtime - including distributed runtime. It is the ‘address’ of the agent instance for receiving messages.

    See here for more information: [Agent Identity and Lifecycle](#agentid-and-lifecycle)

    *classmethod* from\_str(*agent\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_core.AgentId.from_str "Link to this definition")
    :   Convert a string of the format `type/key` into an AgentId

    *property* type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.AgentId.type "Link to this definition")
    :   An identifier that associates an agent with a specific factory function.

        Strings may only be composed of alphanumeric letters (a-z) and (0-9), or underscores (\_).

    *property* key*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.AgentId.key "Link to this definition")
    :   Agent instance identifier.

        Strings may only be composed of alphanumeric letters (a-z) and (0-9), or underscores (\_).

*class* AgentProxy(*agent: [AgentId](#autogen_core.AgentId "autogen_core.AgentId")*, *runtime: [AgentRuntime](#autogen_core.AgentRuntime "autogen_core.AgentRuntime")*)[#](#autogen_core.AgentProxy "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    A helper class that allows you to use an [`AgentId`](#autogen_core.AgentId "autogen_core.AgentId") in place of its associated [`Agent`](#autogen_core.Agent "autogen_core.Agent")

    *property* id*: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*[#](#autogen_core.AgentProxy.id "Link to this definition")
    :   Target agent for this proxy

    *property* metadata*: [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[[AgentMetadata](#autogen_core.AgentMetadata "autogen_core._agent_metadata.AgentMetadata")]*[#](#autogen_core.AgentProxy.metadata "Link to this definition")
    :   Metadata of the agent.

    *async* send\_message(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *\**, *sender: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *message\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_core.AgentProxy.send_message "Link to this definition")

    *async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.AgentProxy.save_state "Link to this definition")
    :   Save the state of the agent. The result must be JSON serializable.

    *async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.AgentProxy.load_state "Link to this definition")
    :   Load in the state of the agent obtained from save\_state.

        Parameters:
        :   **state** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]*) – State of the agent. Must be JSON serializable.

*class* AgentMetadata[#](#autogen_core.AgentMetadata "Link to this definition")
:   Bases: [`TypedDict`](https://docs.python.org/3/library/typing.html#typing.TypedDict "(in Python v3.13)")

    type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.AgentMetadata.type "Link to this definition")

    key*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.AgentMetadata.key "Link to this definition")

    description*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.AgentMetadata.description "Link to this definition")

*class* AgentRuntime(*\*args*, *\*\*kwargs*)[#](#autogen_core.AgentRuntime "Link to this definition")
:   Bases: [`Protocol`](https://docs.python.org/3/library/typing.html#typing.Protocol "(in Python v3.13)")

    *async* send\_message(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *recipient: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *\**, *sender: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *message\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_core.AgentRuntime.send_message "Link to this definition")
    :   Send a message to an agent and get a response.

        Parameters:
        :   * **message** (*Any*) – The message to send.
            * **recipient** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent to send the message to.
            * **sender** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId") *|* *None**,* *optional*) – Agent which sent the message. Should **only** be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
            * **cancellation\_token** ([*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken") *|* *None**,* *optional*) – Token used to cancel an in progress . Defaults to None.

        Raises:
        :   * [**CantHandleException**](#autogen_core.exceptions.CantHandleException "autogen_core.exceptions.CantHandleException") – If the recipient cannot handle the message.
            * [**UndeliverableException**](#autogen_core.exceptions.UndeliverableException "autogen_core.exceptions.UndeliverableException") – If the message cannot be delivered.
            * **Other** – Any other exception raised by the recipient.

        Returns:
        :   **Any** – The response from the agent.

    *async* publish\_message(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *topic\_id: [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId")*, *\**, *sender: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *message\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.AgentRuntime.publish_message "Link to this definition")
    :   Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.

        No responses are expected from publishing.

        Parameters:
        :   * **message** (*Any*) – The message to publish.
            * **topic** ([*TopicId*](#autogen_core.TopicId "autogen_core.TopicId")) – The topic to publish the message to.
            * **sender** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId") *|* *None**,* *optional*) – The agent which sent the message. Defaults to None.
            * **cancellation\_token** ([*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken") *|* *None**,* *optional*) – Token used to cancel an in progress. Defaults to None.
            * **message\_id** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *|* *None**,* *optional*) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.

        Raises:
        :   [**UndeliverableException**](#autogen_core.exceptions.UndeliverableException "autogen_core.exceptions.UndeliverableException") – If the message cannot be delivered.

    *async* register\_factory(*type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType")*, *agent\_factory: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[], T | [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[T]]*, *\**, *expected\_class: [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[T] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType")[#](#autogen_core.AgentRuntime.register_factory "Link to this definition")
    :   Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

        Note

        This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

        Example:

        ```
        from dataclasses import dataclass

        from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
        from autogen_core.models import UserMessage

        @dataclass
        class MyMessage:
            content: str

        class MyAgent(RoutedAgent):
            def __init__(self) -> None:
                super().__init__("My core agent")

            @event
            async def handler(self, message: UserMessage, context: MessageContext) -> None:
                print("Event received: ", message.content)

        async def my_agent_factory():
            return MyAgent()

        async def main() -> None:
            runtime: AgentRuntime = ...  # type: ignore
            await runtime.register_factory("my_agent", lambda: MyAgent())

        import asyncio

        asyncio.run(main())

        ```

        Parameters:
        :   * **type** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
            * **agent\_factory** (*Callable**[**[**]**,* *T**]*) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen\_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
            * **expected\_class** ([*type*](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")*[**T**]* *|* *None**,* *optional*) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.

    *async* try\_get\_underlying\_agent\_instance(*id: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *type: [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[T] = Agent*) → T[#](#autogen_core.AgentRuntime.try_get_underlying_agent_instance "Link to this definition")
    :   Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.

        If the underlying agent is not accessible, this will raise an exception.

        Parameters:
        :   * **id** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent id.
            * **type** (*Type**[**T**]**,* *optional*) – The expected type of the agent. Defaults to Agent.

        Returns:
        :   **T** – The concrete agent instance.

        Raises:
        :   * [**LookupError**](https://docs.python.org/3/library/exceptions.html#LookupError "(in Python v3.13)") – If the agent is not found.
            * [**NotAccessibleError**](#autogen_core.exceptions.NotAccessibleError "autogen_core.exceptions.NotAccessibleError") – If the agent is not accessible, for example if it is located remotely.
            * [**TypeError**](https://docs.python.org/3/library/exceptions.html#TypeError "(in Python v3.13)") – If the agent is not of the expected type.

    *async* get(*id: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, */*, *\**, *lazy: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*) → [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")[#](#autogen_core.AgentRuntime.get "Link to this definition")

    *async* get(*type: [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType") | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, */*, *key: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'default'*, *\**, *lazy: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*) → [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")

    *async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.AgentRuntime.save_state "Link to this definition")
    :   Save the state of the entire runtime, including all hosted agents. The only way to restore the state is to pass it to [`load_state()`](#autogen_core.AgentRuntime.load_state "autogen_core.AgentRuntime.load_state").

        The structure of the state is implementation defined and can be any JSON serializable object.

        Returns:
        :   **Mapping[str, Any]** – The saved state.

    *async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.AgentRuntime.load_state "Link to this definition")
    :   Load the state of the entire runtime, including all hosted agents. The state should be the same as the one returned by [`save_state()`](#autogen_core.AgentRuntime.save_state "autogen_core.AgentRuntime.save_state").

        Parameters:
        :   **state** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]*) – The saved state.

    *async* agent\_metadata(*agent: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*) → [AgentMetadata](#autogen_core.AgentMetadata "autogen_core._agent_metadata.AgentMetadata")[#](#autogen_core.AgentRuntime.agent_metadata "Link to this definition")
    :   Get the metadata for an agent.

        Parameters:
        :   **agent** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent id.

        Returns:
        :   **AgentMetadata** – The agent metadata.

    *async* agent\_save\_state(*agent: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*) → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.AgentRuntime.agent_save_state "Link to this definition")
    :   Save the state of a single agent.

        The structure of the state is implementation defined and can be any JSON serializable object.

        Parameters:
        :   **agent** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent id.

        Returns:
        :   **Mapping[str, Any]** – The saved state.

    *async* agent\_load\_state(*agent: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.AgentRuntime.agent_load_state "Link to this definition")
    :   Load the state of a single agent.

        Parameters:
        :   * **agent** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent id.
            * **state** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]*) – The saved state.

    *async* add\_subscription(*subscription: [Subscription](#autogen_core.Subscription "autogen_core._subscription.Subscription")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.AgentRuntime.add_subscription "Link to this definition")
    :   Add a new subscription that the runtime should fulfill when processing published messages

        Parameters:
        :   **subscription** ([*Subscription*](#autogen_core.Subscription "autogen_core.Subscription")) – The subscription to add

    *async* remove\_subscription(*id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.AgentRuntime.remove_subscription "Link to this definition")
    :   Remove a subscription from the runtime

        Parameters:
        :   **id** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – id of the subscription to remove

        Raises:
        :   [**LookupError**](https://docs.python.org/3/library/exceptions.html#LookupError "(in Python v3.13)") – If the subscription does not exist

    add\_message\_serializer(*serializer: [MessageSerializer](#autogen_core.MessageSerializer "autogen_core._serialization.MessageSerializer")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [Sequence](https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence "(in Python v3.13)")[[MessageSerializer](#autogen_core.MessageSerializer "autogen_core._serialization.MessageSerializer")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.AgentRuntime.add_message_serializer "Link to this definition")
    :   Add a new message serialization serializer to the runtime

        Note: This will deduplicate serializers based on the type\_name and data\_content\_type properties

        Parameters:
        :   **serializer** ([*MessageSerializer*](#autogen_core.MessageSerializer "autogen_core.MessageSerializer")*[**Any**]* *|* *Sequence**[*[*MessageSerializer*](#autogen_core.MessageSerializer "autogen_core.MessageSerializer")*[**Any**]**]*) – The serializer/s to add

*class* BaseAgent(*description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_core.BaseAgent "Link to this definition")
:   Bases: [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC "(in Python v3.13)"), [`Agent`](#autogen_core.Agent "autogen_core._agent.Agent")

    *property* metadata*: [AgentMetadata](#autogen_core.AgentMetadata "autogen_core._agent_metadata.AgentMetadata")*[#](#autogen_core.BaseAgent.metadata "Link to this definition")
    :   Metadata of the agent.

    *property* type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.BaseAgent.type "Link to this definition")

    *property* id*: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*[#](#autogen_core.BaseAgent.id "Link to this definition")
    :   ID of the agent.

    *property* runtime*: [AgentRuntime](#autogen_core.AgentRuntime "autogen_core._agent_runtime.AgentRuntime")*[#](#autogen_core.BaseAgent.runtime "Link to this definition")

    *final async* on\_message(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *ctx: [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_core.BaseAgent.on_message "Link to this definition")
    :   Message handler for the agent. This should only be called by the runtime, not by other agents.

        Parameters:
        :   * **message** (*Any*) – Received message. Type is one of the types in subscriptions.
            * **ctx** ([*MessageContext*](#autogen_core.MessageContext "autogen_core.MessageContext")) – Context of the message.

        Returns:
        :   **Any** – Response to the message. Can be None.

        Raises:
        :   * [**CancelledError**](https://docs.python.org/3/library/asyncio-exceptions.html#asyncio.CancelledError "(in Python v3.13)") – If the message was cancelled.
            * [**CantHandleException**](#autogen_core.exceptions.CantHandleException "autogen_core.exceptions.CantHandleException") – If the agent cannot handle the message.

    *abstract async* on\_message\_impl(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *ctx: [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_core.BaseAgent.on_message_impl "Link to this definition")

    *async* send\_message(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *recipient: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *\**, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *message\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_core.BaseAgent.send_message "Link to this definition")
    :   See [`autogen_core.AgentRuntime.send_message()`](#autogen_core.AgentRuntime.send_message "autogen_core.AgentRuntime.send_message") for more information.

    *async* publish\_message(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *topic\_id: [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId")*, *\**, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.BaseAgent.publish_message "Link to this definition")

    *async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.BaseAgent.save_state "Link to this definition")
    :   Save the state of the agent. The result must be JSON serializable.

    *async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.BaseAgent.load_state "Link to this definition")
    :   Load in the state of the agent obtained from save\_state.

        Parameters:
        :   **state** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]*) – State of the agent. Must be JSON serializable.

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.BaseAgent.close "Link to this definition")
    :   Called when the runtime is closed

    *async classmethod* register(*runtime: [AgentRuntime](#autogen_core.AgentRuntime "autogen_core._agent_runtime.AgentRuntime")*, *type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *factory: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[], [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)") | [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[[Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")]]*, *\**, *skip\_class\_subscriptions: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*, *skip\_direct\_message\_subscription: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*) → [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType")[#](#autogen_core.BaseAgent.register "Link to this definition")
    :   Register a virtual subclass of an ABC.

        Returns the subclass, to allow usage as a class decorator.

*class* CacheStore[#](#autogen_core.CacheStore "Link to this definition")
:   Bases: [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC "(in Python v3.13)"), [`Generic`](https://docs.python.org/3/library/typing.html#typing.Generic "(in Python v3.13)")[`T`], [`ComponentBase`](#autogen_core.ComponentBase "autogen_core._component_config.ComponentBase")[`BaseModel`]

    This protocol defines the basic interface for store/cache operations.

    Sub-classes should handle the lifecycle of underlying storage.

    component\_type*: ClassVar[ComponentType]* *= 'cache\_store'*[#](#autogen_core.CacheStore.component_type "Link to this definition")
    :   The logical type of the component.

    *abstract* get(*key: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *default: T | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → T | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.CacheStore.get "Link to this definition")
    :   Retrieve an item from the store.

        Parameters:
        :   * **key** – The key identifying the item in the store.
            * **default** (*optional*) – The default value to return if the key is not found.
              Defaults to None.

        Returns:
        :   **The value associated with the key if found, else the default value.**

    *abstract* set(*key: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *value: T*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.CacheStore.set "Link to this definition")
    :   Set an item in the store.

        Parameters:
        :   * **key** – The key under which the item is to be stored.
            * **value** – The value to be stored in the store.

*class* InMemoryStore[#](#autogen_core.InMemoryStore "Link to this definition")
:   Bases: [`CacheStore`](#autogen_core.CacheStore "autogen_core._cache_store.CacheStore")[`T`], [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`InMemoryStoreConfig`]

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_core.InMemoryStore'*[#](#autogen_core.InMemoryStore.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    component\_config\_schema[#](#autogen_core.InMemoryStore.component_config_schema "Link to this definition")
    :   alias of `InMemoryStoreConfig`

    get(*key: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *default: T | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → T | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.InMemoryStore.get "Link to this definition")
    :   Retrieve an item from the store.

        Parameters:
        :   * **key** – The key identifying the item in the store.
            * **default** (*optional*) – The default value to return if the key is not found.
              Defaults to None.

        Returns:
        :   **The value associated with the key if found, else the default value.**

    set(*key: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *value: T*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.InMemoryStore.set "Link to this definition")
    :   Set an item in the store.

        Parameters:
        :   * **key** – The key under which the item is to be stored.
            * **value** – The value to be stored in the store.

    \_to\_config() → InMemoryStoreConfig[#](#autogen_core.InMemoryStore._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    *classmethod* \_from\_config(*config: InMemoryStoreConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_core.InMemoryStore._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

*class* CancellationToken[#](#autogen_core.CancellationToken "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    A token used to cancel pending async calls

    cancel() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.CancellationToken.cancel "Link to this definition")
    :   Cancel pending async calls linked to this cancellation token.

    is\_cancelled() → [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")[#](#autogen_core.CancellationToken.is_cancelled "Link to this definition")
    :   Check if the CancellationToken has been used

    add\_callback(*callback: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[], [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.CancellationToken.add_callback "Link to this definition")
    :   Attach a callback that will be called when cancel is invoked

    link\_future(*future: Future[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → Future[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.CancellationToken.link_future "Link to this definition")
    :   Link a pending async call to a token to allow its cancellation

*class* AgentInstantiationContext[#](#autogen_core.AgentInstantiationContext "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    A static class that provides context for agent instantiation.

    This static class can be used to access the current runtime and agent ID
    during agent instantiation – inside the factory function or the agent’s
    class constructor.

    Example

    Get the current runtime and agent ID inside the factory function and
    the agent’s constructor:

    ```
    import asyncio
    from dataclasses import dataclass

    from autogen_core import (
        AgentId,
        AgentInstantiationContext,
        MessageContext,
        RoutedAgent,
        SingleThreadedAgentRuntime,
        message_handler,
    )

    @dataclass
    class TestMessage:
        content: str

    class TestAgent(RoutedAgent):
        def __init__(self, description: str):
            super().__init__(description)
            # Get the current runtime -- we don't use it here, but it's available.
            _ = AgentInstantiationContext.current_runtime()
            # Get the current agent ID.
            agent_id = AgentInstantiationContext.current_agent_id()
            print(f"Current AgentID from constructor: {agent_id}")

        @message_handler
        async def handle_test_message(self, message: TestMessage, ctx: MessageContext) -> None:
            print(f"Received message: {message.content}")

    def test_agent_factory() -> TestAgent:
        # Get the current runtime -- we don't use it here, but it's available.
        _ = AgentInstantiationContext.current_runtime()
        # Get the current agent ID.
        agent_id = AgentInstantiationContext.current_agent_id()
        print(f"Current AgentID from factory: {agent_id}")
        return TestAgent(description="Test agent")

    async def main() -> None:
        # Create a SingleThreadedAgentRuntime instance.
        runtime = SingleThreadedAgentRuntime()

        # Start the runtime.
        runtime.start()

        # Register the agent type with a factory function.
        await runtime.register_factory("test_agent", test_agent_factory)

        # Send a message to the agent. The runtime will instantiate the agent and call the message handler.
        await runtime.send_message(TestMessage(content="Hello, world!"), AgentId("test_agent", "default"))

        # Stop the runtime.
        await runtime.stop()

    asyncio.run(main())

    ```

    *classmethod* current\_runtime() → [AgentRuntime](#autogen_core.AgentRuntime "autogen_core._agent_runtime.AgentRuntime")[#](#autogen_core.AgentInstantiationContext.current_runtime "Link to this definition")

    *classmethod* current\_agent\_id() → [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")[#](#autogen_core.AgentInstantiationContext.current_agent_id "Link to this definition")

*class* TopicId(*type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *source: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_core.TopicId "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    TopicId defines the scope of a broadcast message. In essence, agent runtime implements a publish-subscribe model through its broadcast API: when publishing a message, the topic must be specified.

    See here for more information: [Topic](#topic-and-subscription-topic)

    type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.TopicId.type "Link to this definition")
    :   Type of the event that this topic\_id contains. Adhere’s to the cloud event spec.

        Must match the pattern: ^[w-.:=]+Z

        Learn more here: [cloudevents/spec](https://github.com/cloudevents/spec/blob/main/cloudevents/spec.md#type)

    source*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.TopicId.source "Link to this definition")
    :   Identifies the context in which an event happened. Adhere’s to the cloud event spec.

        Learn more here: [cloudevents/spec](https://github.com/cloudevents/spec/blob/main/cloudevents/spec.md#source-1)

    *classmethod* from\_str(*topic\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_core.TopicId.from_str "Link to this definition")
    :   Convert a string of the format `type/source` into a TopicId

*class* Subscription(*\*args*, *\*\*kwargs*)[#](#autogen_core.Subscription "Link to this definition")
:   Bases: [`Protocol`](https://docs.python.org/3/library/typing.html#typing.Protocol "(in Python v3.13)")

    Subscriptions define the topics that an agent is interested in.

    *property* id*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.Subscription.id "Link to this definition")
    :   Get the ID of the subscription.

        Implementations should return a unique ID for the subscription. Usually this is a UUID.

        Returns:
        :   **str** – ID of the subscription.

    is\_match(*topic\_id: [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId")*) → [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")[#](#autogen_core.Subscription.is_match "Link to this definition")
    :   Check if a given topic\_id matches the subscription.

        Parameters:
        :   **topic\_id** ([*TopicId*](#autogen_core.TopicId "autogen_core.TopicId")) – TopicId to check.

        Returns:
        :   **bool** – True if the topic\_id matches the subscription, False otherwise.

    map\_to\_agent(*topic\_id: [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId")*) → [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")[#](#autogen_core.Subscription.map_to_agent "Link to this definition")
    :   Map a topic\_id to an agent. Should only be called if is\_match returns True for the given topic\_id.

        Parameters:
        :   **topic\_id** ([*TopicId*](#autogen_core.TopicId "autogen_core.TopicId")) – TopicId to map.

        Returns:
        :   **AgentId** – ID of the agent that should handle the topic\_id.

        Raises:
        :   [**CantHandleException**](#autogen_core.exceptions.CantHandleException "autogen_core.exceptions.CantHandleException") – If the subscription cannot handle the topic\_id.

*class* MessageContext(*sender: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*, *topic\_id: [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*, *is\_rpc: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*, *message\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_core.MessageContext "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    sender*: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_core.MessageContext.sender "Link to this definition")

    topic\_id*: [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_core.MessageContext.topic_id "Link to this definition")

    is\_rpc*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*[#](#autogen_core.MessageContext.is_rpc "Link to this definition")

    cancellation\_token*: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*[#](#autogen_core.MessageContext.cancellation_token "Link to this definition")

    message\_id*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.MessageContext.message_id "Link to this definition")

*class* AgentType(*type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_core.AgentType "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.AgentType.type "Link to this definition")
    :   String representation of this agent type.

*class* SubscriptionInstantiationContext[#](#autogen_core.SubscriptionInstantiationContext "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    *classmethod* agent\_type() → [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType")[#](#autogen_core.SubscriptionInstantiationContext.agent_type "Link to this definition")

*class* MessageHandlerContext[#](#autogen_core.MessageHandlerContext "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    *classmethod* agent\_id() → [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")[#](#autogen_core.MessageHandlerContext.agent_id "Link to this definition")

*class* MessageSerializer(*\*args*, *\*\*kwargs*)[#](#autogen_core.MessageSerializer "Link to this definition")
:   Bases: [`Protocol`](https://docs.python.org/3/library/typing.html#typing.Protocol "(in Python v3.13)")[`T`]

    *property* data\_content\_type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.MessageSerializer.data_content_type "Link to this definition")

    *property* type\_name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.MessageSerializer.type_name "Link to this definition")

    deserialize(*payload: [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*) → T[#](#autogen_core.MessageSerializer.deserialize "Link to this definition")

    serialize(*message: T*) → [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")[#](#autogen_core.MessageSerializer.serialize "Link to this definition")

*class* UnknownPayload(*type\_name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *data\_content\_type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *payload: [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*)[#](#autogen_core.UnknownPayload "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    type\_name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.UnknownPayload.type_name "Link to this definition")

    data\_content\_type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.UnknownPayload.data_content_type "Link to this definition")

    payload*: [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*[#](#autogen_core.UnknownPayload.payload "Link to this definition")

*class* Image(*image: Image*)[#](#autogen_core.Image "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    Represents an image.

    Example

    Loading an image from a URL:

    ```
    from autogen_core import Image
    from PIL import Image as PILImage
    import aiohttp
    import asyncio

    async def from_url(url: str) -> Image:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                content = await response.read()
                return Image.from_pil(PILImage.open(content))

    image = asyncio.run(from_url("https://example.com/image"))

    ```

    *classmethod* from\_pil(*pil\_image: Image*) → [Image](#autogen_core.Image "autogen_core._image.Image")[#](#autogen_core.Image.from_pil "Link to this definition")

    *classmethod* from\_uri(*uri: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [Image](#autogen_core.Image "autogen_core._image.Image")[#](#autogen_core.Image.from_uri "Link to this definition")

    *classmethod* from\_base64(*base64\_str: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [Image](#autogen_core.Image "autogen_core._image.Image")[#](#autogen_core.Image.from_base64 "Link to this definition")

    to\_base64() → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_core.Image.to_base64 "Link to this definition")

    *classmethod* from\_file(*file\_path: [Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)")*) → [Image](#autogen_core.Image "autogen_core._image.Image")[#](#autogen_core.Image.from_file "Link to this definition")

    *property* data\_uri*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.Image.data_uri "Link to this definition")

    to\_openai\_format(*detail: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['auto', 'low', 'high'] = 'auto'*) → [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.Image.to_openai_format "Link to this definition")

*class* RoutedAgent(*description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_core.RoutedAgent "Link to this definition")
:   Bases: [`BaseAgent`](#autogen_core.BaseAgent "autogen_core._base_agent.BaseAgent")

    A base class for agents that route messages to handlers based on the type of the message
    and optional matching functions.

    To create a routed agent, subclass this class and add message handlers as methods decorated with
    either [`event()`](#autogen_core.event "autogen_core.event") or [`rpc()`](#autogen_core.rpc "autogen_core.rpc") decorator.

    Example:

    ```
    from dataclasses import dataclass
    from autogen_core import MessageContext
    from autogen_core import RoutedAgent, event, rpc

    @dataclass
    class Message:
        pass

    @dataclass
    class MessageWithContent:
        content: str

    @dataclass
    class Response:
        pass

    class MyAgent(RoutedAgent):
        def __init__(self):
            super().__init__("MyAgent")

        @event
        async def handle_event_message(self, message: Message, ctx: MessageContext) -> None:
            assert ctx.topic_id is not None
            await self.publish_message(MessageWithContent("event handled"), ctx.topic_id)

        @rpc(match=lambda message, ctx: message.content == "special")  # type: ignore
        async def handle_special_rpc_message(self, message: MessageWithContent, ctx: MessageContext) -> Response:
            return Response()

    ```

    *async* on\_message\_impl(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *ctx: [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.RoutedAgent.on_message_impl "Link to this definition")
    :   Handle a message by routing it to the appropriate message handler.
        Do not override this method in subclasses. Instead, add message handlers as methods decorated with
        either the [`event()`](#autogen_core.event "autogen_core.event") or [`rpc()`](#autogen_core.rpc "autogen_core.rpc") decorator.

    *async* on\_unhandled\_message(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *ctx: [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.RoutedAgent.on_unhandled_message "Link to this definition")
    :   Called when a message is received that does not have a matching message handler.
        The default implementation logs an info message.

*class* ClosureAgent(*description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *closure: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[ClosureContext](#autogen_core.ClosureContext "autogen_core._closure_agent.ClosureContext"), T, [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")], [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]]*, *\**, *unknown\_type\_policy: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['error', 'warn', 'ignore'] = 'warn'*)[#](#autogen_core.ClosureAgent "Link to this definition")
:   Bases: [`BaseAgent`](#autogen_core.BaseAgent "autogen_core._base_agent.BaseAgent"), [`ClosureContext`](#autogen_core.ClosureContext "autogen_core._closure_agent.ClosureContext")

    *property* metadata*: [AgentMetadata](#autogen_core.AgentMetadata "autogen_core._agent_metadata.AgentMetadata")*[#](#autogen_core.ClosureAgent.metadata "Link to this definition")
    :   Metadata of the agent.

    *property* id*: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*[#](#autogen_core.ClosureAgent.id "Link to this definition")
    :   ID of the agent.

    *property* runtime*: [AgentRuntime](#autogen_core.AgentRuntime "autogen_core._agent_runtime.AgentRuntime")*[#](#autogen_core.ClosureAgent.runtime "Link to this definition")

    *async* on\_message\_impl(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *ctx: [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_core.ClosureAgent.on_message_impl "Link to this definition")

    *async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.ClosureAgent.save_state "Link to this definition")
    :   Closure agents do not have state. So this method always returns an empty dictionary.

    *async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.ClosureAgent.load_state "Link to this definition")
    :   Closure agents do not have state. So this method does nothing.

    *async classmethod* register\_closure(*runtime: [AgentRuntime](#autogen_core.AgentRuntime "autogen_core._agent_runtime.AgentRuntime")*, *type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *closure: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[ClosureContext](#autogen_core.ClosureContext "autogen_core._closure_agent.ClosureContext"), T, [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")], [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]]*, *\**, *unknown\_type\_policy: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['error', 'warn', 'ignore'] = 'warn'*, *skip\_direct\_message\_subscription: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*, *description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = ''*, *subscriptions: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[], [list](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.13)")[[Subscription](#autogen_core.Subscription "autogen_core._subscription.Subscription")] | [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[[list](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.13)")[[Subscription](#autogen_core.Subscription "autogen_core._subscription.Subscription")]]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType")[#](#autogen_core.ClosureAgent.register_closure "Link to this definition")
    :   The closure agent allows you to define an agent using a closure, or function without needing to define a class. It allows values to be extracted out of the runtime.

        The closure can define the type of message which is expected, or Any can be used to accept any type of message.

        Example:

        ```
        import asyncio
        from autogen_core import SingleThreadedAgentRuntime, MessageContext, ClosureAgent, ClosureContext
        from dataclasses import dataclass

        from autogen_core._default_subscription import DefaultSubscription
        from autogen_core._default_topic import DefaultTopicId

        @dataclass
        class MyMessage:
            content: str

        async def main():
            queue = asyncio.Queue[MyMessage]()

            async def output_result(_ctx: ClosureContext, message: MyMessage, ctx: MessageContext) -> None:
                await queue.put(message)

            runtime = SingleThreadedAgentRuntime()
            await ClosureAgent.register_closure(
                runtime, "output_result", output_result, subscriptions=lambda: [DefaultSubscription()]
            )

            runtime.start()
            await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
            await runtime.stop_when_idle()

            result = await queue.get()
            print(result)

        asyncio.run(main())

        ```

        Parameters:
        :   * **runtime** ([*AgentRuntime*](#autogen_core.AgentRuntime "autogen_core.AgentRuntime")) – Runtime to register the agent to
            * **type** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – Agent type of registered agent
            * **closure** (*Callable**[**[*[*ClosureContext*](#autogen_core.ClosureContext "autogen_core.ClosureContext")*,* *T**,* [*MessageContext*](#autogen_core.MessageContext "autogen_core.MessageContext")*]**,* *Awaitable**[**Any**]**]*) – Closure to handle messages
            * **unknown\_type\_policy** (*Literal**[**"error"**,* *"warn"**,* *"ignore"**]**,* *optional*) – What to do if a type is encountered that does not match the closure type. Defaults to “warn”.
            * **skip\_direct\_message\_subscription** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – Do not add direct message subscription for this agent. Defaults to False.
            * **description** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – Description of what agent does. Defaults to “”.
            * **subscriptions** (*Callable**[**[**]**,* [*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.13)")*[*[*Subscription*](#autogen_core.Subscription "autogen_core.Subscription")*]* *|* *Awaitable**[*[*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.13)")*[*[*Subscription*](#autogen_core.Subscription "autogen_core.Subscription")*]**]**]* *|* *None**,* *optional*) – List of subscriptions for this closure agent. Defaults to None.

        Returns:
        :   **AgentType** – Type of the agent that was registered

*class* ClosureContext(*\*args*, *\*\*kwargs*)[#](#autogen_core.ClosureContext "Link to this definition")
:   Bases: [`Protocol`](https://docs.python.org/3/library/typing.html#typing.Protocol "(in Python v3.13)")

    *property* id*: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*[#](#autogen_core.ClosureContext.id "Link to this definition")

    *async* send\_message(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *recipient: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *\**, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *message\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_core.ClosureContext.send_message "Link to this definition")

    *async* publish\_message(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *topic\_id: [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId")*, *\**, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.ClosureContext.publish_message "Link to this definition")

message\_handler(*func: [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[AgentT, ReceivesT, [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")], [Coroutine](https://docs.python.org/3/library/typing.html#typing.Coroutine "(in Python v3.13)")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), ProducesT]] = None*, *\**, *strict: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*, *match: [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[ReceivesT, [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")], [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")] = None*) → [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[AgentT, ReceivesT, [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")], [Coroutine](https://docs.python.org/3/library/typing.html#typing.Coroutine "(in Python v3.13)")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), ProducesT]]], MessageHandler[AgentT, ReceivesT, ProducesT]] | MessageHandler[AgentT, ReceivesT, ProducesT][#](#autogen_core.message_handler "Link to this definition")
:   Decorator for generic message handlers.

    Add this decorator to methods in a [`RoutedAgent`](#autogen_core.RoutedAgent "autogen_core.RoutedAgent") class that are intended to handle both event and RPC messages.
    These methods must have a specific signature that needs to be followed for it to be valid:

    * The method must be an async method.
    * The method must be decorated with the @message\_handler decorator.
    * The method must have exactly 3 arguments:
      :   1. self
          2. message: The message to be handled, this must be type-hinted with the message type that it is intended to handle.
          3. ctx: A [`autogen_core.MessageContext`](#autogen_core.MessageContext "autogen_core.MessageContext") object.
    * The method must be type hinted with what message types it can return as a response, or it can return None if it does not return anything.

    Handlers can handle more than one message type by accepting a Union of the message types. It can also return more than one message type by returning a Union of the message types.

    Parameters:
    :   * **func** – The function to be decorated.
        * **strict** – If True, the handler will raise an exception if the message type or return type is not in the target types. If False, it will log a warning instead.
        * **match** – A function that takes the message and the context as arguments and returns a boolean. This is used for secondary routing after the message type. For handlers addressing the same message type, the match function is applied in alphabetical order of the handlers and the first matching handler will be called while the rest are skipped. If None, the first handler in alphabetical order matching the same message type will be called.

event(*func: [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[AgentT, ReceivesT, [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")], [Coroutine](https://docs.python.org/3/library/typing.html#typing.Coroutine "(in Python v3.13)")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]] = None*, *\**, *strict: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*, *match: [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[ReceivesT, [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")], [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")] = None*) → [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[AgentT, ReceivesT, [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")], [Coroutine](https://docs.python.org/3/library/typing.html#typing.Coroutine "(in Python v3.13)")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]]], MessageHandler[AgentT, ReceivesT, [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]] | MessageHandler[AgentT, ReceivesT, [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_core.event "Link to this definition")
:   Decorator for event message handlers.

    Add this decorator to methods in a [`RoutedAgent`](#autogen_core.RoutedAgent "autogen_core.RoutedAgent") class that are intended to handle event messages.
    These methods must have a specific signature that needs to be followed for it to be valid:

    * The method must be an async method.
    * The method must be decorated with the @message\_handler decorator.
    * The method must have exactly 3 arguments:
      :   1. self
          2. message: The event message to be handled, this must be type-hinted with the message type that it is intended to handle.
          3. ctx: A [`autogen_core.MessageContext`](#autogen_core.MessageContext "autogen_core.MessageContext") object.
    * The method must return None.

    Handlers can handle more than one message type by accepting a Union of the message types.

    Parameters:
    :   * **func** – The function to be decorated.
        * **strict** – If True, the handler will raise an exception if the message type is not in the target types. If False, it will log a warning instead.
        * **match** – A function that takes the message and the context as arguments and returns a boolean. This is used for secondary routing after the message type. For handlers addressing the same message type, the match function is applied in alphabetical order of the handlers and the first matching handler will be called while the rest are skipped. If None, the first handler in alphabetical order matching the same message type will be called.

rpc(*func: [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[AgentT, ReceivesT, [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")], [Coroutine](https://docs.python.org/3/library/typing.html#typing.Coroutine "(in Python v3.13)")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), ProducesT]] = None*, *\**, *strict: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*, *match: [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[ReceivesT, [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")], [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")] = None*) → [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[AgentT, ReceivesT, [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")], [Coroutine](https://docs.python.org/3/library/typing.html#typing.Coroutine "(in Python v3.13)")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), ProducesT]]], MessageHandler[AgentT, ReceivesT, ProducesT]] | MessageHandler[AgentT, ReceivesT, ProducesT][#](#autogen_core.rpc "Link to this definition")
:   Decorator for RPC message handlers.

    Add this decorator to methods in a [`RoutedAgent`](#autogen_core.RoutedAgent "autogen_core.RoutedAgent") class that are intended to handle RPC messages.
    These methods must have a specific signature that needs to be followed for it to be valid:

    * The method must be an async method.
    * The method must be decorated with the @message\_handler decorator.
    * The method must have exactly 3 arguments:
      :   1. self
          2. message: The message to be handled, this must be type-hinted with the message type that it is intended to handle.
          3. ctx: A [`autogen_core.MessageContext`](#autogen_core.MessageContext "autogen_core.MessageContext") object.
    * The method must be type hinted with what message types it can return as a response, or it can return None if it does not return anything.

    Handlers can handle more than one message type by accepting a Union of the message types. It can also return more than one message type by returning a Union of the message types.

    Parameters:
    :   * **func** – The function to be decorated.
        * **strict** – If True, the handler will raise an exception if the message type or return type is not in the target types. If False, it will log a warning instead.
        * **match** – A function that takes the message and the context as arguments and returns a boolean. This is used for secondary routing after the message type. For handlers addressing the same message type, the match function is applied in alphabetical order of the handlers and the first matching handler will be called while the rest are skipped. If None, the first handler in alphabetical order matching the same message type will be called.

*class* FunctionCall(*id: 'str'*, *arguments: 'str'*, *name: 'str'*)[#](#autogen_core.FunctionCall "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    id*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.FunctionCall.id "Link to this definition")

    arguments*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.FunctionCall.arguments "Link to this definition")

    name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.FunctionCall.name "Link to this definition")

*class* TypeSubscription(*topic\_type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *agent\_type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType")*, *id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_core.TypeSubscription "Link to this definition")
:   Bases: [`Subscription`](#autogen_core.Subscription "autogen_core._subscription.Subscription")

    This subscription matches on topics based on the type and maps to agents using the source of the topic as the agent key.

    This subscription causes each source to have its own agent instance.

    Example

    ```
    from autogen_core import TypeSubscription

    subscription = TypeSubscription(topic_type="t1", agent_type="a1")

    ```

    In this case:

    * A topic\_id with type t1 and source s1 will be handled by an agent of type a1 with key s1
    * A topic\_id with type t1 and source s2 will be handled by an agent of type a1 with key s2.

    Parameters:
    :   * **topic\_type** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – Topic type to match against
        * **agent\_type** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – Agent type to handle this subscription

    *property* id*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.TypeSubscription.id "Link to this definition")
    :   Get the ID of the subscription.

        Implementations should return a unique ID for the subscription. Usually this is a UUID.

        Returns:
        :   **str** – ID of the subscription.

    *property* topic\_type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.TypeSubscription.topic_type "Link to this definition")

    *property* agent\_type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.TypeSubscription.agent_type "Link to this definition")

    is\_match(*topic\_id: [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId")*) → [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")[#](#autogen_core.TypeSubscription.is_match "Link to this definition")
    :   Check if a given topic\_id matches the subscription.

        Parameters:
        :   **topic\_id** ([*TopicId*](#autogen_core.TopicId "autogen_core.TopicId")) – TopicId to check.

        Returns:
        :   **bool** – True if the topic\_id matches the subscription, False otherwise.

    map\_to\_agent(*topic\_id: [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId")*) → [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")[#](#autogen_core.TypeSubscription.map_to_agent "Link to this definition")
    :   Map a topic\_id to an agent. Should only be called if is\_match returns True for the given topic\_id.

        Parameters:
        :   **topic\_id** ([*TopicId*](#autogen_core.TopicId "autogen_core.TopicId")) – TopicId to map.

        Returns:
        :   **AgentId** – ID of the agent that should handle the topic\_id.

        Raises:
        :   [**CantHandleException**](#autogen_core.exceptions.CantHandleException "autogen_core.exceptions.CantHandleException") – If the subscription cannot handle the topic\_id.

*class* DefaultSubscription(*topic\_type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'default'*, *agent\_type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_core.DefaultSubscription "Link to this definition")
:   Bases: [`TypeSubscription`](#autogen_core.TypeSubscription "autogen_core._type_subscription.TypeSubscription")

    The default subscription is designed to be a sensible default for applications that only need global scope for agents.

    This topic by default uses the “default” topic type and attempts to detect the agent type to use based on the instantiation context.

    Parameters:
    :   * **topic\_type** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The topic type to subscribe to. Defaults to “default”.
        * **agent\_type** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The agent type to use for the subscription. Defaults to None, in which case it will attempt to detect the agent type based on the instantiation context.

*class* DefaultTopicId(*type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'default'*, *source: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_core.DefaultTopicId "Link to this definition")
:   Bases: [`TopicId`](#autogen_core.TopicId "autogen_core._topic.TopicId")

    DefaultTopicId provides a sensible default for the topic\_id and source fields of a TopicId.

    If created in the context of a message handler, the source will be set to the agent\_id of the message handler, otherwise it will be set to “default”.

    Parameters:
    :   * **type** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – Topic type to publish message to. Defaults to “default”.
        * **source** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *|* *None**,* *optional*) – Topic source to publish message to. If None, the source will be set to the agent\_id of the message handler if in the context of a message handler, otherwise it will be set to “default”. Defaults to None.

default\_subscription(*cls: [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[BaseAgentType] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[BaseAgentType]], [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[BaseAgentType]] | [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[BaseAgentType][#](#autogen_core.default_subscription "Link to this definition")

type\_subscription(*topic\_type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[BaseAgentType]], [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[BaseAgentType]][#](#autogen_core.type_subscription "Link to this definition")

*class* TypePrefixSubscription(*topic\_type\_prefix: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *agent\_type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType")*, *id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_core.TypePrefixSubscription "Link to this definition")
:   Bases: [`Subscription`](#autogen_core.Subscription "autogen_core._subscription.Subscription")

    This subscription matches on topics based on a prefix of the type and maps to agents using the source of the topic as the agent key.

    This subscription causes each source to have its own agent instance.

    Example

    ```
    from autogen_core import TypePrefixSubscription

    subscription = TypePrefixSubscription(topic_type_prefix="t1", agent_type="a1")

    ```

    In this case:

    * A topic\_id with type t1 and source s1 will be handled by an agent of type a1 with key s1
    * A topic\_id with type t1 and source s2 will be handled by an agent of type a1 with key s2.
    * A topic\_id with type t1SUFFIX and source s2 will be handled by an agent of type a1 with key s2.

    Parameters:
    :   * **topic\_type\_prefix** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – Topic type prefix to match against
        * **agent\_type** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – Agent type to handle this subscription

    *property* id*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.TypePrefixSubscription.id "Link to this definition")
    :   Get the ID of the subscription.

        Implementations should return a unique ID for the subscription. Usually this is a UUID.

        Returns:
        :   **str** – ID of the subscription.

    *property* topic\_type\_prefix*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.TypePrefixSubscription.topic_type_prefix "Link to this definition")

    *property* agent\_type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.TypePrefixSubscription.agent_type "Link to this definition")

    is\_match(*topic\_id: [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId")*) → [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")[#](#autogen_core.TypePrefixSubscription.is_match "Link to this definition")
    :   Check if a given topic\_id matches the subscription.

        Parameters:
        :   **topic\_id** ([*TopicId*](#autogen_core.TopicId "autogen_core.TopicId")) – TopicId to check.

        Returns:
        :   **bool** – True if the topic\_id matches the subscription, False otherwise.

    map\_to\_agent(*topic\_id: [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId")*) → [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")[#](#autogen_core.TypePrefixSubscription.map_to_agent "Link to this definition")
    :   Map a topic\_id to an agent. Should only be called if is\_match returns True for the given topic\_id.

        Parameters:
        :   **topic\_id** ([*TopicId*](#autogen_core.TopicId "autogen_core.TopicId")) – TopicId to map.

        Returns:
        :   **AgentId** – ID of the agent that should handle the topic\_id.

        Raises:
        :   [**CantHandleException**](#autogen_core.exceptions.CantHandleException "autogen_core.exceptions.CantHandleException") – If the subscription cannot handle the topic\_id.

JSON\_DATA\_CONTENT\_TYPE *= 'application/json'*[#](#autogen_core.JSON_DATA_CONTENT_TYPE "Link to this definition")
:   The content type for JSON data.

PROTOBUF\_DATA\_CONTENT\_TYPE *= 'application/x-protobuf'*[#](#autogen_core.PROTOBUF_DATA_CONTENT_TYPE "Link to this definition")
:   The content type for Protobuf data.

*class* SingleThreadedAgentRuntime(*\**, *intervention\_handlers: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[InterventionHandler](#autogen_core.InterventionHandler "autogen_core._intervention.InterventionHandler")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *tracer\_provider: TracerProvider | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *ignore\_unhandled\_exceptions: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*)[#](#autogen_core.SingleThreadedAgentRuntime "Link to this definition")
:   Bases: [`AgentRuntime`](#autogen_core.AgentRuntime "autogen_core._agent_runtime.AgentRuntime")

    A single-threaded agent runtime that processes all messages using a single asyncio queue.
    Messages are delivered in the order they are received, and the runtime processes
    each message in a separate asyncio task concurrently.

    Note

    This runtime is suitable for development and standalone applications.
    It is not suitable for high-throughput or high-concurrency scenarios.

    Parameters:
    :   * **intervention\_handlers** (*List**[*[*InterventionHandler*](#autogen_core.InterventionHandler "autogen_core.InterventionHandler")*]**,* *optional*) – A list of intervention
          handlers that can intercept messages before they are sent or published. Defaults to None.
        * **tracer\_provider** (*TracerProvider**,* *optional*) – The tracer provider to use for tracing. Defaults to None.
        * **ignore\_unhandled\_exceptions** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – Whether to ignore unhandled exceptions in that occur in agent event handlers. Any background exceptions will be raised on the next call to process\_next or from an awaited stop, stop\_when\_idle or stop\_when. Note, this does not apply to RPC handlers. Defaults to True.

    Examples

    A simple example of creating a runtime, registering an agent, sending a message and stopping the runtime:

    ```
    import asyncio
    from dataclasses import dataclass

    from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler

    @dataclass
    class MyMessage:
        content: str

    class MyAgent(RoutedAgent):
        @message_handler
        async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
            print(f"Received message: {message.content}")

    async def main() -> None:
        # Create a runtime and register the agent
        runtime = SingleThreadedAgentRuntime()
        await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))

        # Start the runtime, send a message and stop the runtime
        runtime.start()
        await runtime.send_message(MyMessage("Hello, world!"), recipient=AgentId("my_agent", "default"))
        await runtime.stop()

    asyncio.run(main())

    ```

    An example of creating a runtime, registering an agent, publishing a message and stopping the runtime:

    ```
    import asyncio
    from dataclasses import dataclass

    from autogen_core import (
        DefaultTopicId,
        MessageContext,
        RoutedAgent,
        SingleThreadedAgentRuntime,
        default_subscription,
        message_handler,
    )

    @dataclass
    class MyMessage:
        content: str

    # The agent is subscribed to the default topic.
    @default_subscription
    class MyAgent(RoutedAgent):
        @message_handler
        async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
            print(f"Received message: {message.content}")

    async def main() -> None:
        # Create a runtime and register the agent
        runtime = SingleThreadedAgentRuntime()
        await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))

        # Start the runtime.
        runtime.start()
        # Publish a message to the default topic that the agent is subscribed to.
        await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
        # Wait for the message to be processed and then stop the runtime.
        await runtime.stop_when_idle()

    asyncio.run(main())

    ```

    *property* unprocessed\_messages\_count*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*[#](#autogen_core.SingleThreadedAgentRuntime.unprocessed_messages_count "Link to this definition")

    *async* send\_message(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *recipient: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *\**, *sender: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *message\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_core.SingleThreadedAgentRuntime.send_message "Link to this definition")
    :   Send a message to an agent and get a response.

        Parameters:
        :   * **message** (*Any*) – The message to send.
            * **recipient** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent to send the message to.
            * **sender** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId") *|* *None**,* *optional*) – Agent which sent the message. Should **only** be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
            * **cancellation\_token** ([*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken") *|* *None**,* *optional*) – Token used to cancel an in progress . Defaults to None.

        Raises:
        :   * [**CantHandleException**](#autogen_core.exceptions.CantHandleException "autogen_core.exceptions.CantHandleException") – If the recipient cannot handle the message.
            * [**UndeliverableException**](#autogen_core.exceptions.UndeliverableException "autogen_core.exceptions.UndeliverableException") – If the message cannot be delivered.
            * **Other** – Any other exception raised by the recipient.

        Returns:
        :   **Any** – The response from the agent.

    *async* publish\_message(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *topic\_id: [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId")*, *\**, *sender: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *message\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.SingleThreadedAgentRuntime.publish_message "Link to this definition")
    :   Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.

        No responses are expected from publishing.

        Parameters:
        :   * **message** (*Any*) – The message to publish.
            * **topic** ([*TopicId*](#autogen_core.TopicId "autogen_core.TopicId")) – The topic to publish the message to.
            * **sender** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId") *|* *None**,* *optional*) – The agent which sent the message. Defaults to None.
            * **cancellation\_token** ([*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken") *|* *None**,* *optional*) – Token used to cancel an in progress. Defaults to None.
            * **message\_id** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *|* *None**,* *optional*) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.

        Raises:
        :   [**UndeliverableException**](#autogen_core.exceptions.UndeliverableException "autogen_core.exceptions.UndeliverableException") – If the message cannot be delivered.

    *async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.SingleThreadedAgentRuntime.save_state "Link to this definition")
    :   Save the state of all instantiated agents.

        This method calls the [`save_state()`](#autogen_core.BaseAgent.save_state "autogen_core.BaseAgent.save_state") method on each agent and returns a dictionary
        mapping agent IDs to their state.

        Note

        This method does not currently save the subscription state. We will add this in the future.

        Returns:
        :   **A dictionary mapping agent IDs to their state.**

    *async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.SingleThreadedAgentRuntime.load_state "Link to this definition")
    :   Load the state of all instantiated agents.

        This method calls the [`load_state()`](#autogen_core.BaseAgent.load_state "autogen_core.BaseAgent.load_state") method on each agent with the state
        provided in the dictionary. The keys of the dictionary are the agent IDs, and the values are the state
        dictionaries returned by the [`save_state()`](#autogen_core.BaseAgent.save_state "autogen_core.BaseAgent.save_state") method.

        Note

        This method does not currently load the subscription state. We will add this in the future.

    *async* process\_next() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.SingleThreadedAgentRuntime.process_next "Link to this definition")
    :   Process the next message in the queue.

        If there is an unhandled exception in the background task, it will be raised here. process\_next cannot be called again after an unhandled exception is raised.

    start() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.SingleThreadedAgentRuntime.start "Link to this definition")
    :   Start the runtime message processing loop. This runs in a background task.

        Example:

        ```
        import asyncio
        from autogen_core import SingleThreadedAgentRuntime

        async def main() -> None:
            runtime = SingleThreadedAgentRuntime()
            runtime.start()

            # ... do other things ...

            await runtime.stop()

        asyncio.run(main())

        ```

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.SingleThreadedAgentRuntime.close "Link to this definition")
    :   Calls [`stop()`](#autogen_core.SingleThreadedAgentRuntime.stop "autogen_core.SingleThreadedAgentRuntime.stop") if applicable and the [`Agent.close()`](#autogen_core.Agent.close "autogen_core.Agent.close") method on all instantiated agents

    *async* stop() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.SingleThreadedAgentRuntime.stop "Link to this definition")
    :   Immediately stop the runtime message processing loop. The currently processing message will be completed, but all others following it will be discarded.

    *async* stop\_when\_idle() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.SingleThreadedAgentRuntime.stop_when_idle "Link to this definition")
    :   Stop the runtime message processing loop when there is
        no outstanding message being processed or queued. This is the most common way to stop the runtime.

    *async* stop\_when(*condition: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[], [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.SingleThreadedAgentRuntime.stop_when "Link to this definition")
    :   Stop the runtime message processing loop when the condition is met.

        Caution

        This method is not recommended to be used, and is here for legacy
        reasons. It will spawn a busy loop to continually check the
        condition. It is much more efficient to call stop\_when\_idle or
        stop instead. If you need to stop the runtime based on a
        condition, consider using a background task and asyncio.Event to
        signal when the condition is met and the background task should call
        stop.

    *async* agent\_metadata(*agent: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*) → [AgentMetadata](#autogen_core.AgentMetadata "autogen_core._agent_metadata.AgentMetadata")[#](#autogen_core.SingleThreadedAgentRuntime.agent_metadata "Link to this definition")
    :   Get the metadata for an agent.

        Parameters:
        :   **agent** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent id.

        Returns:
        :   **AgentMetadata** – The agent metadata.

    *async* agent\_save\_state(*agent: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*) → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.SingleThreadedAgentRuntime.agent_save_state "Link to this definition")
    :   Save the state of a single agent.

        The structure of the state is implementation defined and can be any JSON serializable object.

        Parameters:
        :   **agent** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent id.

        Returns:
        :   **Mapping[str, Any]** – The saved state.

    *async* agent\_load\_state(*agent: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.SingleThreadedAgentRuntime.agent_load_state "Link to this definition")
    :   Load the state of a single agent.

        Parameters:
        :   * **agent** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent id.
            * **state** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]*) – The saved state.

    *async* register\_factory(*type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType")*, *agent\_factory: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[], T | [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[T]]*, *\**, *expected\_class: [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[T] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType")[#](#autogen_core.SingleThreadedAgentRuntime.register_factory "Link to this definition")
    :   Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

        Note

        This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

        Example:

        ```
        from dataclasses import dataclass

        from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
        from autogen_core.models import UserMessage

        @dataclass
        class MyMessage:
            content: str

        class MyAgent(RoutedAgent):
            def __init__(self) -> None:
                super().__init__("My core agent")

            @event
            async def handler(self, message: UserMessage, context: MessageContext) -> None:
                print("Event received: ", message.content)

        async def my_agent_factory():
            return MyAgent()

        async def main() -> None:
            runtime: AgentRuntime = ...  # type: ignore
            await runtime.register_factory("my_agent", lambda: MyAgent())

        import asyncio

        asyncio.run(main())

        ```

        Parameters:
        :   * **type** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
            * **agent\_factory** (*Callable**[**[**]**,* *T**]*) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen\_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
            * **expected\_class** ([*type*](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")*[**T**]* *|* *None**,* *optional*) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.

    *async* try\_get\_underlying\_agent\_instance(*id: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *type: [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[T] = Agent*) → T[#](#autogen_core.SingleThreadedAgentRuntime.try_get_underlying_agent_instance "Link to this definition")
    :   Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.

        If the underlying agent is not accessible, this will raise an exception.

        Parameters:
        :   * **id** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent id.
            * **type** (*Type**[**T**]**,* *optional*) – The expected type of the agent. Defaults to Agent.

        Returns:
        :   **T** – The concrete agent instance.

        Raises:
        :   * [**LookupError**](https://docs.python.org/3/library/exceptions.html#LookupError "(in Python v3.13)") – If the agent is not found.
            * [**NotAccessibleError**](#autogen_core.exceptions.NotAccessibleError "autogen_core.exceptions.NotAccessibleError") – If the agent is not accessible, for example if it is located remotely.
            * [**TypeError**](https://docs.python.org/3/library/exceptions.html#TypeError "(in Python v3.13)") – If the agent is not of the expected type.

    *async* add\_subscription(*subscription: [Subscription](#autogen_core.Subscription "autogen_core._subscription.Subscription")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.SingleThreadedAgentRuntime.add_subscription "Link to this definition")
    :   Add a new subscription that the runtime should fulfill when processing published messages

        Parameters:
        :   **subscription** ([*Subscription*](#autogen_core.Subscription "autogen_core.Subscription")) – The subscription to add

    *async* remove\_subscription(*id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.SingleThreadedAgentRuntime.remove_subscription "Link to this definition")
    :   Remove a subscription from the runtime

        Parameters:
        :   **id** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – id of the subscription to remove

        Raises:
        :   [**LookupError**](https://docs.python.org/3/library/exceptions.html#LookupError "(in Python v3.13)") – If the subscription does not exist

    *async* get(*id\_or\_type: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType") | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, */*, *key: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'default'*, *\**, *lazy: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*) → [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")[#](#autogen_core.SingleThreadedAgentRuntime.get "Link to this definition")

    add\_message\_serializer(*serializer: [MessageSerializer](#autogen_core.MessageSerializer "autogen_core._serialization.MessageSerializer")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [Sequence](https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence "(in Python v3.13)")[[MessageSerializer](#autogen_core.MessageSerializer "autogen_core._serialization.MessageSerializer")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.SingleThreadedAgentRuntime.add_message_serializer "Link to this definition")
    :   Add a new message serialization serializer to the runtime

        Note: This will deduplicate serializers based on the type\_name and data\_content\_type properties

        Parameters:
        :   **serializer** ([*MessageSerializer*](#autogen_core.MessageSerializer "autogen_core.MessageSerializer")*[**Any**]* *|* *Sequence**[*[*MessageSerializer*](#autogen_core.MessageSerializer "autogen_core.MessageSerializer")*[**Any**]**]*) – The serializer/s to add

ROOT\_LOGGER\_NAME *= 'autogen\_core'*[#](#autogen_core.ROOT_LOGGER_NAME "Link to this definition")
:   The name of the root logger.

EVENT\_LOGGER\_NAME *= 'autogen\_core.events'*[#](#autogen_core.EVENT_LOGGER_NAME "Link to this definition")
:   The name of the logger used for structured events.

TRACE\_LOGGER\_NAME *= 'autogen\_core.trace'*[#](#autogen_core.TRACE_LOGGER_NAME "Link to this definition")
:   Logger name used for developer intended trace logging. The content and format of this log should not be depended upon.

*class* Component[#](#autogen_core.Component "Link to this definition")
:   Bases: [`ComponentFromConfig`](#autogen_core.ComponentFromConfig "autogen_core._component_config.ComponentFromConfig")[`ConfigT`], [`ComponentSchemaType`](#autogen_core.ComponentSchemaType "autogen_core._component_config.ComponentSchemaType")[`ConfigT`], [`Generic`](https://docs.python.org/3/library/typing.html#typing.Generic "(in Python v3.13)")[`ConfigT`]

    To create a component class, inherit from this class for the concrete class and ComponentBase on the interface. Then implement two class variables:

    * `component_config_schema` - A Pydantic model class which represents the configuration of the component. This is also the type parameter of Component.
    * `component_type` - What is the logical type of the component.

    Example:

    ```
    from __future__ import annotations

    from pydantic import BaseModel
    from autogen_core import Component

    class Config(BaseModel):
        value: str

    class MyComponent(Component[Config]):
        component_type = "custom"
        component_config_schema = Config

        def __init__(self, value: str):
            self.value = value

        def _to_config(self) -> Config:
            return Config(value=self.value)

        @classmethod
        def _from_config(cls, config: Config) -> MyComponent:
            return cls(value=config.value)

    ```

*class* ComponentBase[#](#autogen_core.ComponentBase "Link to this definition")
:   Bases: [`ComponentToConfig`](#autogen_core.ComponentToConfig "autogen_core._component_config.ComponentToConfig")[`ConfigT`], [`ComponentLoader`](#autogen_core.ComponentLoader "autogen_core._component_config.ComponentLoader"), [`Generic`](https://docs.python.org/3/library/typing.html#typing.Generic "(in Python v3.13)")[`ConfigT`]

*class* ComponentFromConfig[#](#autogen_core.ComponentFromConfig "Link to this definition")
:   Bases: [`Generic`](https://docs.python.org/3/library/typing.html#typing.Generic "(in Python v3.13)")[`FromConfigT`]

    *classmethod* \_from\_config(*config: FromConfigT*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_core.ComponentFromConfig._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    *classmethod* \_from\_config\_past\_version(*config: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*, *version: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_core.ComponentFromConfig._from_config_past_version "Link to this definition")
    :   Create a new instance of the component from a previous version of the configuration object.

        This is only called when the version of the configuration object is less than the current version, since in this case the schema is not known.

        Parameters:
        :   * **config** (*Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]*) – The configuration object.
            * **version** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The version of the configuration object.

        Returns:
        :   **Self** – The new instance of the component.

*class* ComponentLoader[#](#autogen_core.ComponentLoader "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    *classmethod* load\_component(*model: [ComponentModel](#autogen_core.ComponentModel "autogen_core._component_config.ComponentModel") | [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*, *expected: [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_core.ComponentLoader.load_component "Link to this definition")

    *classmethod* load\_component(*model: [ComponentModel](#autogen_core.ComponentModel "autogen_core._component_config.ComponentModel") | [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*, *expected: [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[ExpectedType]*) → ExpectedType
    :   Load a component from a model. Intended to be used with the return type of `autogen_core.ComponentConfig.dump_component()`.

        Example

        ```
        from autogen_core import ComponentModel
        from autogen_core.models import ChatCompletionClient

        component: ComponentModel = ...  # type: ignore

        model_client = ChatCompletionClient.load_component(component)

        ```

        Parameters:
        :   * **model** ([*ComponentModel*](#autogen_core.ComponentModel "autogen_core.ComponentModel")) – The model to load the component from.
            * **model** – \_description\_
            * **expected** (*Type**[**ExpectedType**]* *|* *None**,* *optional*) – Explicit type only if used directly on ComponentLoader. Defaults to None.

        Returns:
        :   **Self** – The loaded component.

        Raises:
        :   * [**ValueError**](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.13)") – If the provider string is invalid.
            * [**TypeError**](https://docs.python.org/3/library/exceptions.html#TypeError "(in Python v3.13)") – Provider is not a subclass of ComponentConfigImpl, or the expected type does not match.

        Returns:
        :   **Self | ExpectedType** – The loaded component.

*pydantic model* ComponentModel[#](#autogen_core.ComponentModel "Link to this definition")
:   Bases: `BaseModel`

    Model class for a component. Contains all information required to instantiate a component.

    Show JSON schema

    ```
    {
       "title": "ComponentModel",
       "description": "Model class for a component. Contains all information required to instantiate a component.",
       "type": "object",
       "properties": {
          "provider": {
             "title": "Provider",
             "type": "string"
          },
          "component_type": {
             "anyOf": [
                {
                   "enum": [
                      "model",
                      "agent",
                      "tool",
                      "termination",
                      "token_provider"
                   ],
                   "type": "string"
                },
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Component Type"
          },
          "version": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Version"
          },
          "component_version": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Component Version"
          },
          "description": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Description"
          },
          "label": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Label"
          },
          "config": {
             "title": "Config",
             "type": "object"
          }
       },
       "required": [
          "provider",
          "config"
       ]
    }

    ```

    Fields:
    :   * `component_type (Literal['model', 'agent', 'tool', 'termination', 'token_provider'] | str | None)`
        * `component_version (int | None)`
        * `config (dict[str, Any])`
        * `description (str | None)`
        * `label (str | None)`
        * `provider (str)`
        * `version (int | None)`

    *field* provider*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_core.ComponentModel.provider "Link to this definition")
    :   Describes how the component can be instantiated.

    *field* component\_type*: ComponentType | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_core.ComponentModel.component_type "Link to this definition")
    :   Logical type of the component. If missing, the component assumes the default type of the provider.

    *field* version*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_core.ComponentModel.version "Link to this definition")
    :   Version of the component specification. If missing, the component assumes whatever is the current version of the library used to load it. This is obviously dangerous and should be used for user authored ephmeral config. For all other configs version should be specified.

    *field* component\_version*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_core.ComponentModel.component_version "Link to this definition")
    :   Version of the component. If missing, the component assumes the default version of the provider.

    *field* description*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_core.ComponentModel.description "Link to this definition")
    :   Description of the component.

    *field* label*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_core.ComponentModel.label "Link to this definition")
    :   Human readable label for the component. If missing the component assumes the class name of the provider.

    *field* config*: [dict](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), Any]* *[Required]*[#](#autogen_core.ComponentModel.config "Link to this definition")
    :   The schema validated config field is passed to a given class’s implmentation of `autogen_core.ComponentConfigImpl._from_config()` to create a new instance of the component class.

*class* ComponentSchemaType[#](#autogen_core.ComponentSchemaType "Link to this definition")
:   Bases: [`Generic`](https://docs.python.org/3/library/typing.html#typing.Generic "(in Python v3.13)")[`ConfigT`]

    component\_config\_schema*: [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[ConfigT]*[#](#autogen_core.ComponentSchemaType.component_config_schema "Link to this definition")
    :   The Pydantic model class which represents the configuration of the component.

    required\_class\_vars *= ['component\_config\_schema', 'component\_type']*[#](#autogen_core.ComponentSchemaType.required_class_vars "Link to this definition")

*class* ComponentToConfig[#](#autogen_core.ComponentToConfig "Link to this definition")
:   Bases: [`Generic`](https://docs.python.org/3/library/typing.html#typing.Generic "(in Python v3.13)")[`ToConfigT`]

    The two methods a class must implement to be a component.

    Parameters:
    :   **Protocol** (*ConfigT*) – Type which derives from `pydantic.BaseModel`.

    component\_type*: [ClassVar](https://docs.python.org/3/library/typing.html#typing.ClassVar "(in Python v3.13)")[[Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['model', 'agent', 'tool', 'termination', 'token\_provider'] | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*[#](#autogen_core.ComponentToConfig.component_type "Link to this definition")
    :   The logical type of the component.

    component\_version*: [ClassVar](https://docs.python.org/3/library/typing.html#typing.ClassVar "(in Python v3.13)")[[int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")]* *= 1*[#](#autogen_core.ComponentToConfig.component_version "Link to this definition")
    :   The version of the component, if schema incompatibilities are introduced this should be updated.

    component\_provider\_override*: [ClassVar](https://docs.python.org/3/library/typing.html#typing.ClassVar "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= None*[#](#autogen_core.ComponentToConfig.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    component\_description*: [ClassVar](https://docs.python.org/3/library/typing.html#typing.ClassVar "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= None*[#](#autogen_core.ComponentToConfig.component_description "Link to this definition")
    :   A description of the component. If not provided, the docstring of the class will be used.

    component\_label*: [ClassVar](https://docs.python.org/3/library/typing.html#typing.ClassVar "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= None*[#](#autogen_core.ComponentToConfig.component_label "Link to this definition")
    :   A human readable label for the component. If not provided, the component class name will be used.

    \_to\_config() → ToConfigT[#](#autogen_core.ComponentToConfig._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    dump\_component() → [ComponentModel](#autogen_core.ComponentModel "autogen_core._component_config.ComponentModel")[#](#autogen_core.ComponentToConfig.dump_component "Link to this definition")
    :   Dump the component to a model that can be loaded back in.

        Raises:
        :   [**TypeError**](https://docs.python.org/3/library/exceptions.html#TypeError "(in Python v3.13)") – If the component is a local class.

        Returns:
        :   **ComponentModel** – The model representing the component.

is\_component\_class(*cls: [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")*) → [TypeGuard](https://docs.python.org/3/library/typing.html#typing.TypeGuard "(in Python v3.13)")[[Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[\_ConcreteComponent[BaseModel]]][#](#autogen_core.is_component_class "Link to this definition")

is\_component\_instance(*cls: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*) → [TypeGuard](https://docs.python.org/3/library/typing.html#typing.TypeGuard "(in Python v3.13)")[\_ConcreteComponent[BaseModel]][#](#autogen_core.is_component_instance "Link to this definition")

*final class* DropMessage[#](#autogen_core.DropMessage "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    Marker type for signalling that a message should be dropped by an intervention handler. The type itself should be returned from the handler.

*class* InterventionHandler(*\*args*, *\*\*kwargs*)[#](#autogen_core.InterventionHandler "Link to this definition")
:   Bases: [`Protocol`](https://docs.python.org/3/library/typing.html#typing.Protocol "(in Python v3.13)")

    An intervention handler is a class that can be used to modify, log or drop messages that are being processed by the `autogen_core.base.AgentRuntime`.

    The handler is called when the message is submitted to the runtime.

    Currently the only runtime which supports this is the `autogen_core.base.SingleThreadedAgentRuntime`.

    Note: Returning None from any of the intervention handler methods will result in a warning being issued and treated as “no change”. If you intend to drop a message, you should return [`DropMessage`](#autogen_core.DropMessage "autogen_core.DropMessage") explicitly.

    Example:

    ```
    from autogen_core import DefaultInterventionHandler, MessageContext, AgentId, SingleThreadedAgentRuntime
    from dataclasses import dataclass
    from typing import Any

    @dataclass
    class MyMessage:
        content: str

    class MyInterventionHandler(DefaultInterventionHandler):
        async def on_send(self, message: Any, *, message_context: MessageContext, recipient: AgentId) -> MyMessage:
            if isinstance(message, MyMessage):
                message.content = message.content.upper()
            return message

    runtime = SingleThreadedAgentRuntime(intervention_handlers=[MyInterventionHandler()])

    ```

    *async* on\_send(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *\**, *message\_context: [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")*, *recipient: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[DropMessage](#autogen_core.DropMessage "autogen_core._intervention.DropMessage")][#](#autogen_core.InterventionHandler.on_send "Link to this definition")
    :   Called when a message is submitted to the AgentRuntime using `autogen_core.base.AgentRuntime.send_message()`.

    *async* on\_publish(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *\**, *message\_context: [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[DropMessage](#autogen_core.DropMessage "autogen_core._intervention.DropMessage")][#](#autogen_core.InterventionHandler.on_publish "Link to this definition")
    :   Called when a message is published to the AgentRuntime using `autogen_core.base.AgentRuntime.publish_message()`.

    *async* on\_response(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *\**, *sender: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *recipient: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[DropMessage](#autogen_core.DropMessage "autogen_core._intervention.DropMessage")][#](#autogen_core.InterventionHandler.on_response "Link to this definition")
    :   Called when a response is received by the AgentRuntime from an Agent’s message handler returning a value.

*class* DefaultInterventionHandler(*\*args*, *\*\*kwargs*)[#](#autogen_core.DefaultInterventionHandler "Link to this definition")
:   Bases: [`InterventionHandler`](#autogen_core.InterventionHandler "autogen_core._intervention.InterventionHandler")

    Simple class that provides a default implementation for all intervention
    handler methods, that simply returns the message unchanged. Allows for easy
    subclassing to override only the desired methods.

    *async* on\_send(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *\**, *message\_context: [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")*, *recipient: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[DropMessage](#autogen_core.DropMessage "autogen_core._intervention.DropMessage")][#](#autogen_core.DefaultInterventionHandler.on_send "Link to this definition")
    :   Called when a message is submitted to the AgentRuntime using `autogen_core.base.AgentRuntime.send_message()`.

    *async* on\_publish(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *\**, *message\_context: [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[DropMessage](#autogen_core.DropMessage "autogen_core._intervention.DropMessage")][#](#autogen_core.DefaultInterventionHandler.on_publish "Link to this definition")
    :   Called when a message is published to the AgentRuntime using `autogen_core.base.AgentRuntime.publish_message()`.

    *async* on\_response(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *\**, *sender: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *recipient: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[DropMessage](#autogen_core.DropMessage "autogen_core._intervention.DropMessage")][#](#autogen_core.DefaultInterventionHandler.on_response "Link to this definition")
    :   Called when a response is received by the AgentRuntime from an Agent’s message handler returning a value.

ComponentType[#](#autogen_core.ComponentType "Link to this definition")
:   alias of [`Literal`](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")[‘model’, ‘agent’, ‘tool’, ‘termination’, ‘token\_provider’] | [`str`](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")

### autogen\_core.code\_executor[#](#module-autogen_core.code_executor "Link to this heading")

*class* Alias(*name: 'str'*, *alias: 'str'*)[#](#autogen_core.code_executor.Alias "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    alias*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.code_executor.Alias.alias "Link to this definition")

    name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.code_executor.Alias.name "Link to this definition")

*class* CodeBlock(*code: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *language: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_core.code_executor.CodeBlock "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    A code block extracted fromm an agent message.

    code*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.code_executor.CodeBlock.code "Link to this definition")

    language*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.code_executor.CodeBlock.language "Link to this definition")

*class* CodeExecutor[#](#autogen_core.code_executor.CodeExecutor "Link to this definition")
:   Bases: [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC "(in Python v3.13)"), [`ComponentBase`](#autogen_core.ComponentBase "autogen_core._component_config.ComponentBase")[`BaseModel`]

    Executes code blocks and returns the result.

    This is an abstract base class for code executors. It defines the interface
    for executing code blocks and returning the result. A concrete implementation
    of this class should be provided to execute code blocks in a specific
    environment. For example, [`DockerCommandLineCodeExecutor`](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor "autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor") executes
    code blocks in a command line environment in a Docker container.

    It is recommended for subclass to be used as a context manager to ensure
    that resources are cleaned up properly. To do this, implement the
    [`start()`](#autogen_core.code_executor.CodeExecutor.start "autogen_core.code_executor.CodeExecutor.start") and
    [`stop()`](#autogen_core.code_executor.CodeExecutor.stop "autogen_core.code_executor.CodeExecutor.stop") methods
    that will be called when entering and exiting the context manager.

    component\_type*: ClassVar[ComponentType]* *= 'code\_executor'*[#](#autogen_core.code_executor.CodeExecutor.component_type "Link to this definition")
    :   The logical type of the component.

    *abstract async* execute\_code\_blocks(*code\_blocks: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[CodeBlock](#autogen_core.code_executor.CodeBlock "autogen_core.code_executor._base.CodeBlock")]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [CodeResult](#autogen_core.code_executor.CodeResult "autogen_core.code_executor._base.CodeResult")[#](#autogen_core.code_executor.CodeExecutor.execute_code_blocks "Link to this definition")
    :   Execute code blocks and return the result.

        This method should be implemented by the code executor.

        Parameters:
        :   **code\_blocks** (*List**[*[*CodeBlock*](#autogen_core.code_executor.CodeBlock "autogen_core.code_executor.CodeBlock")*]*) – The code blocks to execute.

        Returns:
        :   **CodeResult** – The result of the code execution.

        Raises:
        :   * [**ValueError**](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.13)") – Errors in user inputs
            * [**TimeoutError**](https://docs.python.org/3/library/asyncio-exceptions.html#asyncio.TimeoutError "(in Python v3.13)") – Code execution timeouts
            * [**CancelledError**](https://docs.python.org/3/library/asyncio-exceptions.html#asyncio.CancelledError "(in Python v3.13)") – CancellationToken evoked during execution

    *abstract async* restart() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.code_executor.CodeExecutor.restart "Link to this definition")
    :   Restart the code executor.

        This method should be implemented by the code executor.

        This method is called when the agent is reset.

    *abstract async* start() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.code_executor.CodeExecutor.start "Link to this definition")
    :   Start the code executor.

    *abstract async* stop() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.code_executor.CodeExecutor.stop "Link to this definition")
    :   Stop the code executor and release any resources.

*class* CodeResult(*exit\_code: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*, *output: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_core.code_executor.CodeResult "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    Result of a code execution.

    exit\_code*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*[#](#autogen_core.code_executor.CodeResult.exit_code "Link to this definition")

    output*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.code_executor.CodeResult.output "Link to this definition")

*class* FunctionWithRequirements(*func: 'Callable[P*, *T]'*, *python\_packages: 'Sequence[str]' = <factory>*, *global\_imports: 'Sequence[Import]' = <factory>*)[#](#autogen_core.code_executor.FunctionWithRequirements "Link to this definition")
:   Bases: [`Generic`](https://docs.python.org/3/library/typing.html#typing.Generic "(in Python v3.13)")[`T`, `P`]

    *classmethod* from\_callable(*func: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[P], T]*, *python\_packages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] = []*, *global\_imports: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [ImportFromModule](#autogen_core.code_executor.ImportFromModule "autogen_core.code_executor._func_with_reqs.ImportFromModule") | [Alias](#autogen_core.code_executor.Alias "autogen_core.code_executor._func_with_reqs.Alias")] = []*) → [FunctionWithRequirements](#autogen_core.code_executor.FunctionWithRequirements "autogen_core.code_executor._func_with_reqs.FunctionWithRequirements")[T, P][#](#autogen_core.code_executor.FunctionWithRequirements.from_callable "Link to this definition")

    *static* from\_str(*func: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *python\_packages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] = []*, *global\_imports: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [ImportFromModule](#autogen_core.code_executor.ImportFromModule "autogen_core.code_executor._func_with_reqs.ImportFromModule") | [Alias](#autogen_core.code_executor.Alias "autogen_core.code_executor._func_with_reqs.Alias")] = []*) → [FunctionWithRequirementsStr](#autogen_core.code_executor.FunctionWithRequirementsStr "autogen_core.code_executor._func_with_reqs.FunctionWithRequirementsStr")[#](#autogen_core.code_executor.FunctionWithRequirements.from_str "Link to this definition")

    func*: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[P], T]*[#](#autogen_core.code_executor.FunctionWithRequirements.func "Link to this definition")

    global\_imports*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [ImportFromModule](#autogen_core.code_executor.ImportFromModule "autogen_core.code_executor._func_with_reqs.ImportFromModule") | [Alias](#autogen_core.code_executor.Alias "autogen_core.code_executor._func_with_reqs.Alias")]*[#](#autogen_core.code_executor.FunctionWithRequirements.global_imports "Link to this definition")

    python\_packages*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*[#](#autogen_core.code_executor.FunctionWithRequirements.python_packages "Link to this definition")

*class* FunctionWithRequirementsStr(*func: 'str'*, *python\_packages: 'Sequence[str]' = []*, *global\_imports: 'Sequence[Import]' = []*)[#](#autogen_core.code_executor.FunctionWithRequirementsStr "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    compiled\_func*: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[...], [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*[#](#autogen_core.code_executor.FunctionWithRequirementsStr.compiled_func "Link to this definition")

    func*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.code_executor.FunctionWithRequirementsStr.func "Link to this definition")

    global\_imports*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [ImportFromModule](#autogen_core.code_executor.ImportFromModule "autogen_core.code_executor._func_with_reqs.ImportFromModule") | [Alias](#autogen_core.code_executor.Alias "autogen_core.code_executor._func_with_reqs.Alias")]*[#](#autogen_core.code_executor.FunctionWithRequirementsStr.global_imports "Link to this definition")

    python\_packages*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*[#](#autogen_core.code_executor.FunctionWithRequirementsStr.python_packages "Link to this definition")

*class* ImportFromModule(*module: 'str'*, *imports: 'Union[Tuple[Union[str, Alias], ...], List[Union[str, Alias]]]'*)[#](#autogen_core.code_executor.ImportFromModule "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    imports*: [Tuple](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Alias](#autogen_core.code_executor.Alias "autogen_core.code_executor._func_with_reqs.Alias"), ...]*[#](#autogen_core.code_executor.ImportFromModule.imports "Link to this definition")

    module*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.code_executor.ImportFromModule.module "Link to this definition")

with\_requirements(*python\_packages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] = []*, *global\_imports: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [ImportFromModule](#autogen_core.code_executor.ImportFromModule "autogen_core.code_executor._func_with_reqs.ImportFromModule") | [Alias](#autogen_core.code_executor.Alias "autogen_core.code_executor._func_with_reqs.Alias")] = []*) → [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[P], T]], [FunctionWithRequirements](#autogen_core.code_executor.FunctionWithRequirements "autogen_core.code_executor._func_with_reqs.FunctionWithRequirements")[T, P]][#](#autogen_core.code_executor.with_requirements "Link to this definition")
:   Decorate a function with package and import requirements for code execution environments.

    This decorator makes a function available for reference in dynamically executed code blocks
    by wrapping it in a FunctionWithRequirements object that tracks its dependencies. When the
    decorated function is passed to a code executor, it can be imported by name in the executed
    code, with all dependencies automatically handled.

    Parameters:
    :   * **python\_packages** (*Sequence**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]**,* *optional*) – Python packages required by the function.
          Can include version specifications (e.g., [“pandas>=1.0.0”]). Defaults to [].
        * **global\_imports** (*Sequence**[**Import**]**,* *optional*) – Import statements required by the function.
          Can be strings (“numpy”), ImportFromModule objects, or Alias objects. Defaults to [].

    Returns:
    :   **Callable[[Callable[P, T]], FunctionWithRequirements[T, P]]** – A decorator that wraps
        the target function, preserving its functionality while registering its dependencies.

    Example

    ```
    import tempfile
    import asyncio
    from autogen_core import CancellationToken
    from autogen_core.code_executor import with_requirements, CodeBlock
    from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
    import pandas

    @with_requirements(python_packages=["pandas"], global_imports=["pandas"])
    def load_data() -> pandas.DataFrame:
        """Load some sample data.

        Returns:
            pandas.DataFrame: A DataFrame with sample data
        """
        data = {
            "name": ["John", "Anna", "Peter", "Linda"],
            "location": ["New York", "Paris", "Berlin", "London"],
            "age": [24, 13, 53, 33],
        }
        return pandas.DataFrame(data)

    async def run_example():
        # The decorated function can be used in executed code
        with tempfile.TemporaryDirectory() as temp_dir:
            executor = LocalCommandLineCodeExecutor(work_dir=temp_dir, functions=[load_data])
            code = f"""from {executor.functions_module} import load_data

            # Use the imported function
            data = load_data()
            print(data['name'][0])"""

            result = await executor.execute_code_blocks(
                code_blocks=[CodeBlock(language="python", code=code)],
                cancellation_token=CancellationToken(),
            )
            print(result.output)  # Output: John

    # Run the async example
    asyncio.run(run_example())

    ```

### autogen\_core.models[#](#module-autogen_core.models "Link to this heading")

*pydantic model* AssistantMessage[#](#autogen_core.models.AssistantMessage "Link to this definition")
:   Bases: `BaseModel`

    Assistant message are sampled from the language model.

    Show JSON schema

    ```
    {
       "title": "AssistantMessage",
       "description": "Assistant message are sampled from the language model.",
       "type": "object",
       "properties": {
          "content": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "items": {
                      "$ref": "#/$defs/FunctionCall"
                   },
                   "type": "array"
                }
             ],
             "title": "Content"
          },
          "thought": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Thought"
          },
          "source": {
             "title": "Source",
             "type": "string"
          },
          "type": {
             "const": "AssistantMessage",
             "default": "AssistantMessage",
             "title": "Type",
             "type": "string"
          }
       },
       "$defs": {
          "FunctionCall": {
             "properties": {
                "id": {
                   "title": "Id",
                   "type": "string"
                },
                "arguments": {
                   "title": "Arguments",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                }
             },
             "required": [
                "id",
                "arguments",
                "name"
             ],
             "title": "FunctionCall",
             "type": "object"
          }
       },
       "required": [
          "content",
          "source"
       ]
    }

    ```

    Fields:
    :   * `content (str | List[autogen_core._types.FunctionCall])`
        * `source (str)`
        * `thought (str | None)`
        * `type (Literal['AssistantMessage'])`

    *field* content*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[FunctionCall](#autogen_core.FunctionCall "autogen_core._types.FunctionCall")]* *[Required]*[#](#autogen_core.models.AssistantMessage.content "Link to this definition")
    :   The content of the message.

    *field* source*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_core.models.AssistantMessage.source "Link to this definition")
    :   The name of the agent that sent this message.

    *field* thought*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_core.models.AssistantMessage.thought "Link to this definition")
    :   The reasoning text for the completion if available. Used for reasoning model and additional text content besides function calls.

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['AssistantMessage']* *= 'AssistantMessage'*[#](#autogen_core.models.AssistantMessage.type "Link to this definition")

*class* ChatCompletionClient[#](#autogen_core.models.ChatCompletionClient "Link to this definition")
:   Bases: [`ComponentBase`](#autogen_core.ComponentBase "autogen_core._component_config.ComponentBase")[`BaseModel`], [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC "(in Python v3.13)")

    *abstract* actual\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_core.models.ChatCompletionClient.actual_usage "Link to this definition")

    *abstract property* capabilities*: [ModelCapabilities](#autogen_core.models.ModelCapabilities "autogen_core.models._model_client.ModelCapabilities")*[#](#autogen_core.models.ChatCompletionClient.capabilities "Link to this definition")

    *abstract async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.models.ChatCompletionClient.close "Link to this definition")

    *abstract* count\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_core.models.ChatCompletionClient.count_tokens "Link to this definition")

    *abstract async* create(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult")[#](#autogen_core.models.ChatCompletionClient.create "Link to this definition")
    :   Creates a single response from the model.

        Parameters:
        :   * **messages** (*Sequence**[**LLMMessage**]*) – The messages to send to the model.
            * **tools** (*Sequence**[*[*Tool*](#autogen_core.tools.Tool "autogen_core.tools.Tool") *|* [*ToolSchema*](#autogen_core.tools.ToolSchema "autogen_core.tools.ToolSchema")*]**,* *optional*) – The tools to use with the model. Defaults to [].
            * **json\_output** (*Optional**[*[*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") *|* [*type*](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")*[**BaseModel**]**]**,* *optional*) – Whether to use JSON mode, structured output, or neither.
              Defaults to None. If set to a [Pydantic BaseModel](https://docs.pydantic.dev/latest/usage/models/#model) type,
              it will be used as the output type for structured output.
              If set to a boolean, it will be used to determine whether to use JSON mode or not.
              If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
            * **extra\_create\_args** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]**,* *optional*) – Extra arguments to pass to the underlying client. Defaults to {}.
            * **cancellation\_token** (*Optional**[*[*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken")*]**,* *optional*) – A token for cancellation. Defaults to None.

        Returns:
        :   **CreateResult** – The result of the model call.

    *abstract* create\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_core.models.ChatCompletionClient.create_stream "Link to this definition")
    :   Creates a stream of string chunks from the model ending with a CreateResult.

        Parameters:
        :   * **messages** (*Sequence**[**LLMMessage**]*) – The messages to send to the model.
            * **tools** (*Sequence**[*[*Tool*](#autogen_core.tools.Tool "autogen_core.tools.Tool") *|* [*ToolSchema*](#autogen_core.tools.ToolSchema "autogen_core.tools.ToolSchema")*]**,* *optional*) – The tools to use with the model. Defaults to [].
            * **json\_output** (*Optional**[*[*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") *|* [*type*](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")*[**BaseModel**]**]**,* *optional*) –

              Whether to use JSON mode, structured output, or neither.
              Defaults to None. If set to a [Pydantic BaseModel](https://docs.pydantic.dev/latest/usage/models/#model) type,
              it will be used as the output type for structured output.
              If set to a boolean, it will be used to determine whether to use JSON mode or not.
              If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
            * **extra\_create\_args** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]**,* *optional*) – Extra arguments to pass to the underlying client. Defaults to {}.
            * **cancellation\_token** (*Optional**[*[*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken")*]**,* *optional*) – A token for cancellation. Defaults to None.

        Returns:
        :   **AsyncGenerator[Union[str, CreateResult], None]** – A generator that yields string chunks and ends with a [`CreateResult`](#autogen_core.models.CreateResult "autogen_core.models.CreateResult").

    *abstract property* model\_info*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*[#](#autogen_core.models.ChatCompletionClient.model_info "Link to this definition")

    *abstract* remaining\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_core.models.ChatCompletionClient.remaining_tokens "Link to this definition")

    *abstract* total\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_core.models.ChatCompletionClient.total_usage "Link to this definition")

*pydantic model* ChatCompletionTokenLogprob[#](#autogen_core.models.ChatCompletionTokenLogprob "Link to this definition")
:   Bases: `BaseModel`

    Show JSON schema

    ```
    {
       "title": "ChatCompletionTokenLogprob",
       "type": "object",
       "properties": {
          "token": {
             "title": "Token",
             "type": "string"
          },
          "logprob": {
             "title": "Logprob",
             "type": "number"
          },
          "top_logprobs": {
             "anyOf": [
                {
                   "items": {
                      "$ref": "#/$defs/TopLogprob"
                   },
                   "type": "array"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Top Logprobs"
          },
          "bytes": {
             "anyOf": [
                {
                   "items": {
                      "type": "integer"
                   },
                   "type": "array"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Bytes"
          }
       },
       "$defs": {
          "TopLogprob": {
             "properties": {
                "logprob": {
                   "title": "Logprob",
                   "type": "number"
                },
                "bytes": {
                   "anyOf": [
                      {
                         "items": {
                            "type": "integer"
                         },
                         "type": "array"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Bytes"
                }
             },
             "required": [
                "logprob"
             ],
             "title": "TopLogprob",
             "type": "object"
          }
       },
       "required": [
          "token",
          "logprob"
       ]
    }

    ```

    Fields:
    :   * `bytes (List[int] | None)`
        * `logprob (float)`
        * `token (str)`
        * `top_logprobs (List[autogen_core.models._types.TopLogprob] | None)`

    *field* bytes*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_core.models.ChatCompletionTokenLogprob.bytes "Link to this definition")

    *field* logprob*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")* *[Required]*[#](#autogen_core.models.ChatCompletionTokenLogprob.logprob "Link to this definition")

    *field* token*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_core.models.ChatCompletionTokenLogprob.token "Link to this definition")

    *field* top\_logprobs*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[TopLogprob](#autogen_core.models.TopLogprob "autogen_core.models._types.TopLogprob")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_core.models.ChatCompletionTokenLogprob.top_logprobs "Link to this definition")

*pydantic model* CreateResult[#](#autogen_core.models.CreateResult "Link to this definition")
:   Bases: `BaseModel`

    Create result contains the output of a model completion.

    Show JSON schema

    ```
    {
       "title": "CreateResult",
       "description": "Create result contains the output of a model completion.",
       "type": "object",
       "properties": {
          "finish_reason": {
             "enum": [
                "stop",
                "length",
                "function_calls",
                "content_filter",
                "unknown"
             ],
             "title": "Finish Reason",
             "type": "string"
          },
          "content": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "items": {
                      "$ref": "#/$defs/FunctionCall"
                   },
                   "type": "array"
                }
             ],
             "title": "Content"
          },
          "usage": {
             "$ref": "#/$defs/RequestUsage"
          },
          "cached": {
             "title": "Cached",
             "type": "boolean"
          },
          "logprobs": {
             "anyOf": [
                {
                   "items": {
                      "$ref": "#/$defs/ChatCompletionTokenLogprob"
                   },
                   "type": "array"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Logprobs"
          },
          "thought": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Thought"
          }
       },
       "$defs": {
          "ChatCompletionTokenLogprob": {
             "properties": {
                "token": {
                   "title": "Token",
                   "type": "string"
                },
                "logprob": {
                   "title": "Logprob",
                   "type": "number"
                },
                "top_logprobs": {
                   "anyOf": [
                      {
                         "items": {
                            "$ref": "#/$defs/TopLogprob"
                         },
                         "type": "array"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Top Logprobs"
                },
                "bytes": {
                   "anyOf": [
                      {
                         "items": {
                            "type": "integer"
                         },
                         "type": "array"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Bytes"
                }
             },
             "required": [
                "token",
                "logprob"
             ],
             "title": "ChatCompletionTokenLogprob",
             "type": "object"
          },
          "FunctionCall": {
             "properties": {
                "id": {
                   "title": "Id",
                   "type": "string"
                },
                "arguments": {
                   "title": "Arguments",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                }
             },
             "required": [
                "id",
                "arguments",
                "name"
             ],
             "title": "FunctionCall",
             "type": "object"
          },
          "RequestUsage": {
             "properties": {
                "prompt_tokens": {
                   "title": "Prompt Tokens",
                   "type": "integer"
                },
                "completion_tokens": {
                   "title": "Completion Tokens",
                   "type": "integer"
                }
             },
             "required": [
                "prompt_tokens",
                "completion_tokens"
             ],
             "title": "RequestUsage",
             "type": "object"
          },
          "TopLogprob": {
             "properties": {
                "logprob": {
                   "title": "Logprob",
                   "type": "number"
                },
                "bytes": {
                   "anyOf": [
                      {
                         "items": {
                            "type": "integer"
                         },
                         "type": "array"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Bytes"
                }
             },
             "required": [
                "logprob"
             ],
             "title": "TopLogprob",
             "type": "object"
          }
       },
       "required": [
          "finish_reason",
          "content",
          "usage",
          "cached"
       ]
    }

    ```

    Fields:
    :   * `cached (bool)`
        * `content (str | List[autogen_core._types.FunctionCall])`
        * `finish_reason (Literal['stop', 'length', 'function_calls', 'content_filter', 'unknown'])`
        * `logprobs (List[autogen_core.models._types.ChatCompletionTokenLogprob] | None)`
        * `thought (str | None)`
        * `usage (autogen_core.models._types.RequestUsage)`

    *field* cached*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")* *[Required]*[#](#autogen_core.models.CreateResult.cached "Link to this definition")
    :   Whether the completion was generated from a cached response.

    *field* content*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[FunctionCall](#autogen_core.FunctionCall "autogen_core._types.FunctionCall")]* *[Required]*[#](#autogen_core.models.CreateResult.content "Link to this definition")
    :   The output of the model completion.

    *field* finish\_reason*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['stop', 'length', 'function\_calls', 'content\_filter', 'unknown']* *[Required]*[#](#autogen_core.models.CreateResult.finish_reason "Link to this definition")
    :   The reason the model finished generating the completion.

    *field* logprobs*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[ChatCompletionTokenLogprob](#autogen_core.models.ChatCompletionTokenLogprob "autogen_core.models._types.ChatCompletionTokenLogprob")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_core.models.CreateResult.logprobs "Link to this definition")
    :   The logprobs of the tokens in the completion.

    *field* thought*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_core.models.CreateResult.thought "Link to this definition")
    :   The reasoning text for the completion if available. Used for reasoning models
        and additional text content besides function calls.

    *field* usage*: [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")* *[Required]*[#](#autogen_core.models.CreateResult.usage "Link to this definition")
    :   The usage of tokens in the prompt and completion.

*pydantic model* FunctionExecutionResult[#](#autogen_core.models.FunctionExecutionResult "Link to this definition")
:   Bases: `BaseModel`

    Function execution result contains the output of a function call.

    Show JSON schema

    ```
    {
       "title": "FunctionExecutionResult",
       "description": "Function execution result contains the output of a function call.",
       "type": "object",
       "properties": {
          "content": {
             "title": "Content",
             "type": "string"
          },
          "name": {
             "title": "Name",
             "type": "string"
          },
          "call_id": {
             "title": "Call Id",
             "type": "string"
          },
          "is_error": {
             "anyOf": [
                {
                   "type": "boolean"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Is Error"
          }
       },
       "required": [
          "content",
          "name",
          "call_id"
       ]
    }

    ```

    Fields:
    :   * `call_id (str)`
        * `content (str)`
        * `is_error (bool | None)`
        * `name (str)`

    *field* call\_id*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_core.models.FunctionExecutionResult.call_id "Link to this definition")
    :   The ID of the function call. Note this ID may be empty for some models.

    *field* content*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_core.models.FunctionExecutionResult.content "Link to this definition")
    :   The output of the function call.

    *field* is\_error*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_core.models.FunctionExecutionResult.is_error "Link to this definition")
    :   Whether the function call resulted in an error.

    *field* name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_core.models.FunctionExecutionResult.name "Link to this definition")
    :   (New in v0.4.8) The name of the function that was called.

*pydantic model* FunctionExecutionResultMessage[#](#autogen_core.models.FunctionExecutionResultMessage "Link to this definition")
:   Bases: `BaseModel`

    Function execution result message contains the output of multiple function calls.

    Show JSON schema

    ```
    {
       "title": "FunctionExecutionResultMessage",
       "description": "Function execution result message contains the output of multiple function calls.",
       "type": "object",
       "properties": {
          "content": {
             "items": {
                "$ref": "#/$defs/FunctionExecutionResult"
             },
             "title": "Content",
             "type": "array"
          },
          "type": {
             "const": "FunctionExecutionResultMessage",
             "default": "FunctionExecutionResultMessage",
             "title": "Type",
             "type": "string"
          }
       },
       "$defs": {
          "FunctionExecutionResult": {
             "description": "Function execution result contains the output of a function call.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                },
                "call_id": {
                   "title": "Call Id",
                   "type": "string"
                },
                "is_error": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Is Error"
                }
             },
             "required": [
                "content",
                "name",
                "call_id"
             ],
             "title": "FunctionExecutionResult",
             "type": "object"
          }
       },
       "required": [
          "content"
       ]
    }

    ```

    Fields:
    :   * `content (List[autogen_core.models._types.FunctionExecutionResult])`
        * `type (Literal['FunctionExecutionResultMessage'])`

    *field* content*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[FunctionExecutionResult](#autogen_core.models.FunctionExecutionResult "autogen_core.models._types.FunctionExecutionResult")]* *[Required]*[#](#autogen_core.models.FunctionExecutionResultMessage.content "Link to this definition")

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['FunctionExecutionResultMessage']* *= 'FunctionExecutionResultMessage'*[#](#autogen_core.models.FunctionExecutionResultMessage.type "Link to this definition")

*class* ModelCapabilities(*\*\*kwargs*)[#](#autogen_core.models.ModelCapabilities "Link to this definition")
:   Bases: `TypedDict`

    function\_calling*: [Required](https://docs.python.org/3/library/typing.html#typing.Required "(in Python v3.13)")[[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")]*[#](#autogen_core.models.ModelCapabilities.function_calling "Link to this definition")

    json\_output*: [Required](https://docs.python.org/3/library/typing.html#typing.Required "(in Python v3.13)")[[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")]*[#](#autogen_core.models.ModelCapabilities.json_output "Link to this definition")

    vision*: [Required](https://docs.python.org/3/library/typing.html#typing.Required "(in Python v3.13)")[[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")]*[#](#autogen_core.models.ModelCapabilities.vision "Link to this definition")

*class* ModelFamily(*\*args: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *\*\*kwargs: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*)[#](#autogen_core.models.ModelFamily "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    A model family is a group of models that share similar characteristics from a capabilities perspective. This is different to discrete supported features such as vision, function calling, and JSON output.

    This namespace class holds constants for the model families that AutoGen understands. Other families definitely exist and can be represented by a string, however, AutoGen will treat them as unknown.

    ANY[#](#autogen_core.models.ModelFamily.ANY "Link to this definition")
    :   alias of [`Literal`](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")[‘gpt-4o’, ‘o1’, ‘o3’, ‘gpt-4’, ‘gpt-35’, ‘r1’, ‘gemini-1.5-flash’, ‘gemini-1.5-pro’, ‘gemini-2.0-flash’, ‘claude-3-haiku’, ‘claude-3-sonnet’, ‘claude-3-opus’, ‘claude-3.5-haiku’, ‘claude-3.5-sonnet’, ‘unknown’]

    CLAUDE\_3\_5\_HAIKU *= 'claude-3.5-haiku'*[#](#autogen_core.models.ModelFamily.CLAUDE_3_5_HAIKU "Link to this definition")

    CLAUDE\_3\_5\_SONNET *= 'claude-3.5-sonnet'*[#](#autogen_core.models.ModelFamily.CLAUDE_3_5_SONNET "Link to this definition")

    CLAUDE\_3\_7\_SONNET *= 'claude-3.7-sonnet'*[#](#autogen_core.models.ModelFamily.CLAUDE_3_7_SONNET "Link to this definition")

    CLAUDE\_3\_HAIKU *= 'claude-3-haiku'*[#](#autogen_core.models.ModelFamily.CLAUDE_3_HAIKU "Link to this definition")

    CLAUDE\_3\_OPUS *= 'claude-3-opus'*[#](#autogen_core.models.ModelFamily.CLAUDE_3_OPUS "Link to this definition")

    CLAUDE\_3\_SONNET *= 'claude-3-sonnet'*[#](#autogen_core.models.ModelFamily.CLAUDE_3_SONNET "Link to this definition")

    GEMINI\_1\_5\_FLASH *= 'gemini-1.5-flash'*[#](#autogen_core.models.ModelFamily.GEMINI_1_5_FLASH "Link to this definition")

    GEMINI\_1\_5\_PRO *= 'gemini-1.5-pro'*[#](#autogen_core.models.ModelFamily.GEMINI_1_5_PRO "Link to this definition")

    GEMINI\_2\_0\_FLASH *= 'gemini-2.0-flash'*[#](#autogen_core.models.ModelFamily.GEMINI_2_0_FLASH "Link to this definition")

    GPT\_35 *= 'gpt-35'*[#](#autogen_core.models.ModelFamily.GPT_35 "Link to this definition")

    GPT\_4 *= 'gpt-4'*[#](#autogen_core.models.ModelFamily.GPT_4 "Link to this definition")

    GPT\_4O *= 'gpt-4o'*[#](#autogen_core.models.ModelFamily.GPT_4O "Link to this definition")

    O1 *= 'o1'*[#](#autogen_core.models.ModelFamily.O1 "Link to this definition")

    O3 *= 'o3'*[#](#autogen_core.models.ModelFamily.O3 "Link to this definition")

    R1 *= 'r1'*[#](#autogen_core.models.ModelFamily.R1 "Link to this definition")

    UNKNOWN *= 'unknown'*[#](#autogen_core.models.ModelFamily.UNKNOWN "Link to this definition")

    *static* is\_claude(*family: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")[#](#autogen_core.models.ModelFamily.is_claude "Link to this definition")

    *static* is\_gemini(*family: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")[#](#autogen_core.models.ModelFamily.is_gemini "Link to this definition")

    *static* is\_openai(*family: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")[#](#autogen_core.models.ModelFamily.is_openai "Link to this definition")

*class* ModelInfo[#](#autogen_core.models.ModelInfo "Link to this definition")
:   Bases: `TypedDict`

    ModelInfo is a dictionary that contains information about a model’s properties.
    It is expected to be used in the model\_info property of a model client.

    We are expecting this to grow over time as we add more features.

    family*: [Required](https://docs.python.org/3/library/typing.html#typing.Required "(in Python v3.13)")[[Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['gpt-4o', 'o1', 'o3', 'gpt-4', 'gpt-35', 'r1', 'gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-2.0-flash', 'claude-3-haiku', 'claude-3-sonnet', 'claude-3-opus', 'claude-3.5-haiku', 'claude-3.5-sonnet', 'unknown'] | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*[#](#autogen_core.models.ModelInfo.family "Link to this definition")
    :   Model family should be one of the constants from [`ModelFamily`](#autogen_core.models.ModelFamily "autogen_core.models.ModelFamily") or a string representing an unknown model family.

    function\_calling*: [Required](https://docs.python.org/3/library/typing.html#typing.Required "(in Python v3.13)")[[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")]*[#](#autogen_core.models.ModelInfo.function_calling "Link to this definition")
    :   True if the model supports function calling, otherwise False.

    json\_output*: [Required](https://docs.python.org/3/library/typing.html#typing.Required "(in Python v3.13)")[[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")]*[#](#autogen_core.models.ModelInfo.json_output "Link to this definition")
    :   this is different to structured json.

        Type:
        :   True if the model supports json output, otherwise False. Note

    structured\_output*: [Required](https://docs.python.org/3/library/typing.html#typing.Required "(in Python v3.13)")[[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")]*[#](#autogen_core.models.ModelInfo.structured_output "Link to this definition")
    :   True if the model supports structured output, otherwise False. This is different to json\_output.

    vision*: [Required](https://docs.python.org/3/library/typing.html#typing.Required "(in Python v3.13)")[[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")]*[#](#autogen_core.models.ModelInfo.vision "Link to this definition")
    :   True if the model supports vision, aka image input, otherwise False.

*class* RequestUsage(*prompt\_tokens: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*, *completion\_tokens: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*)[#](#autogen_core.models.RequestUsage "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    completion\_tokens*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*[#](#autogen_core.models.RequestUsage.completion_tokens "Link to this definition")

    prompt\_tokens*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*[#](#autogen_core.models.RequestUsage.prompt_tokens "Link to this definition")

*pydantic model* SystemMessage[#](#autogen_core.models.SystemMessage "Link to this definition")
:   Bases: `BaseModel`

    System message contains instructions for the model coming from the developer.

    Note

    Open AI is moving away from using ‘system’ role in favor of ‘developer’ role.
    See [Model Spec](https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions) for more details.
    However, the ‘system’ role is still allowed in their API and will be automatically converted to ‘developer’ role
    on the server side.
    So, you can use SystemMessage for developer messages.

    Show JSON schema

    ```
    {
       "title": "SystemMessage",
       "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
       "type": "object",
       "properties": {
          "content": {
             "title": "Content",
             "type": "string"
          },
          "type": {
             "const": "SystemMessage",
             "default": "SystemMessage",
             "title": "Type",
             "type": "string"
          }
       },
       "required": [
          "content"
       ]
    }

    ```

    Fields:
    :   * `content (str)`
        * `type (Literal['SystemMessage'])`

    *field* content*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_core.models.SystemMessage.content "Link to this definition")
    :   The content of the message.

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['SystemMessage']* *= 'SystemMessage'*[#](#autogen_core.models.SystemMessage.type "Link to this definition")

*class* TopLogprob(*logprob: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*, *bytes: List[[int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_core.models.TopLogprob "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    bytes*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_core.models.TopLogprob.bytes "Link to this definition")

    logprob*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*[#](#autogen_core.models.TopLogprob.logprob "Link to this definition")

*pydantic model* UserMessage[#](#autogen_core.models.UserMessage "Link to this definition")
:   Bases: `BaseModel`

    User message contains input from end users, or a catch-all for data provided to the model.

    Show JSON schema

    ```
    {
       "title": "UserMessage",
       "description": "User message contains input from end users, or a catch-all for data provided to the model.",
       "type": "object",
       "properties": {
          "content": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "items": {
                      "anyOf": [
                         {
                            "type": "string"
                         },
                         {}
                      ]
                   },
                   "type": "array"
                }
             ],
             "title": "Content"
          },
          "source": {
             "title": "Source",
             "type": "string"
          },
          "type": {
             "const": "UserMessage",
             "default": "UserMessage",
             "title": "Type",
             "type": "string"
          }
       },
       "required": [
          "content",
          "source"
       ]
    }

    ```

    Fields:
    :   * `content (str | List[str | autogen_core._image.Image])`
        * `source (str)`
        * `type (Literal['UserMessage'])`

    *field* content*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Image](#autogen_core.Image "autogen_core._image.Image")]* *[Required]*[#](#autogen_core.models.UserMessage.content "Link to this definition")
    :   The content of the message.

    *field* source*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_core.models.UserMessage.source "Link to this definition")
    :   The name of the agent that sent this message.

    *field* type*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['UserMessage']* *= 'UserMessage'*[#](#autogen_core.models.UserMessage.type "Link to this definition")

validate\_model\_info(*model\_info: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.models.validate_model_info "Link to this definition")
:   Validates the model info dictionary.

    Raises:
    :   [**ValueError**](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.13)") – If the model info dictionary is missing required fields.

### autogen\_core.model\_context[#](#module-autogen_core.model_context "Link to this heading")

*class* BufferedChatCompletionContext(*buffer\_size: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*, *initial\_messages: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_core.model_context.BufferedChatCompletionContext "Link to this definition")
:   Bases: [`ChatCompletionContext`](#autogen_core.model_context.ChatCompletionContext "autogen_core.model_context._chat_completion_context.ChatCompletionContext"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`BufferedChatCompletionContextConfig`]

    A buffered chat completion context that keeps a view of the last n messages,
    where n is the buffer size. The buffer size is set at initialization.

    Parameters:
    :   * **buffer\_size** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The size of the buffer.
        * **initial\_messages** (*List**[**LLMMessage**]* *|* *None*) – The initial messages.

    *classmethod* \_from\_config(*config: BufferedChatCompletionContextConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_core.model_context.BufferedChatCompletionContext._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → BufferedChatCompletionContextConfig[#](#autogen_core.model_context.BufferedChatCompletionContext._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_core.model_context.BufferedChatCompletionContext.component_config_schema "Link to this definition")
    :   alias of `BufferedChatCompletionContextConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_core.model\_context.BufferedChatCompletionContext'*[#](#autogen_core.model_context.BufferedChatCompletionContext.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* get\_messages() → [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]][#](#autogen_core.model_context.BufferedChatCompletionContext.get_messages "Link to this definition")
    :   Get at most buffer\_size recent messages.

*class* ChatCompletionContext(*initial\_messages: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_core.model_context.ChatCompletionContext "Link to this definition")
:   Bases: [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC "(in Python v3.13)"), [`ComponentBase`](#autogen_core.ComponentBase "autogen_core._component_config.ComponentBase")[`BaseModel`]

    An abstract base class for defining the interface of a chat completion context.
    A chat completion context lets agents store and retrieve LLM messages.
    It can be implemented with different recall strategies.

    Parameters:
    :   **initial\_messages** (*List**[**LLMMessage**]* *|* *None*) – The initial messages.

    Example

    To create a custom model context that filters out the thought field from AssistantMessage.
    This is useful for reasoning models like DeepSeek R1, which produces
    very long thought that is not needed for subsequent completions.

    ```
    from typing import List

    from autogen_core.model_context import UnboundedChatCompletionContext
    from autogen_core.models import AssistantMessage, LLMMessage

    class ReasoningModelContext(UnboundedChatCompletionContext):
        """A model context for reasoning models."""

        async def get_messages(self) -> List[LLMMessage]:
            messages = await super().get_messages()
            # Filter out thought field from AssistantMessage.
            messages_out: List[LLMMessage] = []
            for message in messages:
                if isinstance(message, AssistantMessage):
                    message.thought = None
                messages_out.append(message)
            return messages_out

    ```

    *async* add\_message(*message: [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.model_context.ChatCompletionContext.add_message "Link to this definition")
    :   Add a message to the context.

    *async* clear() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.model_context.ChatCompletionContext.clear "Link to this definition")
    :   Clear the context.

    component\_type*: ClassVar[ComponentType]* *= 'chat\_completion\_context'*[#](#autogen_core.model_context.ChatCompletionContext.component_type "Link to this definition")
    :   The logical type of the component.

    *abstract async* get\_messages() → [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]][#](#autogen_core.model_context.ChatCompletionContext.get_messages "Link to this definition")

    *async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.model_context.ChatCompletionContext.load_state "Link to this definition")

    *async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.model_context.ChatCompletionContext.save_state "Link to this definition")

*pydantic model* ChatCompletionContextState[#](#autogen_core.model_context.ChatCompletionContextState "Link to this definition")
:   Bases: `BaseModel`

    Show JSON schema

    ```
    {
       "title": "ChatCompletionContextState",
       "type": "object",
       "properties": {
          "messages": {
             "items": {
                "discriminator": {
                   "mapping": {
                      "AssistantMessage": "#/$defs/AssistantMessage",
                      "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                      "SystemMessage": "#/$defs/SystemMessage",
                      "UserMessage": "#/$defs/UserMessage"
                   },
                   "propertyName": "type"
                },
                "oneOf": [
                   {
                      "$ref": "#/$defs/SystemMessage"
                   },
                   {
                      "$ref": "#/$defs/UserMessage"
                   },
                   {
                      "$ref": "#/$defs/AssistantMessage"
                   },
                   {
                      "$ref": "#/$defs/FunctionExecutionResultMessage"
                   }
                ]
             },
             "title": "Messages",
             "type": "array"
          }
       },
       "$defs": {
          "AssistantMessage": {
             "description": "Assistant message are sampled from the language model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "$ref": "#/$defs/FunctionCall"
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "thought": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Thought"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "AssistantMessage",
                   "default": "AssistantMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "AssistantMessage",
             "type": "object"
          },
          "FunctionCall": {
             "properties": {
                "id": {
                   "title": "Id",
                   "type": "string"
                },
                "arguments": {
                   "title": "Arguments",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                }
             },
             "required": [
                "id",
                "arguments",
                "name"
             ],
             "title": "FunctionCall",
             "type": "object"
          },
          "FunctionExecutionResult": {
             "description": "Function execution result contains the output of a function call.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "name": {
                   "title": "Name",
                   "type": "string"
                },
                "call_id": {
                   "title": "Call Id",
                   "type": "string"
                },
                "is_error": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Is Error"
                }
             },
             "required": [
                "content",
                "name",
                "call_id"
             ],
             "title": "FunctionExecutionResult",
             "type": "object"
          },
          "FunctionExecutionResultMessage": {
             "description": "Function execution result message contains the output of multiple function calls.",
             "properties": {
                "content": {
                   "items": {
                      "$ref": "#/$defs/FunctionExecutionResult"
                   },
                   "title": "Content",
                   "type": "array"
                },
                "type": {
                   "const": "FunctionExecutionResultMessage",
                   "default": "FunctionExecutionResultMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "FunctionExecutionResultMessage",
             "type": "object"
          },
          "SystemMessage": {
             "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
             "properties": {
                "content": {
                   "title": "Content",
                   "type": "string"
                },
                "type": {
                   "const": "SystemMessage",
                   "default": "SystemMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content"
             ],
             "title": "SystemMessage",
             "type": "object"
          },
          "UserMessage": {
             "description": "User message contains input from end users, or a catch-all for data provided to the model.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "items": {
                            "anyOf": [
                               {
                                  "type": "string"
                               },
                               {}
                            ]
                         },
                         "type": "array"
                      }
                   ],
                   "title": "Content"
                },
                "source": {
                   "title": "Source",
                   "type": "string"
                },
                "type": {
                   "const": "UserMessage",
                   "default": "UserMessage",
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "content",
                "source"
             ],
             "title": "UserMessage",
             "type": "object"
          }
       }
    }

    ```

    Fields:
    :   * `messages (List[autogen_core.models._types.SystemMessage | autogen_core.models._types.UserMessage | autogen_core.models._types.AssistantMessage | autogen_core.models._types.FunctionExecutionResultMessage])`

    *field* messages*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]* *[Optional]*[#](#autogen_core.model_context.ChatCompletionContextState.messages "Link to this definition")

*class* HeadAndTailChatCompletionContext(*head\_size: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*, *tail\_size: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*, *initial\_messages: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_core.model_context.HeadAndTailChatCompletionContext "Link to this definition")
:   Bases: [`ChatCompletionContext`](#autogen_core.model_context.ChatCompletionContext "autogen_core.model_context._chat_completion_context.ChatCompletionContext"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`HeadAndTailChatCompletionContextConfig`]

    A chat completion context that keeps a view of the first n and last m messages,
    where n is the head size and m is the tail size. The head and tail sizes
    are set at initialization.

    Parameters:
    :   * **head\_size** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The size of the head.
        * **tail\_size** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The size of the tail.
        * **initial\_messages** (*List**[**LLMMessage**]* *|* *None*) – The initial messages.

    *classmethod* \_from\_config(*config: HeadAndTailChatCompletionContextConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_core.model_context.HeadAndTailChatCompletionContext._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → HeadAndTailChatCompletionContextConfig[#](#autogen_core.model_context.HeadAndTailChatCompletionContext._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_core.model_context.HeadAndTailChatCompletionContext.component_config_schema "Link to this definition")
    :   alias of `HeadAndTailChatCompletionContextConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_core.model\_context.HeadAndTailChatCompletionContext'*[#](#autogen_core.model_context.HeadAndTailChatCompletionContext.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* get\_messages() → [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]][#](#autogen_core.model_context.HeadAndTailChatCompletionContext.get_messages "Link to this definition")
    :   Get at most head\_size recent messages and tail\_size oldest messages.

*class* UnboundedChatCompletionContext(*initial\_messages: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_core.model_context.UnboundedChatCompletionContext "Link to this definition")
:   Bases: [`ChatCompletionContext`](#autogen_core.model_context.ChatCompletionContext "autogen_core.model_context._chat_completion_context.ChatCompletionContext"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`UnboundedChatCompletionContextConfig`]

    An unbounded chat completion context that keeps a view of the all the messages.

    *classmethod* \_from\_config(*config: UnboundedChatCompletionContextConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_core.model_context.UnboundedChatCompletionContext._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → UnboundedChatCompletionContextConfig[#](#autogen_core.model_context.UnboundedChatCompletionContext._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_core.model_context.UnboundedChatCompletionContext.component_config_schema "Link to this definition")
    :   alias of `UnboundedChatCompletionContextConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_core.model\_context.UnboundedChatCompletionContext'*[#](#autogen_core.model_context.UnboundedChatCompletionContext.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* get\_messages() → [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]][#](#autogen_core.model_context.UnboundedChatCompletionContext.get_messages "Link to this definition")
    :   Get at most buffer\_size recent messages.

### autogen\_core.tools[#](#module-autogen_core.tools "Link to this heading")

*class* BaseTool(*args\_type: [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[ArgsT]*, *return\_type: [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[ReturnT]*, *name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *strict: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*)[#](#autogen_core.tools.BaseTool "Link to this definition")
:   Bases: [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC "(in Python v3.13)"), [`Tool`](#autogen_core.tools.Tool "autogen_core.tools._base.Tool"), [`Generic`](https://docs.python.org/3/library/typing.html#typing.Generic "(in Python v3.13)")[`ArgsT`, `ReturnT`], [`ComponentBase`](#autogen_core.ComponentBase "autogen_core._component_config.ComponentBase")[`BaseModel`]

    args\_type() → [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[BaseModel][#](#autogen_core.tools.BaseTool.args_type "Link to this definition")

    component\_type*: ClassVar[ComponentType]* *= 'tool'*[#](#autogen_core.tools.BaseTool.component_type "Link to this definition")
    :   The logical type of the component.

    *property* description*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.tools.BaseTool.description "Link to this definition")

    load\_state\_json(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.tools.BaseTool.load_state_json "Link to this definition")

    *property* name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.tools.BaseTool.name "Link to this definition")

    return\_type() → [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.tools.BaseTool.return_type "Link to this definition")

    return\_value\_as\_string(*value: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_core.tools.BaseTool.return_value_as_string "Link to this definition")

    *abstract async* run(*args: ArgsT*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → ReturnT[#](#autogen_core.tools.BaseTool.run "Link to this definition")

    *async* run\_json(*args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_core.tools.BaseTool.run_json "Link to this definition")

    save\_state\_json() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.tools.BaseTool.save_state_json "Link to this definition")

    *property* schema*: [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")*[#](#autogen_core.tools.BaseTool.schema "Link to this definition")

    state\_type() → [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.tools.BaseTool.state_type "Link to this definition")

*class* BaseToolWithState(*args\_type: [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[ArgsT]*, *return\_type: [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[ReturnT]*, *state\_type: [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[StateT]*, *name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_core.tools.BaseToolWithState "Link to this definition")
:   Bases: [`BaseTool`](#autogen_core.tools.BaseTool "autogen_core.tools._base.BaseTool")[`ArgsT`, `ReturnT`], [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC "(in Python v3.13)"), [`Generic`](https://docs.python.org/3/library/typing.html#typing.Generic "(in Python v3.13)")[`ArgsT`, `ReturnT`, `StateT`], [`ComponentBase`](#autogen_core.ComponentBase "autogen_core._component_config.ComponentBase")[`BaseModel`]

    component\_type*: ClassVar[ComponentType]* *= 'tool'*[#](#autogen_core.tools.BaseToolWithState.component_type "Link to this definition")
    :   The logical type of the component.

    *abstract* load\_state(*state: StateT*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.tools.BaseToolWithState.load_state "Link to this definition")

    load\_state\_json(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.tools.BaseToolWithState.load_state_json "Link to this definition")

    *abstract* save\_state() → StateT[#](#autogen_core.tools.BaseToolWithState.save_state "Link to this definition")

    save\_state\_json() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.tools.BaseToolWithState.save_state_json "Link to this definition")

*class* FunctionTool(*func: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[...], [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*, *description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *global\_imports: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [ImportFromModule](#autogen_core.code_executor.ImportFromModule "autogen_core.code_executor._func_with_reqs.ImportFromModule") | [Alias](#autogen_core.code_executor.Alias "autogen_core.code_executor._func_with_reqs.Alias")] = []*, *strict: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*)[#](#autogen_core.tools.FunctionTool "Link to this definition")
:   Bases: [`BaseTool`](#autogen_core.tools.BaseTool "autogen_core.tools._base.BaseTool")[`BaseModel`, `BaseModel`], [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`FunctionToolConfig`]

    Create custom tools by wrapping standard Python functions.

    FunctionTool offers an interface for executing Python functions either asynchronously or synchronously.
    Each function must include type annotations for all parameters and its return type. These annotations
    enable FunctionTool to generate a schema necessary for input validation, serialization, and for informing
    the LLM about expected parameters. When the LLM prepares a function call, it leverages this schema to
    generate arguments that align with the function’s specifications.

    Note

    It is the user’s responsibility to verify that the tool’s output type matches the expected type.

    Parameters:
    :   * **func** (*Callable**[**...**,* *ReturnT* *|* *Awaitable**[**ReturnT**]**]*) – The function to wrap and expose as a tool.
        * **description** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – A description to inform the model of the function’s purpose, specifying what
          it does and the context in which it should be called.
        * **name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – An optional custom name for the tool. Defaults to
          the function’s original name if not provided.
        * **strict** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – If set to True, the tool schema will only contain arguments that are explicitly
          defined in the function signature, and no default values will be allowed. Defaults to False.
          This is required to be set to True when used with models in structured output mode.

    Example

    ```
    import random
    from autogen_core import CancellationToken
    from autogen_core.tools import FunctionTool
    from typing_extensions import Annotated
    import asyncio

    async def get_stock_price(ticker: str, date: Annotated[str, "Date in YYYY/MM/DD"]) -> float:
        # Simulates a stock price retrieval by returning a random float within a specified range.
        return random.uniform(10, 200)

    async def example():
        # Initialize a FunctionTool instance for retrieving stock prices.
        stock_price_tool = FunctionTool(get_stock_price, description="Fetch the stock price for a given ticker.")

        # Execute the tool with cancellation support.
        cancellation_token = CancellationToken()
        result = await stock_price_tool.run_json({"ticker": "AAPL", "date": "2021/01/01"}, cancellation_token)

        # Output the result as a formatted string.
        print(stock_price_tool.return_value_as_string(result))

    asyncio.run(example())

    ```

    *classmethod* \_from\_config(*config: FunctionToolConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_core.tools.FunctionTool._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → FunctionToolConfig[#](#autogen_core.tools.FunctionTool._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_core.tools.FunctionTool.component_config_schema "Link to this definition")
    :   alias of `FunctionToolConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_core.tools.FunctionTool'*[#](#autogen_core.tools.FunctionTool.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* run(*args: BaseModel*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_core.tools.FunctionTool.run "Link to this definition")

*class* ParametersSchema[#](#autogen_core.tools.ParametersSchema "Link to this definition")
:   Bases: [`TypedDict`](https://docs.python.org/3/library/typing.html#typing.TypedDict "(in Python v3.13)")

    additionalProperties*: [NotRequired](https://docs.python.org/3/library/typing.html#typing.NotRequired "(in Python v3.13)")[[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")]*[#](#autogen_core.tools.ParametersSchema.additionalProperties "Link to this definition")

    properties*: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*[#](#autogen_core.tools.ParametersSchema.properties "Link to this definition")

    required*: [NotRequired](https://docs.python.org/3/library/typing.html#typing.NotRequired "(in Python v3.13)")[[Sequence](https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]]*[#](#autogen_core.tools.ParametersSchema.required "Link to this definition")

    type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.tools.ParametersSchema.type "Link to this definition")

*class* Tool(*\*args*, *\*\*kwargs*)[#](#autogen_core.tools.Tool "Link to this definition")
:   Bases: [`Protocol`](https://docs.python.org/3/library/typing.html#typing.Protocol "(in Python v3.13)")

    args\_type() → [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[BaseModel][#](#autogen_core.tools.Tool.args_type "Link to this definition")

    *property* description*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.tools.Tool.description "Link to this definition")

    load\_state\_json(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.tools.Tool.load_state_json "Link to this definition")

    *property* name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.tools.Tool.name "Link to this definition")

    return\_type() → [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.tools.Tool.return_type "Link to this definition")

    return\_value\_as\_string(*value: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_core.tools.Tool.return_value_as_string "Link to this definition")

    *async* run\_json(*args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_core.tools.Tool.run_json "Link to this definition")

    save\_state\_json() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_core.tools.Tool.save_state_json "Link to this definition")

    *property* schema*: [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")*[#](#autogen_core.tools.Tool.schema "Link to this definition")

    state\_type() → [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.tools.Tool.state_type "Link to this definition")

*class* ToolSchema[#](#autogen_core.tools.ToolSchema "Link to this definition")
:   Bases: [`TypedDict`](https://docs.python.org/3/library/typing.html#typing.TypedDict "(in Python v3.13)")

    description*: [NotRequired](https://docs.python.org/3/library/typing.html#typing.NotRequired "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*[#](#autogen_core.tools.ToolSchema.description "Link to this definition")

    name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.tools.ToolSchema.name "Link to this definition")

    parameters*: [NotRequired](https://docs.python.org/3/library/typing.html#typing.NotRequired "(in Python v3.13)")[[ParametersSchema](#autogen_core.tools.ParametersSchema "autogen_core.tools._base.ParametersSchema")]*[#](#autogen_core.tools.ToolSchema.parameters "Link to this definition")

    strict*: [NotRequired](https://docs.python.org/3/library/typing.html#typing.NotRequired "(in Python v3.13)")[[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")]*[#](#autogen_core.tools.ToolSchema.strict "Link to this definition")

### autogen\_core.tool\_agent[#](#module-autogen_core.tool_agent "Link to this heading")

*exception* InvalidToolArgumentsException(*call\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *content: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_core.tool_agent.InvalidToolArgumentsException "Link to this definition")
:   Bases: [`ToolException`](#autogen_core.tool_agent.ToolException "autogen_core.tool_agent._tool_agent.ToolException")

*class* ToolAgent(*description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *tools: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool")]*)[#](#autogen_core.tool_agent.ToolAgent "Link to this definition")
:   Bases: [`RoutedAgent`](#autogen_core.RoutedAgent "autogen_core._routed_agent.RoutedAgent")

    A tool agent accepts direct messages of the type FunctionCall,
    executes the requested tool with the provided arguments, and returns the
    result as FunctionExecutionResult messages.

    Parameters:
    :   * **description** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The description of the agent.
        * **tools** (*List**[*[*Tool*](#autogen_core.tools.Tool "autogen_core.tools.Tool")*]*) – The list of tools that the agent can execute.

    *async* handle\_function\_call(*message: [FunctionCall](#autogen_core.FunctionCall "autogen_core._types.FunctionCall")*, *ctx: [MessageContext](#autogen_core.MessageContext "autogen_core._message_context.MessageContext")*) → [FunctionExecutionResult](#autogen_core.models.FunctionExecutionResult "autogen_core.models._types.FunctionExecutionResult")[#](#autogen_core.tool_agent.ToolAgent.handle_function_call "Link to this definition")
    :   Handles a FunctionCall message by executing the requested tool with the provided arguments.

        Parameters:
        :   * **message** ([*FunctionCall*](#autogen_core.FunctionCall "autogen_core.FunctionCall")) – The function call message.
            * **cancellation\_token** ([*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken")) – The cancellation token.

        Returns:
        :   **FunctionExecutionResult** – The result of the function execution.

        Raises:
        :   * [**ToolNotFoundException**](#autogen_core.tool_agent.ToolNotFoundException "autogen_core.tool_agent.ToolNotFoundException") – If the tool is not found.
            * [**InvalidToolArgumentsException**](#autogen_core.tool_agent.InvalidToolArgumentsException "autogen_core.tool_agent.InvalidToolArgumentsException") – If the tool arguments are invalid.
            * [**ToolExecutionException**](#autogen_core.tool_agent.ToolExecutionException "autogen_core.tool_agent.ToolExecutionException") – If the tool execution fails.

    *property* tools*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool")]*[#](#autogen_core.tool_agent.ToolAgent.tools "Link to this definition")

*exception* ToolException(*call\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *content: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_core.tool_agent.ToolException "Link to this definition")
:   Bases: [`BaseException`](https://docs.python.org/3/library/exceptions.html#BaseException "(in Python v3.13)")

    call\_id*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.tool_agent.ToolException.call_id "Link to this definition")

    content*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.tool_agent.ToolException.content "Link to this definition")

    name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.tool_agent.ToolException.name "Link to this definition")

*exception* ToolExecutionException(*call\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *content: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_core.tool_agent.ToolExecutionException "Link to this definition")
:   Bases: [`ToolException`](#autogen_core.tool_agent.ToolException "autogen_core.tool_agent._tool_agent.ToolException")

*exception* ToolNotFoundException(*call\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *content: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_core.tool_agent.ToolNotFoundException "Link to this definition")
:   Bases: [`ToolException`](#autogen_core.tool_agent.ToolException "autogen_core.tool_agent._tool_agent.ToolException")

*async* tool\_agent\_caller\_loop(*caller: [BaseAgent](#autogen_core.BaseAgent "autogen_core._base_agent.BaseAgent") | [AgentRuntime](#autogen_core.AgentRuntime "autogen_core._agent_runtime.AgentRuntime")*, *tool\_agent\_id: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *model\_client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *input\_messages: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *tool\_schema: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] | [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool")]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *caller\_source: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'assistant'*) → [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]][#](#autogen_core.tool_agent.tool_agent_caller_loop "Link to this definition")
:   Start a caller loop for a tool agent. This function sends messages to the tool agent
    and the model client in an alternating fashion until the model client stops generating tool calls.

    Parameters:
    :   * **tool\_agent\_id** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The Agent ID of the tool agent.
        * **input\_messages** (*List**[**LLMMessage**]*) – The list of input messages.
        * **model\_client** ([*ChatCompletionClient*](#autogen_core.models.ChatCompletionClient "autogen_core.models.ChatCompletionClient")) – The model client to use for the model API.
        * **tool\_schema** (*List**[*[*Tool*](#autogen_core.tools.Tool "autogen_core.tools.Tool") *|* [*ToolSchema*](#autogen_core.tools.ToolSchema "autogen_core.tools.ToolSchema")*]*) – The list of tools that the model can use.

    Returns:
    :   **List[LLMMessage]** – The list of output messages created in the caller loop.

### autogen\_core.memory[#](#module-autogen_core.memory "Link to this heading")

*class* ListMemory(*name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *memory\_contents: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[MemoryContent](#autogen_core.memory.MemoryContent "autogen_core.memory._base_memory.MemoryContent")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_core.memory.ListMemory "Link to this definition")
:   Bases: [`Memory`](#autogen_core.memory.Memory "autogen_core.memory._base_memory.Memory"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`ListMemoryConfig`]

    Simple chronological list-based memory implementation.

    This memory implementation stores contents in a list and retrieves them in
    chronological order. It has an update\_context method that updates model contexts
    by appending all stored memories.

    The memory content can be directly accessed and modified through the content property,
    allowing external applications to manage memory contents directly.

    Example

    ```
    import asyncio
    from autogen_core.memory import ListMemory, MemoryContent
    from autogen_core.model_context import BufferedChatCompletionContext

    async def main() -> None:
        # Initialize memory
        memory = ListMemory(name="chat_history")

        # Add memory content
        content = MemoryContent(content="User prefers formal language", mime_type="text/plain")
        await memory.add(content)

        # Directly modify memory contents
        memory.content = [MemoryContent(content="New preference", mime_type="text/plain")]

        # Create a model context
        model_context = BufferedChatCompletionContext(buffer_size=10)

        # Update a model context with memory
        await memory.update_context(model_context)

        # See the updated model context
        print(await model_context.get_messages())

    asyncio.run(main())

    ```

    Parameters:
    :   **name** – Optional identifier for this memory instance

    *classmethod* \_from\_config(*config: ListMemoryConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_core.memory.ListMemory._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → ListMemoryConfig[#](#autogen_core.memory.ListMemory._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    *async* add(*content: [MemoryContent](#autogen_core.memory.MemoryContent "autogen_core.memory._base_memory.MemoryContent")*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.memory.ListMemory.add "Link to this definition")
    :   Add new content to memory.

        Parameters:
        :   * **content** – Memory content to store
            * **cancellation\_token** – Optional token to cancel operation

    *async* clear() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.memory.ListMemory.clear "Link to this definition")
    :   Clear all memory content.

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.memory.ListMemory.close "Link to this definition")
    :   Cleanup resources if needed.

    component\_config\_schema[#](#autogen_core.memory.ListMemory.component_config_schema "Link to this definition")
    :   alias of `ListMemoryConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_core.memory.ListMemory'*[#](#autogen_core.memory.ListMemory.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    component\_type*: ClassVar[ComponentType]* *= 'memory'*[#](#autogen_core.memory.ListMemory.component_type "Link to this definition")
    :   The logical type of the component.

    *property* content*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[MemoryContent](#autogen_core.memory.MemoryContent "autogen_core.memory._base_memory.MemoryContent")]*[#](#autogen_core.memory.ListMemory.content "Link to this definition")
    :   Get the current memory contents.

        Returns:
        :   **List[MemoryContent]** – List of stored memory contents

    *property* name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_core.memory.ListMemory.name "Link to this definition")
    :   Get the memory instance identifier.

        Returns:
        :   **str** – Memory instance name

    *async* query(*query: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [MemoryContent](#autogen_core.memory.MemoryContent "autogen_core.memory._base_memory.MemoryContent") = ''*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *\*\*kwargs: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*) → [MemoryQueryResult](#autogen_core.memory.MemoryQueryResult "autogen_core.memory._base_memory.MemoryQueryResult")[#](#autogen_core.memory.ListMemory.query "Link to this definition")
    :   Return all memories without any filtering.

        Parameters:
        :   * **query** – Ignored in this implementation
            * **cancellation\_token** – Optional token to cancel operation
            * **\*\*kwargs** – Additional parameters (ignored)

        Returns:
        :   **MemoryQueryResult containing all stored memories**

    *async* update\_context(*model\_context: [ChatCompletionContext](#autogen_core.model_context.ChatCompletionContext "autogen_core.model_context._chat_completion_context.ChatCompletionContext")*) → [UpdateContextResult](#autogen_core.memory.UpdateContextResult "autogen_core.memory._base_memory.UpdateContextResult")[#](#autogen_core.memory.ListMemory.update_context "Link to this definition")
    :   Update the model context by appending memory content.

        This method mutates the provided model\_context by adding all memories as a
        SystemMessage.

        Parameters:
        :   **model\_context** – The context to update. Will be mutated if memories exist.

        Returns:
        :   **UpdateContextResult containing the memories that were added to the context**

*class* Memory[#](#autogen_core.memory.Memory "Link to this definition")
:   Bases: [`ABC`](https://docs.python.org/3/library/abc.html#abc.ABC "(in Python v3.13)"), [`ComponentBase`](#autogen_core.ComponentBase "autogen_core._component_config.ComponentBase")[`BaseModel`]

    Protocol defining the interface for memory implementations.

    A memory is the storage for data that can be used to enrich or modify the model context.

    A memory implementation can use any storage mechanism, such as a list, a database, or a file system.
    It can also use any retrieval mechanism, such as vector search or text search.
    It is up to the implementation to decide how to store and retrieve data.

    It is also a memory implementation’s responsibility to update the model context
    with relevant memory content based on the current model context and querying the memory store.

    See [`ListMemory`](#autogen_core.memory.ListMemory "autogen_core.memory.ListMemory") for an example implementation.

    *abstract async* add(*content: [MemoryContent](#autogen_core.memory.MemoryContent "autogen_core.memory._base_memory.MemoryContent")*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.memory.Memory.add "Link to this definition")
    :   Add a new content to memory.

        Parameters:
        :   * **content** – The memory content to add
            * **cancellation\_token** – Optional token to cancel operation

    *abstract async* clear() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.memory.Memory.clear "Link to this definition")
    :   Clear all entries from memory.

    *abstract async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_core.memory.Memory.close "Link to this definition")
    :   Clean up any resources used by the memory implementation.

    component\_type*: ClassVar[ComponentType]* *= 'memory'*[#](#autogen_core.memory.Memory.component_type "Link to this definition")
    :   The logical type of the component.

    *abstract async* query(*query: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [MemoryContent](#autogen_core.memory.MemoryContent "autogen_core.memory._base_memory.MemoryContent")*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *\*\*kwargs: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*) → [MemoryQueryResult](#autogen_core.memory.MemoryQueryResult "autogen_core.memory._base_memory.MemoryQueryResult")[#](#autogen_core.memory.Memory.query "Link to this definition")
    :   Query the memory store and return relevant entries.

        Parameters:
        :   * **query** – Query content item
            * **cancellation\_token** – Optional token to cancel operation
            * **\*\*kwargs** – Additional implementation-specific parameters

        Returns:
        :   **MemoryQueryResult containing memory entries with relevance scores**

    *abstract async* update\_context(*model\_context: [ChatCompletionContext](#autogen_core.model_context.ChatCompletionContext "autogen_core.model_context._chat_completion_context.ChatCompletionContext")*) → [UpdateContextResult](#autogen_core.memory.UpdateContextResult "autogen_core.memory._base_memory.UpdateContextResult")[#](#autogen_core.memory.Memory.update_context "Link to this definition")
    :   Update the provided model context using relevant memory content.

        Parameters:
        :   **model\_context** – The context to update.

        Returns:
        :   **UpdateContextResult containing relevant memories**

*pydantic model* MemoryContent[#](#autogen_core.memory.MemoryContent "Link to this definition")
:   Bases: `BaseModel`

    A memory content item.

    Show JSON schema

    ```
    {
       "title": "MemoryContent",
       "description": "A memory content item.",
       "type": "object",
       "properties": {
          "content": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "format": "binary",
                   "type": "string"
                },
                {
                   "type": "object"
                },
                {}
             ],
             "title": "Content"
          },
          "mime_type": {
             "anyOf": [
                {
                   "$ref": "#/$defs/MemoryMimeType"
                },
                {
                   "type": "string"
                }
             ],
             "title": "Mime Type"
          },
          "metadata": {
             "anyOf": [
                {
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Metadata"
          }
       },
       "$defs": {
          "MemoryMimeType": {
             "description": "Supported MIME types for memory content.",
             "enum": [
                "text/plain",
                "application/json",
                "text/markdown",
                "image/*",
                "application/octet-stream"
             ],
             "title": "MemoryMimeType",
             "type": "string"
          }
       },
       "required": [
          "content",
          "mime_type"
       ]
    }

    ```

    Fields:
    :   * `content (str | bytes | Dict[str, Any] | autogen_core._image.Image)`
        * `metadata (Dict[str, Any] | None)`
        * `mime_type (autogen_core.memory._base_memory.MemoryMimeType | str)`

    *field* content*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)") | [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [Image](#autogen_core.Image "autogen_core._image.Image")* *[Required]*[#](#autogen_core.memory.MemoryContent.content "Link to this definition")
    :   The content of the memory item. It can be a string, bytes, dict, or [`Image`](#autogen_core.Image "autogen_core.Image").

    *field* metadata*: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_core.memory.MemoryContent.metadata "Link to this definition")
    :   Metadata associated with the memory item.

    *field* mime\_type*: [MemoryMimeType](#autogen_core.memory.MemoryMimeType "autogen_core.memory._base_memory.MemoryMimeType") | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_core.memory.MemoryContent.mime_type "Link to this definition")
    :   The MIME type of the memory content.

    serialize\_mime\_type(*mime\_type: [MemoryMimeType](#autogen_core.memory.MemoryMimeType "autogen_core.memory._base_memory.MemoryMimeType") | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_core.memory.MemoryContent.serialize_mime_type "Link to this definition")
    :   Serialize the MIME type to a string.

*class* MemoryMimeType(*value*, *names=\_not\_given*, *\*values*, *module=None*, *qualname=None*, *type=None*, *start=1*, *boundary=None*)[#](#autogen_core.memory.MemoryMimeType "Link to this definition")
:   Bases: [`Enum`](https://docs.python.org/3/library/enum.html#enum.Enum "(in Python v3.13)")

    Supported MIME types for memory content.

    BINARY *= 'application/octet-stream'*[#](#autogen_core.memory.MemoryMimeType.BINARY "Link to this definition")

    IMAGE *= 'image/\*'*[#](#autogen_core.memory.MemoryMimeType.IMAGE "Link to this definition")

    JSON *= 'application/json'*[#](#autogen_core.memory.MemoryMimeType.JSON "Link to this definition")

    MARKDOWN *= 'text/markdown'*[#](#autogen_core.memory.MemoryMimeType.MARKDOWN "Link to this definition")

    TEXT *= 'text/plain'*[#](#autogen_core.memory.MemoryMimeType.TEXT "Link to this definition")

*pydantic model* MemoryQueryResult[#](#autogen_core.memory.MemoryQueryResult "Link to this definition")
:   Bases: `BaseModel`

    Result of a memory [`query()`](#autogen_core.memory.Memory.query "autogen_core.memory.Memory.query") operation.

    Show JSON schema

    ```
    {
       "title": "MemoryQueryResult",
       "description": "Result of a memory :meth:`~autogen_core.memory.Memory.query` operation.",
       "type": "object",
       "properties": {
          "results": {
             "items": {
                "$ref": "#/$defs/MemoryContent"
             },
             "title": "Results",
             "type": "array"
          }
       },
       "$defs": {
          "MemoryContent": {
             "description": "A memory content item.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "format": "binary",
                         "type": "string"
                      },
                      {
                         "type": "object"
                      },
                      {}
                   ],
                   "title": "Content"
                },
                "mime_type": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/MemoryMimeType"
                      },
                      {
                         "type": "string"
                      }
                   ],
                   "title": "Mime Type"
                },
                "metadata": {
                   "anyOf": [
                      {
                         "type": "object"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Metadata"
                }
             },
             "required": [
                "content",
                "mime_type"
             ],
             "title": "MemoryContent",
             "type": "object"
          },
          "MemoryMimeType": {
             "description": "Supported MIME types for memory content.",
             "enum": [
                "text/plain",
                "application/json",
                "text/markdown",
                "image/*",
                "application/octet-stream"
             ],
             "title": "MemoryMimeType",
             "type": "string"
          }
       },
       "required": [
          "results"
       ]
    }

    ```

    Fields:
    :   * `results (List[autogen_core.memory._base_memory.MemoryContent])`

    *field* results*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[MemoryContent](#autogen_core.memory.MemoryContent "autogen_core.memory._base_memory.MemoryContent")]* *[Required]*[#](#autogen_core.memory.MemoryQueryResult.results "Link to this definition")

*pydantic model* UpdateContextResult[#](#autogen_core.memory.UpdateContextResult "Link to this definition")
:   Bases: `BaseModel`

    Result of a memory [`update_context()`](#autogen_core.memory.Memory.update_context "autogen_core.memory.Memory.update_context") operation.

    Show JSON schema

    ```
    {
       "title": "UpdateContextResult",
       "description": "Result of a memory :meth:`~autogen_core.memory.Memory.update_context` operation.",
       "type": "object",
       "properties": {
          "memories": {
             "$ref": "#/$defs/MemoryQueryResult"
          }
       },
       "$defs": {
          "MemoryContent": {
             "description": "A memory content item.",
             "properties": {
                "content": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "format": "binary",
                         "type": "string"
                      },
                      {
                         "type": "object"
                      },
                      {}
                   ],
                   "title": "Content"
                },
                "mime_type": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/MemoryMimeType"
                      },
                      {
                         "type": "string"
                      }
                   ],
                   "title": "Mime Type"
                },
                "metadata": {
                   "anyOf": [
                      {
                         "type": "object"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Metadata"
                }
             },
             "required": [
                "content",
                "mime_type"
             ],
             "title": "MemoryContent",
             "type": "object"
          },
          "MemoryMimeType": {
             "description": "Supported MIME types for memory content.",
             "enum": [
                "text/plain",
                "application/json",
                "text/markdown",
                "image/*",
                "application/octet-stream"
             ],
             "title": "MemoryMimeType",
             "type": "string"
          },
          "MemoryQueryResult": {
             "description": "Result of a memory :meth:`~autogen_core.memory.Memory.query` operation.",
             "properties": {
                "results": {
                   "items": {
                      "$ref": "#/$defs/MemoryContent"
                   },
                   "title": "Results",
                   "type": "array"
                }
             },
             "required": [
                "results"
             ],
             "title": "MemoryQueryResult",
             "type": "object"
          }
       },
       "required": [
          "memories"
       ]
    }

    ```

    Fields:
    :   * `memories (autogen_core.memory._base_memory.MemoryQueryResult)`

    *field* memories*: [MemoryQueryResult](#autogen_core.memory.MemoryQueryResult "autogen_core.memory._base_memory.MemoryQueryResult")* *[Required]*[#](#autogen_core.memory.UpdateContextResult.memories "Link to this definition")

### autogen\_core.exceptions[#](#module-autogen_core.exceptions "Link to this heading")

*exception* CantHandleException[#](#autogen_core.exceptions.CantHandleException "Link to this definition")
:   Bases: [`Exception`](https://docs.python.org/3/library/exceptions.html#Exception "(in Python v3.13)")

    Raised when a handler can’t handle the exception.

*exception* MessageDroppedException[#](#autogen_core.exceptions.MessageDroppedException "Link to this definition")
:   Bases: [`Exception`](https://docs.python.org/3/library/exceptions.html#Exception "(in Python v3.13)")

    Raised when a message is dropped.

*exception* NotAccessibleError[#](#autogen_core.exceptions.NotAccessibleError "Link to this definition")
:   Bases: [`Exception`](https://docs.python.org/3/library/exceptions.html#Exception "(in Python v3.13)")

    Tried to access a value that is not accessible. For example if it is remote cannot be accessed locally.

*exception* UndeliverableException[#](#autogen_core.exceptions.UndeliverableException "Link to this definition")
:   Bases: [`Exception`](https://docs.python.org/3/library/exceptions.html#Exception "(in Python v3.13)")

    Raised when a message can’t be delivered.

### autogen\_core.logging[#](#module-autogen_core.logging "Link to this heading")

*class* AgentConstructionExceptionEvent(*\**, *agent\_id: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *exception: [BaseException](https://docs.python.org/3/library/exceptions.html#BaseException "(in Python v3.13)")*, *\*\*kwargs: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*)[#](#autogen_core.logging.AgentConstructionExceptionEvent "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

*class* DeliveryStage(*value*, *names=\_not\_given*, *\*values*, *module=None*, *qualname=None*, *type=None*, *start=1*, *boundary=None*)[#](#autogen_core.logging.DeliveryStage "Link to this definition")
:   Bases: [`Enum`](https://docs.python.org/3/library/enum.html#enum.Enum "(in Python v3.13)")

    DELIVER *= 2*[#](#autogen_core.logging.DeliveryStage.DELIVER "Link to this definition")

    SEND *= 1*[#](#autogen_core.logging.DeliveryStage.SEND "Link to this definition")

*class* LLMCallEvent(*\**, *messages: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]]*, *response: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*, *prompt\_tokens: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*, *completion\_tokens: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*, *\*\*kwargs: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*)[#](#autogen_core.logging.LLMCallEvent "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    *property* completion\_tokens*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*[#](#autogen_core.logging.LLMCallEvent.completion_tokens "Link to this definition")

    *property* prompt\_tokens*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*[#](#autogen_core.logging.LLMCallEvent.prompt_tokens "Link to this definition")

*class* LLMStreamEndEvent(*\**, *response: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*, *prompt\_tokens: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*, *completion\_tokens: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*, *\*\*kwargs: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*)[#](#autogen_core.logging.LLMStreamEndEvent "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    *property* completion\_tokens*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*[#](#autogen_core.logging.LLMStreamEndEvent.completion_tokens "Link to this definition")

    *property* prompt\_tokens*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*[#](#autogen_core.logging.LLMStreamEndEvent.prompt_tokens "Link to this definition")

*class* LLMStreamStartEvent(*\**, *messages: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]]*, *\*\*kwargs: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*)[#](#autogen_core.logging.LLMStreamStartEvent "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    To be used by model clients to log the start of a stream.

    Parameters:
    :   **messages** (*List**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]**]*) – The messages used in the call. Must be json serializable.

    Example

    ```
    import logging
    from autogen_core import EVENT_LOGGER_NAME
    from autogen_core.logging import LLMStreamStartEvent

    messages = [{"role": "user", "content": "Hello, world!"}]
    logger = logging.getLogger(EVENT_LOGGER_NAME)
    logger.info(LLMStreamStartEvent(messages=messages))

    ```

*class* MessageDroppedEvent(*\**, *payload: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *sender: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*, *receiver: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*, *kind: [MessageKind](#autogen_core.logging.MessageKind "autogen_core.logging.MessageKind")*, *\*\*kwargs: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*)[#](#autogen_core.logging.MessageDroppedEvent "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

*class* MessageEvent(*\**, *payload: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *sender: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*, *receiver: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*, *kind: [MessageKind](#autogen_core.logging.MessageKind "autogen_core.logging.MessageKind")*, *delivery\_stage: [DeliveryStage](#autogen_core.logging.DeliveryStage "autogen_core.logging.DeliveryStage")*, *\*\*kwargs: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*)[#](#autogen_core.logging.MessageEvent "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

*class* MessageHandlerExceptionEvent(*\**, *payload: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *handling\_agent: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *exception: [BaseException](https://docs.python.org/3/library/exceptions.html#BaseException "(in Python v3.13)")*, *\*\*kwargs: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*)[#](#autogen_core.logging.MessageHandlerExceptionEvent "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

*class* MessageKind(*value*, *names=\_not\_given*, *\*values*, *module=None*, *qualname=None*, *type=None*, *start=1*, *boundary=None*)[#](#autogen_core.logging.MessageKind "Link to this definition")
:   Bases: [`Enum`](https://docs.python.org/3/library/enum.html#enum.Enum "(in Python v3.13)")

    DIRECT *= 1*[#](#autogen_core.logging.MessageKind.DIRECT "Link to this definition")

    PUBLISH *= 2*[#](#autogen_core.logging.MessageKind.PUBLISH "Link to this definition")

    RESPOND *= 3*[#](#autogen_core.logging.MessageKind.RESPOND "Link to this definition")

*class* ToolCallEvent(*\**, *tool\_name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *arguments: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*, *result: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_core.logging.ToolCallEvent "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

### autogen\_ext.agents.magentic\_one[#](#module-autogen_ext.agents.magentic_one "Link to this heading")

*class* MagenticOneCoderAgent(*name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *model\_client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *\*\*kwargs: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*)[#](#autogen_ext.agents.magentic_one.MagenticOneCoderAgent "Link to this definition")
:   Bases: [`AssistantAgent`](#autogen_agentchat.agents.AssistantAgent "autogen_agentchat.agents._assistant_agent.AssistantAgent")

    An agent, used by MagenticOne that provides coding assistance using an LLM model client.

    The prompts and description are sealed, to replicate the original MagenticOne configuration. See AssistantAgent if you wish to modify these values.

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.agents.magentic\_one.MagenticOneCoderAgent'*[#](#autogen_ext.agents.magentic_one.MagenticOneCoderAgent.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

### autogen\_ext.agents.openai[#](#module-autogen_ext.agents.openai "Link to this heading")

*class* OpenAIAssistantAgent(*name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *client: AsyncOpenAI | AsyncAzureOpenAI*, *model: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *instructions: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *tools: [Iterable](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.13)")[[Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['code\_interpreter', 'file\_search'] | [Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[...], [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[...], [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *assistant\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *thread\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *metadata: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *response\_format: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['auto'] | ResponseFormatText | ResponseFormatJSONObject | ResponseFormatJSONSchema | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *temperature: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *tool\_resources: ToolResources | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *top\_p: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.agents.openai.OpenAIAssistantAgent "Link to this definition")
:   Bases: [`BaseChatAgent`](#autogen_agentchat.agents.BaseChatAgent "autogen_agentchat.agents._base_chat_agent.BaseChatAgent")

    An agent implementation that uses the Assistant API to generate responses.

    Installation:

    ```
    pip install "autogen-ext[openai]"
    # pip install "autogen-ext[openai,azure]"  # For Azure OpenAI Assistant

    ```

    This agent leverages the Assistant API to create AI assistants with capabilities like:

    * Code interpretation and execution
    * File handling and search
    * Custom function calling
    * Multi-turn conversations

    The agent maintains a thread of conversation and can use various tools including

    * Code interpreter: For executing code and working with files
    * File search: For searching through uploaded documents
    * Custom functions: For extending capabilities with user-defined tools

    Key Features:

    * Supports multiple file formats including code, documents, images
    * Can handle up to 128 tools per assistant
    * Maintains conversation context in threads
    * Supports file uploads for code interpreter and search
    * Vector store integration for efficient file search
    * Automatic file parsing and embedding

    You can use an existing thread or assistant by providing the thread\_id or assistant\_id parameters.

    Examples

    Use the assistant to analyze data in a CSV file:

    ```
    from openai import AsyncOpenAI
    from autogen_core import CancellationToken
    import asyncio
    from autogen_ext.agents.openai import OpenAIAssistantAgent
    from autogen_agentchat.messages import TextMessage

    async def example():
        cancellation_token = CancellationToken()

        # Create an OpenAI client
        client = AsyncOpenAI(api_key="your-api-key", base_url="your-base-url")

        # Create an assistant with code interpreter
        assistant = OpenAIAssistantAgent(
            name="Python Helper",
            description="Helps with Python programming",
            client=client,
            model="gpt-4",
            instructions="You are a helpful Python programming assistant.",
            tools=["code_interpreter"],
        )

        # Upload files for the assistant to use
        await assistant.on_upload_for_code_interpreter("data.csv", cancellation_token)

        # Get response from the assistant
        response = await assistant.on_messages(
            [TextMessage(source="user", content="Analyze the data in data.csv")], cancellation_token
        )

        print(response)

        # Clean up resources
        await assistant.delete_uploaded_files(cancellation_token)
        await assistant.delete_assistant(cancellation_token)

    asyncio.run(example())

    ```

    Use Azure OpenAI Assistant with AAD authentication:

    ```
    from openai import AsyncAzureOpenAI
    import asyncio
    from azure.identity import DefaultAzureCredential, get_bearer_token_provider
    from autogen_core import CancellationToken
    from autogen_ext.agents.openai import OpenAIAssistantAgent
    from autogen_agentchat.messages import TextMessage

    async def example():
        cancellation_token = CancellationToken()

        # Create an Azure OpenAI client
        token_provider = get_bearer_token_provider(DefaultAzureCredential())
        client = AsyncAzureOpenAI(
            azure_deployment="YOUR_AZURE_DEPLOYMENT",
            api_version="YOUR_API_VERSION",
            azure_endpoint="YOUR_AZURE_ENDPOINT",
            azure_ad_token_provider=token_provider,
        )

        # Create an assistant with code interpreter
        assistant = OpenAIAssistantAgent(
            name="Python Helper",
            description="Helps with Python programming",
            client=client,
            model="gpt-4o",
            instructions="You are a helpful Python programming assistant.",
            tools=["code_interpreter"],
        )

        # Get response from the assistant
        response = await assistant.on_messages([TextMessage(source="user", content="Hello.")], cancellation_token)

        print(response)

        # Clean up resources
        await assistant.delete_assistant(cancellation_token)

    asyncio.run(example())

    ```

    Parameters:
    :   * **name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – Name of the assistant
        * **description** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – Description of the assistant’s purpose
        * **client** (*AsyncOpenAI* *|* *AsyncAzureOpenAI*) – OpenAI client or Azure OpenAI client instance
        * **model** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – Model to use (e.g. “gpt-4”)
        * **instructions** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – System instructions for the assistant
        * **tools** (*Optional**[**Iterable**[**Union**[**Literal**[**"code\_interpreter"**,* *"file\_search"**]**,* *Tool* *|* *Callable**[**...**,* *Any**]* *|* *Callable**[**...**,* *Awaitable**[**Any**]**]**]**]**]*) – Tools the assistant can use
        * **assistant\_id** (*Optional**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]*) – ID of existing assistant to use
        * **thread\_id** (*Optional**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]*) – ID of existing thread to use
        * **metadata** (*Optional**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]**]*) – Additional metadata for the assistant.
        * **response\_format** (*Optional**[**AssistantResponseFormatOptionParam**]*) – Response format settings
        * **temperature** (*Optional**[*[*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*]*) – Temperature for response generation
        * **tool\_resources** (*Optional**[**ToolResources**]*) – Additional tool configuration
        * **top\_p** (*Optional**[*[*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*]*) – Top p sampling parameter

    *async* delete\_assistant(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.openai.OpenAIAssistantAgent.delete_assistant "Link to this definition")
    :   Delete the assistant if it was created by this instance.

    *async* delete\_uploaded\_files(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.openai.OpenAIAssistantAgent.delete_uploaded_files "Link to this definition")
    :   Delete all files that were uploaded by this agent instance.

    *async* delete\_vector\_store(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.openai.OpenAIAssistantAgent.delete_vector_store "Link to this definition")
    :   Delete the vector store if it was created by this instance.

    *async* handle\_text\_message(*content: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.openai.OpenAIAssistantAgent.handle_text_message "Link to this definition")
    :   Handle regular text messages by adding them to the thread.

    *async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.openai.OpenAIAssistantAgent.load_state "Link to this definition")
    :   Restore agent from saved state. Default implementation for stateless agents.

    *property* messages*: AsyncMessages*[#](#autogen_ext.agents.openai.OpenAIAssistantAgent.messages "Link to this definition")

    *async* on\_messages(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response")[#](#autogen_ext.agents.openai.OpenAIAssistantAgent.on_messages "Link to this definition")
    :   Handle incoming messages and return a response.

    *async* on\_messages\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_ext.agents.openai.OpenAIAssistantAgent.on_messages_stream "Link to this definition")
    :   Handle incoming messages and return a response.

    *async* on\_reset(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.openai.OpenAIAssistantAgent.on_reset "Link to this definition")
    :   Handle reset command by deleting new messages and runs since initialization.

    *async* on\_upload\_for\_code\_interpreter(*file\_paths: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Iterable](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.openai.OpenAIAssistantAgent.on_upload_for_code_interpreter "Link to this definition")
    :   Handle file uploads for the code interpreter.

    *async* on\_upload\_for\_file\_search(*file\_paths: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Iterable](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.openai.OpenAIAssistantAgent.on_upload_for_file_search "Link to this definition")
    :   Handle file uploads for file search.

    *property* produced\_message\_types*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]*[#](#autogen_ext.agents.openai.OpenAIAssistantAgent.produced_message_types "Link to this definition")
    :   The types of messages that the assistant agent produces.

    *property* runs*: AsyncRuns*[#](#autogen_ext.agents.openai.OpenAIAssistantAgent.runs "Link to this definition")

    *async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_ext.agents.openai.OpenAIAssistantAgent.save_state "Link to this definition")
    :   Export state. Default implementation for stateless agents.

    *property* threads*: AsyncThreads*[#](#autogen_ext.agents.openai.OpenAIAssistantAgent.threads "Link to this definition")

### autogen\_ext.agents.web\_surfer[#](#module-autogen_ext.agents.web_surfer "Link to this heading")

*class* MultimodalWebSurfer(*name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *model\_client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *downloads\_folder: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = DEFAULT\_DESCRIPTION*, *debug\_dir: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *headless: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*, *start\_page: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = DEFAULT\_START\_PAGE*, *animate\_actions: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*, *to\_save\_screenshots: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*, *use\_ocr: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*, *browser\_channel: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *browser\_data\_dir: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *to\_resize\_viewport: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*, *playwright: Playwright | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *context: BrowserContext | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer "Link to this definition")
:   Bases: [`BaseChatAgent`](#autogen_agentchat.agents.BaseChatAgent "autogen_agentchat.agents._base_chat_agent.BaseChatAgent"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`MultimodalWebSurferConfig`]

    MultimodalWebSurfer is a multimodal agent that acts as a web surfer that can search the web and visit web pages.

    Installation:

    ```
    pip install "autogen-ext[web-surfer]"

    ```

    It launches a chromium browser and allows the playwright to interact with the web browser and can perform a variety of actions. The browser is launched on the first call to the agent and is reused for subsequent calls.

    It must be used with a multimodal model client that supports function/tool calling, ideally GPT-4o currently.

    When [`on_messages()`](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.on_messages "autogen_ext.agents.web_surfer.MultimodalWebSurfer.on_messages") or [`on_messages_stream()`](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.on_messages_stream "autogen_ext.agents.web_surfer.MultimodalWebSurfer.on_messages_stream") is called, the following occurs:
    :   1. If this is the first call, the browser is initialized and the page is loaded. This is done in `_lazy_init()`. The browser is only closed when [`close()`](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.close "autogen_ext.agents.web_surfer.MultimodalWebSurfer.close") is called.
        2. The method `_generate_reply()` is called, which then creates the final response as below.
        3. The agent takes a screenshot of the page, extracts the interactive elements, and prepares a set-of-mark screenshot with bounding boxes around the interactive elements.
        4. The agent makes a call to the `model_client` with the SOM screenshot, history of messages, and the list of available tools.
           :   * If the model returns a string, the agent returns the string as the final response.
               * If the model returns a list of tool calls, the agent executes the tool calls with `_execute_tool()` using `_playwright_controller`.
               * The agent returns a final response which includes a screenshot of the page, page metadata, description of the action taken and the inner text of the webpage.
        5. If at any point the agent encounters an error, it returns the error message as the final response.

    Note

    Please note that using the MultimodalWebSurfer involves interacting with a digital world designed for humans, which carries inherent risks.
    Be aware that agents may occasionally attempt risky actions, such as recruiting humans for help or accepting cookie agreements without human involvement. Always ensure agents are monitored and operate within a controlled environment to prevent unintended consequences.
    Moreover, be cautious that MultimodalWebSurfer may be susceptible to prompt injection attacks from webpages.

    Note

    On Windows, the event loop policy must be set to WindowsProactorEventLoopPolicy to avoid issues with subprocesses.

    ```
    import sys
    import asyncio

    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

    ```

    Parameters:
    :   * **name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The name of the agent.
        * **model\_client** ([*ChatCompletionClient*](#autogen_core.models.ChatCompletionClient "autogen_core.models.ChatCompletionClient")) – The model client used by the agent. Must be multimodal and support function calling.
        * **downloads\_folder** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The folder where downloads are saved. Defaults to None, no downloads are saved.
        * **description** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The description of the agent. Defaults to MultimodalWebSurfer.DEFAULT\_DESCRIPTION.
        * **debug\_dir** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The directory where debug information is saved. Defaults to None.
        * **headless** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – Whether the browser should be headless. Defaults to True.
        * **start\_page** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The start page for the browser. Defaults to MultimodalWebSurfer.DEFAULT\_START\_PAGE.
        * **animate\_actions** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – Whether to animate actions. Defaults to False.
        * **to\_save\_screenshots** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – Whether to save screenshots. Defaults to False.
        * **use\_ocr** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – Whether to use OCR. Defaults to False.
        * **browser\_channel** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The browser channel. Defaults to None.
        * **browser\_data\_dir** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The browser data directory. Defaults to None.
        * **to\_resize\_viewport** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – Whether to resize the viewport. Defaults to True.
        * **playwright** (*Playwright**,* *optional*) – The playwright instance. Defaults to None.
        * **context** (*BrowserContext**,* *optional*) – The browser context. Defaults to None.

    Example usage:

    The following example demonstrates how to create a web surfing agent with
    a model client and run it for multiple turns.

    > ```
    > import asyncio
    > from autogen_agentchat.ui import Console
    > from autogen_agentchat.teams import RoundRobinGroupChat
    > from autogen_ext.models.openai import OpenAIChatCompletionClient
    > from autogen_ext.agents.web_surfer import MultimodalWebSurfer
    >
    >
    > async def main() -> None:
    >     # Define an agent
    >     web_surfer_agent = MultimodalWebSurfer(
    >         name="MultimodalWebSurfer",
    >         model_client=OpenAIChatCompletionClient(model="gpt-4o-2024-08-06"),
    >     )
    >
    >     # Define a team
    >     agent_team = RoundRobinGroupChat([web_surfer_agent], max_turns=3)
    >
    >     # Run the team and stream messages to the console
    >     stream = agent_team.run_stream(task="Navigate to the AutoGen readme on GitHub.")
    >     await Console(stream)
    >     # Close the browser controlled by the agent
    >     await web_surfer_agent.close()
    >
    >
    > asyncio.run(main())
    >
    > ```

    DEFAULT\_DESCRIPTION *= '\n    A helpful assistant with access to a web browser.\n    Ask them to perform web searches, open pages, and interact with content (e.g., clicking links, scrolling the viewport, filling in form fields, etc.).\n    It can also summarize the entire page, or answer questions based on the content of the page.\n    It can also be asked to sleep and wait for pages to load, in cases where the page seems not yet fully loaded.\n    '*[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.DEFAULT_DESCRIPTION "Link to this definition")

    DEFAULT\_START\_PAGE *= 'https://www.bing.com/'*[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.DEFAULT_START_PAGE "Link to this definition")

    MLM\_HEIGHT *= 765*[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.MLM_HEIGHT "Link to this definition")

    MLM\_WIDTH *= 1224*[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.MLM_WIDTH "Link to this definition")

    SCREENSHOT\_TOKENS *= 1105*[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.SCREENSHOT_TOKENS "Link to this definition")

    VIEWPORT\_HEIGHT *= 900*[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.VIEWPORT_HEIGHT "Link to this definition")

    VIEWPORT\_WIDTH *= 1440*[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.VIEWPORT_WIDTH "Link to this definition")

    *classmethod* \_from\_config(*config: MultimodalWebSurferConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → MultimodalWebSurferConfig[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.close "Link to this definition")
    :   Close the browser and the page.
        Should be called when the agent is no longer needed.

    component\_config\_schema[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.component_config_schema "Link to this definition")
    :   alias of `MultimodalWebSurferConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.agents.web\_surfer.MultimodalWebSurfer'*[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    component\_type*: ClassVar[ComponentType]* *= 'agent'*[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.component_type "Link to this definition")
    :   The logical type of the component.

    *async* on\_messages(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response")[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.on_messages "Link to this definition")
    :   Handles incoming messages and returns a response.

        Note

        Agents are stateful and the messages passed to this method should
        be the new messages since the last call to this method. The agent
        should maintain its state between calls to this method. For example,
        if the agent needs to remember the previous messages to respond to
        the current message, it should store the previous messages in the
        agent state.

    *async* on\_messages\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[ToolCallRequestEvent](#autogen_agentchat.messages.ToolCallRequestEvent "autogen_agentchat.messages.ToolCallRequestEvent") | [ToolCallExecutionEvent](#autogen_agentchat.messages.ToolCallExecutionEvent "autogen_agentchat.messages.ToolCallExecutionEvent") | [MemoryQueryEvent](#autogen_agentchat.messages.MemoryQueryEvent "autogen_agentchat.messages.MemoryQueryEvent") | [UserInputRequestedEvent](#autogen_agentchat.messages.UserInputRequestedEvent "autogen_agentchat.messages.UserInputRequestedEvent") | [ModelClientStreamingChunkEvent](#autogen_agentchat.messages.ModelClientStreamingChunkEvent "autogen_agentchat.messages.ModelClientStreamingChunkEvent") | [ThoughtEvent](#autogen_agentchat.messages.ThoughtEvent "autogen_agentchat.messages.ThoughtEvent"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')] | [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.on_messages_stream "Link to this definition")
    :   Handles incoming messages and returns a stream of messages and
        and the final item is the response. The base implementation in
        `BaseChatAgent` simply calls [`on_messages()`](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.on_messages "autogen_ext.agents.web_surfer.MultimodalWebSurfer.on_messages") and yields
        the messages in the response.

        Note

        Agents are stateful and the messages passed to this method should
        be the new messages since the last call to this method. The agent
        should maintain its state between calls to this method. For example,
        if the agent needs to remember the previous messages to respond to
        the current message, it should store the previous messages in the
        agent state.

    *async* on\_reset(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.on_reset "Link to this definition")
    :   Resets the agent to its initialization state.

    *property* produced\_message\_types*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]*[#](#autogen_ext.agents.web_surfer.MultimodalWebSurfer.produced_message_types "Link to this definition")
    :   The types of messages that the agent produces in the
        `Response.chat_message` field. They must be `ChatMessage` types.

*class* PlaywrightController(*downloads\_folder: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *animate\_actions: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*, *viewport\_width: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 1440*, *viewport\_height: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 900*, *\_download\_handler: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[Download], [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *to\_resize\_viewport: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*)[#](#autogen_ext.agents.web_surfer.PlaywrightController "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    A helper class to allow Playwright to interact with web pages to perform actions such as clicking, filling, and scrolling.

    Parameters:
    :   * **downloads\_folder** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *|* *None*) – The folder to save downloads to. If None, downloads are not saved.
        * **animate\_actions** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")) – Whether to animate the actions (create fake cursor to click).
        * **viewport\_width** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The width of the viewport.
        * **viewport\_height** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The height of the viewport.
        * **\_download\_handler** (*Optional**[**Callable**[**[**Download**]**,* *None**]**]*) – A function to handle downloads.
        * **to\_resize\_viewport** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")) – Whether to resize the viewport

    *async* add\_cursor\_box(*page: Page*, *identifier: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.add_cursor_box "Link to this definition")
    :   Add a red cursor box around the element with the given identifier.

        Parameters:
        :   * **page** (*Page*) – The Playwright page object.
            * **identifier** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The element identifier.

    *async* back(*page: Page*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.back "Link to this definition")
    :   Navigate back to the previous page.

        Parameters:
        :   **page** (*Page*) – The Playwright page object.

    *async* click\_id(*page: Page*, *identifier: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → Page | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.click_id "Link to this definition")
    :   Click the element with the given identifier.

        Parameters:
        :   * **page** (*Page*) – The Playwright page object.
            * **identifier** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The element identifier.

        Returns:
        :   **Page | None** – The new page if a new page is opened, otherwise None.

    *async* fill\_id(*page: Page*, *identifier: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *value: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *press\_enter: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.fill_id "Link to this definition")
    :   Fill the element with the given identifier with the specified value.

        Parameters:
        :   * **page** (*Page*) – The Playwright page object.
            * **identifier** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The element identifier.
            * **value** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The value to fill.

    *async* get\_focused\_rect\_id(*page: Page*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.get_focused_rect_id "Link to this definition")
    :   Retrieve the ID of the currently focused element.

        Parameters:
        :   **page** (*Page*) – The Playwright page object.

        Returns:
        :   **str** – The ID of the focused element or None if no control has focus.

    *async* get\_interactive\_rects(*page: Page*) → [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), InteractiveRegion][#](#autogen_ext.agents.web_surfer.PlaywrightController.get_interactive_rects "Link to this definition")
    :   Retrieve interactive regions from the web page.

        Parameters:
        :   **page** (*Page*) – The Playwright page object.

        Returns:
        :   **Dict[str, InteractiveRegion]** – A dictionary of interactive regions.

    *async* get\_page\_markdown(*page: Page*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.get_page_markdown "Link to this definition")
    :   Retrieve the markdown content of the web page.
        Currently not implemented.

        Parameters:
        :   **page** (*Page*) – The Playwright page object.

        Returns:
        :   **str** – The markdown content of the page.

    *async* get\_page\_metadata(*page: Page*) → [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_ext.agents.web_surfer.PlaywrightController.get_page_metadata "Link to this definition")
    :   Retrieve metadata from the web page.

        Parameters:
        :   **page** (*Page*) – The Playwright page object.

        Returns:
        :   **Dict[str, Any]** – A dictionary of page metadata.

    *async* get\_visible\_text(*page: Page*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.get_visible_text "Link to this definition")
    :   Retrieve the text content of the browser viewport (approximately).

        Parameters:
        :   **page** (*Page*) – The Playwright page object.

        Returns:
        :   **str** – The text content of the page.

    *async* get\_visual\_viewport(*page: Page*) → VisualViewport[#](#autogen_ext.agents.web_surfer.PlaywrightController.get_visual_viewport "Link to this definition")
    :   Retrieve the visual viewport of the web page.

        Parameters:
        :   **page** (*Page*) – The Playwright page object.

        Returns:
        :   **VisualViewport** – The visual viewport of the page.

    *async* get\_webpage\_text(*page: Page*, *n\_lines: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 50*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.get_webpage_text "Link to this definition")
    :   Retrieve the text content of the web page.

        Parameters:
        :   * **page** (*Page*) – The Playwright page object.
            * **n\_lines** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The number of lines to return from the page inner text.

        Returns:
        :   **str** – The text content of the page.

    *async* gradual\_cursor\_animation(*page: Page*, *start\_x: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*, *start\_y: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*, *end\_x: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*, *end\_y: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.gradual_cursor_animation "Link to this definition")
    :   Animate the cursor movement gradually from start to end coordinates.

        Parameters:
        :   * **page** (*Page*) – The Playwright page object.
            * **start\_x** ([*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")) – The starting x-coordinate.
            * **start\_y** ([*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")) – The starting y-coordinate.
            * **end\_x** ([*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")) – The ending x-coordinate.
            * **end\_y** ([*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")) – The ending y-coordinate.

    *async* hover\_id(*page: Page*, *identifier: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.hover_id "Link to this definition")
    :   Hover the mouse over the element with the given identifier.

        Parameters:
        :   * **page** (*Page*) – The Playwright page object.
            * **identifier** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The element identifier.

    *async* on\_new\_page(*page: Page*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.on_new_page "Link to this definition")
    :   Handle actions to perform on a new page.

        Parameters:
        :   **page** (*Page*) – The Playwright page object.

    *async* page\_down(*page: Page*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.page_down "Link to this definition")
    :   Scroll the page down by one viewport height minus 50 pixels.

        Parameters:
        :   **page** (*Page*) – The Playwright page object.

    *async* page\_up(*page: Page*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.page_up "Link to this definition")
    :   Scroll the page up by one viewport height minus 50 pixels.

        Parameters:
        :   **page** (*Page*) – The Playwright page object.

    *async* remove\_cursor\_box(*page: Page*, *identifier: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.remove_cursor_box "Link to this definition")
    :   Remove the red cursor box around the element with the given identifier.

        Parameters:
        :   * **page** (*Page*) – The Playwright page object.
            * **identifier** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The element identifier.

    *async* scroll\_id(*page: Page*, *identifier: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *direction: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.scroll_id "Link to this definition")
    :   Scroll the element with the given identifier in the specified direction.

        Parameters:
        :   * **page** (*Page*) – The Playwright page object.
            * **identifier** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The element identifier.
            * **direction** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The direction to scroll (“up” or “down”).

    *async* sleep(*page: Page*, *duration: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.web_surfer.PlaywrightController.sleep "Link to this definition")
    :   Pause the execution for a specified duration.

        Parameters:
        :   * **page** (*Page*) – The Playwright page object.
            * **duration** (*Union**[*[*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*,* [*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*]*) – The duration to sleep in milliseconds.

    *async* visit\_page(*page: Page*, *url: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [Tuple](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")[[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)"), [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")][#](#autogen_ext.agents.web_surfer.PlaywrightController.visit_page "Link to this definition")
    :   Visit a specified URL.

        Parameters:
        :   * **page** (*Page*) – The Playwright page object.
            * **url** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The URL to visit.

        Returns:
        :   **Tuple[bool, bool]** – A tuple indicating whether to reset prior metadata hash and last download.

### autogen\_ext.agents.file\_surfer[#](#module-autogen_ext.agents.file_surfer "Link to this heading")

*class* FileSurfer(*name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *model\_client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = DEFAULT\_DESCRIPTION*, *base\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = os.getcwd()*)[#](#autogen_ext.agents.file_surfer.FileSurfer "Link to this definition")
:   Bases: [`BaseChatAgent`](#autogen_agentchat.agents.BaseChatAgent "autogen_agentchat.agents._base_chat_agent.BaseChatAgent"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`FileSurferConfig`]

    An agent, used by MagenticOne, that acts as a local file previewer. FileSurfer can open and read a variety of common file types, and can navigate the local file hierarchy.

    Installation:

    ```
    pip install "autogen-ext[file-surfer]"

    ```

    Parameters:
    :   * **name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The agent’s name
        * **model\_client** ([*ChatCompletionClient*](#autogen_core.models.ChatCompletionClient "autogen_core.models.ChatCompletionClient")) – The model to use (must be tool-use enabled)
        * **description** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The agent’s description used by the team. Defaults to DEFAULT\_DESCRIPTION
        * **base\_path** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The base path to use for the file browser. Defaults to the current working directory.

    DEFAULT\_DESCRIPTION *= 'An agent that can handle local files.'*[#](#autogen_ext.agents.file_surfer.FileSurfer.DEFAULT_DESCRIPTION "Link to this definition")

    DEFAULT\_SYSTEM\_MESSAGES *= [SystemMessage(content='\n        You are a helpful AI Assistant.\n        When given a user query, use available functions to help the user with their request.', type='SystemMessage')]*[#](#autogen_ext.agents.file_surfer.FileSurfer.DEFAULT_SYSTEM_MESSAGES "Link to this definition")

    *classmethod* \_from\_config(*config: FileSurferConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_ext.agents.file_surfer.FileSurfer._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → FileSurferConfig[#](#autogen_ext.agents.file_surfer.FileSurfer._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_ext.agents.file_surfer.FileSurfer.component_config_schema "Link to this definition")
    :   alias of `FileSurferConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.agents.file\_surfer.FileSurfer'*[#](#autogen_ext.agents.file_surfer.FileSurfer.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* on\_messages(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [Response](#autogen_agentchat.base.Response "autogen_agentchat.base._chat_agent.Response")[#](#autogen_ext.agents.file_surfer.FileSurfer.on_messages "Link to this definition")
    :   Handles incoming messages and returns a response.

        Note

        Agents are stateful and the messages passed to this method should
        be the new messages since the last call to this method. The agent
        should maintain its state between calls to this method. For example,
        if the agent needs to remember the previous messages to respond to
        the current message, it should store the previous messages in the
        agent state.

    *async* on\_reset(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.file_surfer.FileSurfer.on_reset "Link to this definition")
    :   Resets the agent to its initialization state.

    *property* produced\_message\_types*: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[TextMessage](#autogen_agentchat.messages.TextMessage "autogen_agentchat.messages.TextMessage") | [MultiModalMessage](#autogen_agentchat.messages.MultiModalMessage "autogen_agentchat.messages.MultiModalMessage") | [StopMessage](#autogen_agentchat.messages.StopMessage "autogen_agentchat.messages.StopMessage") | [ToolCallSummaryMessage](#autogen_agentchat.messages.ToolCallSummaryMessage "autogen_agentchat.messages.ToolCallSummaryMessage") | [HandoffMessage](#autogen_agentchat.messages.HandoffMessage "autogen_agentchat.messages.HandoffMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]*[#](#autogen_ext.agents.file_surfer.FileSurfer.produced_message_types "Link to this definition")
    :   The types of messages that the agent produces in the
        `Response.chat_message` field. They must be `ChatMessage` types.

### autogen\_ext.agents.video\_surfer[#](#module-autogen_ext.agents.video_surfer "Link to this heading")

*class* VideoSurfer(*name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *model\_client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *\**, *tools: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[BaseTool](#autogen_core.tools.BaseTool "autogen_core.tools._base.BaseTool")[BaseModel, BaseModel] | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[...], [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[...], [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *system\_message: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.agents.video_surfer.VideoSurfer "Link to this definition")
:   Bases: [`AssistantAgent`](#autogen_agentchat.agents.AssistantAgent "autogen_agentchat.agents._assistant_agent.AssistantAgent")

    VideoSurfer is a specialized agent designed to answer questions about a local video file.

    Installation:

    ```
    pip install "autogen-ext[video-surfer]"

    ```

    This agent utilizes various tools to extract information from the video, such as its length, screenshots at specific timestamps, and audio transcriptions. It processes these elements to provide detailed answers to user queries.

    Available tools:

    * [`extract_audio()`](#autogen_ext.agents.video_surfer.tools.extract_audio "autogen_ext.agents.video_surfer.tools.extract_audio")
    * [`get_video_length()`](#autogen_ext.agents.video_surfer.tools.get_video_length "autogen_ext.agents.video_surfer.tools.get_video_length")
    * [`transcribe_audio_with_timestamps()`](#autogen_ext.agents.video_surfer.tools.transcribe_audio_with_timestamps "autogen_ext.agents.video_surfer.tools.transcribe_audio_with_timestamps")
    * [`get_screenshot_at()`](#autogen_ext.agents.video_surfer.tools.get_screenshot_at "autogen_ext.agents.video_surfer.tools.get_screenshot_at")
    * [`save_screenshot()`](#autogen_ext.agents.video_surfer.tools.save_screenshot "autogen_ext.agents.video_surfer.tools.save_screenshot")
    * [`transcribe_video_screenshot()`](#autogen_ext.agents.video_surfer.tools.transcribe_video_screenshot "autogen_ext.agents.video_surfer.tools.transcribe_video_screenshot")

    Parameters:
    :   * **name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The name of the agent.
        * **model\_client** ([*ChatCompletionClient*](#autogen_core.models.ChatCompletionClient "autogen_core.models.ChatCompletionClient")) – The model client used for generating responses.
        * **tools** (*List**[*[*BaseTool*](#autogen_core.tools.BaseTool "autogen_core.tools.BaseTool")*[**BaseModel**,* *BaseModel**]* *|* *Callable**[**...**,* *Any**]* *|* *Callable**[**...**,* *Awaitable**[**Any**]**]**]* *|* *None**,* *optional*) – A list of tools or functions the agent can use. If not provided, defaults to all video tools from the action space.
        * **description** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – A brief description of the agent. Defaults to “An agent that can answer questions about a local video.”.
        * **system\_message** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *|* *None**,* *optional*) – The system message guiding the agent’s behavior. Defaults to a predefined message.

    Example usage:

    The following example demonstrates how to create an video surfing agent with
    a model client and generate a response to a simple query about a local video
    called video.mp4.

    > ```
    > import asyncio
    > from autogen_agentchat.ui import Console
    > from autogen_agentchat.conditions import TextMentionTermination
    > from autogen_agentchat.teams import RoundRobinGroupChat
    > from autogen_ext.models.openai import OpenAIChatCompletionClient
    > from autogen_ext.agents.video_surfer import VideoSurfer
    >
    > async def main() -> None:
    >     """
    >     Main function to run the video agent.
    >     """
    >     # Define an agent
    >     video_agent = VideoSurfer(
    >         name="VideoSurfer",
    >         model_client=OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")
    >         )
    >
    >     # Define termination condition
    >     termination = TextMentionTermination("TERMINATE")
    >
    >     # Define a team
    >     agent_team = RoundRobinGroupChat([video_agent], termination_condition=termination)
    >
    >     # Run the team and stream messages to the console
    >     stream = agent_team.run_stream(task="How does Adam define complex tasks in video.mp4? What concrete example of complex does his use? Can you save this example to disk as well?")
    >     await Console(stream)
    >
    > asyncio.run(main())
    >
    > ```

    The following example demonstrates how to create and use a VideoSurfer and UserProxyAgent with MagenticOneGroupChat.

    > ```
    > import asyncio
    >
    > from autogen_agentchat.ui import Console
    > from autogen_agentchat.teams import MagenticOneGroupChat
    > from autogen_agentchat.agents import UserProxyAgent
    > from autogen_ext.models.openai import OpenAIChatCompletionClient
    > from autogen_ext.agents.video_surfer import VideoSurfer
    >
    > async def main() -> None:
    >     """
    >     Main function to run the video agent.
    >     """
    >
    >     model_client = OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")
    >
    >     # Define an agent
    >     video_agent = VideoSurfer(
    >         name="VideoSurfer",
    >         model_client=model_client
    >         )
    >
    >     web_surfer_agent = UserProxyAgent(
    >         name="User"
    >     )
    >
    >     # Define a team
    >     agent_team = MagenticOneGroupChat([web_surfer_agent, video_agent], model_client=model_client,)
    >
    >     # Run the team and stream messages to the console
    >     stream = agent_team.run_stream(task="Find a latest video about magentic one on youtube and extract quotes from it that make sense.")
    >     await Console(stream)
    >
    > asyncio.run(main())
    >
    > ```

    DEFAULT\_DESCRIPTION *= 'An agent that can answer questions about a local video.'*[#](#autogen_ext.agents.video_surfer.VideoSurfer.DEFAULT_DESCRIPTION "Link to this definition")

    DEFAULT\_SYSTEM\_MESSAGE *= '\n    You are a helpful agent that is an expert at answering questions from a video.\n    When asked to answer a question about a video, you should:\n    1. Check if that video is available locally.\n    2. Use the transcription to find which part of the video the question is referring to.\n    3. Optionally use screenshots from those timestamps\n    4. Provide a detailed answer to the question.\n    Reply with TERMINATE when the task has been completed.\n    '*[#](#autogen_ext.agents.video_surfer.VideoSurfer.DEFAULT_SYSTEM_MESSAGE "Link to this definition")

    *async* vs\_transribe\_video\_screenshot(*video\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *timestamp: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.agents.video_surfer.VideoSurfer.vs_transribe_video_screenshot "Link to this definition")
    :   Transcribes the video screenshot at a specific timestamp.

        Parameters:
        :   * **video\_path** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – Path to the video file.
            * **timestamp** ([*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")) – Timestamp to take the screenshot.

        Returns:
        :   **str** – Transcription of the video screenshot.

### autogen\_ext.agents.video\_surfer.tools[#](#module-autogen_ext.agents.video_surfer.tools "Link to this heading")

extract\_audio(*video\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *audio\_output\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.agents.video_surfer.tools.extract_audio "Link to this definition")
:   Extracts audio from a video file and saves it as an MP3 file.

    Parameters:
    :   * **video\_path** – Path to the video file.
        * **audio\_output\_path** – Path to save the extracted audio file.

    Returns:
    :   Confirmation message with the path to the saved audio file.

get\_screenshot\_at(*video\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *timestamps: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")]*) → [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Tuple](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")[[float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)"), ndarray[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]]][#](#autogen_ext.agents.video_surfer.tools.get_screenshot_at "Link to this definition")
:   Captures screenshots at the specified timestamps and returns them as Python objects.

    Parameters:
    :   * **video\_path** – Path to the video file.
        * **timestamps** – List of timestamps in seconds.

    Returns:
    :   List of tuples containing timestamp and the corresponding frame (image).
        Each frame is a NumPy array (height x width x channels).

get\_video\_length(*video\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.agents.video_surfer.tools.get_video_length "Link to this definition")
:   Returns the length of the video in seconds.

    Parameters:
    :   **video\_path** – Path to the video file.

    Returns:
    :   Duration of the video in seconds.

save\_screenshot(*video\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *timestamp: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*, *output\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.agents.video_surfer.tools.save_screenshot "Link to this definition")
:   Captures a screenshot at the specified timestamp and saves it to the output path.

    Parameters:
    :   * **video\_path** – Path to the video file.
        * **timestamp** – Timestamp in seconds.
        * **output\_path** – Path to save the screenshot. The file format is determined by the extension in the path.

transcribe\_audio\_with\_timestamps(*audio\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.agents.video_surfer.tools.transcribe_audio_with_timestamps "Link to this definition")
:   Transcribes the audio file with timestamps using the Whisper model.

    Parameters:
    :   **audio\_path** – Path to the audio file.

    Returns:
    :   Transcription with timestamps.

*async* transcribe\_video\_screenshot(*video\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *timestamp: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*, *model\_client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.agents.video_surfer.tools.transcribe_video_screenshot "Link to this definition")
:   Transcribes the content of a video screenshot captured at the specified timestamp using OpenAI API.

    Parameters:
    :   * **video\_path** – Path to the video file.
        * **timestamp** – Timestamp in seconds.
        * **model\_client** – ChatCompletionClient instance.

    Returns:
    :   Description of the screenshot content.

### autogen\_ext.teams.magentic\_one[#](#module-autogen_ext.teams.magentic_one "Link to this heading")

*class* MagenticOne(*client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *hil\_mode: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*, *input\_func: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")], [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")], [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *code\_executor: [CodeExecutor](#autogen_core.code_executor.CodeExecutor "autogen_core.code_executor._base.CodeExecutor") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.teams.magentic_one.MagenticOne "Link to this definition")
:   Bases: [`MagenticOneGroupChat`](#autogen_agentchat.teams.MagenticOneGroupChat "autogen_agentchat.teams._group_chat._magentic_one._magentic_one_group_chat.MagenticOneGroupChat")

    MagenticOne is a specialized group chat class that integrates various agents
    such as FileSurfer, WebSurfer, Coder, and Executor to solve complex tasks.
    To read more about the science behind Magentic-One, see the full blog post: [Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks) and the references below.

    Installation:

    ```
    pip install "autogen-ext[magentic-one]"

    ```

    Parameters:
    :   * **client** ([*ChatCompletionClient*](#autogen_core.models.ChatCompletionClient "autogen_core.models.ChatCompletionClient")) – The client used for model interactions.
        * **hil\_mode** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")) – Optional; If set to True, adds the UserProxyAgent to the list of agents.

    Warning

    Using Magentic-One involves interacting with a digital world designed for humans, which carries inherent risks. To minimize these risks, consider the following precautions:

    1. **Use Containers**: Run all tasks in docker containers to isolate the agents and prevent direct system attacks.
    2. **Virtual Environment**: Use a virtual environment to run the agents and prevent them from accessing sensitive data.
    3. **Monitor Logs**: Closely monitor logs during and after execution to detect and mitigate risky behavior.
    4. **Human Oversight**: Run the examples with a human in the loop to supervise the agents and prevent unintended consequences.
    5. **Limit Access**: Restrict the agents’ access to the internet and other resources to prevent unauthorized actions.
    6. **Safeguard Data**: Ensure that the agents do not have access to sensitive data or resources that could be compromised. Do not share sensitive information with the agents.

    Be aware that agents may occasionally attempt risky actions, such as recruiting humans for help or accepting cookie agreements without human involvement. Always ensure agents are monitored and operate within a controlled environment to prevent unintended consequences. Moreover, be cautious that Magentic-One may be susceptible to prompt injection attacks from webpages.

    Architecture:

    Magentic-One is a generalist multi-agent system for solving open-ended web and file-based tasks across a variety of domains. It represents a significant step towards developing agents that can complete tasks that people encounter in their work and personal lives.

    Magentic-One work is based on a multi-agent architecture where a lead Orchestrator agent is responsible for high-level planning, directing other agents, and tracking task progress. The Orchestrator begins by creating a plan to tackle the task, gathering needed facts and educated guesses in a Task Ledger that is maintained. At each step of its plan, the Orchestrator creates a Progress Ledger where it self-reflects on task progress and checks whether the task is completed. If the task is not yet completed, it assigns one of Magentic-One’s other agents a subtask to complete. After the assigned agent completes its subtask, the Orchestrator updates the Progress Ledger and continues in this way until the task is complete. If the Orchestrator finds that progress is not being made for enough steps, it can update the Task Ledger and create a new plan.

    Overall, Magentic-One consists of the following agents:

    * Orchestrator: The lead agent responsible for task decomposition and planning, directing other agents in executing subtasks, tracking overall progress, and taking corrective actions as needed.
    * WebSurfer: An LLM-based agent proficient in commanding and managing the state of a Chromium-based web browser. It performs actions on the browser and reports on the new state of the web page.
    * FileSurfer: An LLM-based agent that commands a markdown-based file preview application to read local files of most types. It can also perform common navigation tasks such as listing the contents of directories and navigating a folder structure.
    * Coder: An LLM-based agent specialized in writing code, analyzing information collected from other agents, or creating new artifacts.
    * ComputerTerminal: Provides the team with access to a console shell where the Coder’s programs can be executed, and where new programming libraries can be installed.

    Together, Magentic-One’s agents provide the Orchestrator with the tools and capabilities needed to solve a broad variety of open-ended problems, as well as the ability to autonomously adapt to, and act in, dynamic and ever-changing web and file-system environments.

    Examples

    ```
    # Autonomously complete a coding task:
    import asyncio
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_ext.teams.magentic_one import MagenticOne
    from autogen_agentchat.ui import Console

    async def example_usage():
        client = OpenAIChatCompletionClient(model="gpt-4o")
        m1 = MagenticOne(client=client)
        task = "Write a Python script to fetch data from an API."
        result = await Console(m1.run_stream(task=task))
        print(result)

    if __name__ == "__main__":
        asyncio.run(example_usage())

    ```

    ```
    # Enable human-in-the-loop mode
    import asyncio
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_ext.teams.magentic_one import MagenticOne
    from autogen_agentchat.ui import Console

    async def example_usage_hil():
        client = OpenAIChatCompletionClient(model="gpt-4o")
        # to enable human-in-the-loop mode, set hil_mode=True
        m1 = MagenticOne(client=client, hil_mode=True)
        task = "Write a Python script to fetch data from an API."
        result = await Console(m1.run_stream(task=task))
        print(result)

    if __name__ == "__main__":
        asyncio.run(example_usage_hil())

    ```

    References

    ```
    @article{fourney2024magentic,
        title={Magentic-one: A generalist multi-agent system for solving complex tasks},
        author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others},
        journal={arXiv preprint arXiv:2411.04468},
        year={2024},
        url={https://arxiv.org/abs/2411.04468}
    }

    ```

### autogen\_ext.models.cache[#](#module-autogen_ext.models.cache "Link to this heading")

*class* ChatCompletionCache(*client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *store: [CacheStore](#autogen_core.CacheStore "autogen_core._cache_store.CacheStore")[[CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult") | [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult")]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.models.cache.ChatCompletionCache "Link to this definition")
:   Bases: [`ChatCompletionClient`](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`ChatCompletionCacheConfig`]

    A wrapper around a `ChatCompletionClient` that caches
    creation results from an underlying client.
    Cache hits do not contribute to token usage of the original client.

    Typical Usage:

    Lets use caching on disk with openai client as an example.
    First install autogen-ext with the required packages:

    ```
    pip install -U "autogen-ext[openai, diskcache]"

    ```

    And use it as:

    ```
    import asyncio
    import tempfile

    from autogen_core.models import UserMessage
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE
    from autogen_ext.cache_store.diskcache import DiskCacheStore
    from diskcache import Cache

    async def main():
        with tempfile.TemporaryDirectory() as tmpdirname:
            # Initialize the original client
            openai_model_client = OpenAIChatCompletionClient(model="gpt-4o")

            # Then initialize the CacheStore, in this case with diskcache.Cache.
            # You can also use redis like:
            # from autogen_ext.cache_store.redis import RedisStore
            # import redis
            # redis_instance = redis.Redis()
            # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)
            cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))
            cache_client = ChatCompletionCache(openai_model_client, cache_store)

            response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
            print(response)  # Should print response from OpenAI
            response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
            print(response)  # Should print cached response

    asyncio.run(main())

    ```

    You can now use the cached\_client as you would the original client, but with caching enabled.

    Parameters:
    :   * **client** ([*ChatCompletionClient*](#autogen_core.models.ChatCompletionClient "autogen_core.models.ChatCompletionClient")) – The original ChatCompletionClient to wrap.
        * **store** ([*CacheStore*](#autogen_core.CacheStore "autogen_core.CacheStore")) – A store object that implements get and set methods.
          The user is responsible for managing the store’s lifecycle & clearing it (if needed).
          Defaults to using in-memory cache.

    *classmethod* \_from\_config(*config: ChatCompletionCacheConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_ext.models.cache.ChatCompletionCache._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → ChatCompletionCacheConfig[#](#autogen_ext.models.cache.ChatCompletionCache._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    actual\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.models.cache.ChatCompletionCache.actual_usage "Link to this definition")

    *property* capabilities*: [ModelCapabilities](#autogen_core.models.ModelCapabilities "autogen_core.models._model_client.ModelCapabilities")*[#](#autogen_ext.models.cache.ChatCompletionCache.capabilities "Link to this definition")

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.models.cache.ChatCompletionCache.close "Link to this definition")

    component\_config\_schema[#](#autogen_ext.models.cache.ChatCompletionCache.component_config_schema "Link to this definition")
    :   alias of `ChatCompletionCacheConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.models.cache.ChatCompletionCache'*[#](#autogen_ext.models.cache.ChatCompletionCache.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    component\_type*: ClassVar[ComponentType]* *= 'chat\_completion\_cache'*[#](#autogen_ext.models.cache.ChatCompletionCache.component_type "Link to this definition")
    :   The logical type of the component.

    count\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.models.cache.ChatCompletionCache.count_tokens "Link to this definition")

    *async* create(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult")[#](#autogen_ext.models.cache.ChatCompletionCache.create "Link to this definition")
    :   Cached version of ChatCompletionClient.create.
        If the result of a call to create has been cached, it will be returned immediately
        without invoking the underlying client.

        NOTE: cancellation\_token is ignored for cached results.

    create\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_ext.models.cache.ChatCompletionCache.create_stream "Link to this definition")
    :   Cached version of ChatCompletionClient.create\_stream.
        If the result of a call to create\_stream has been cached, it will be returned
        without streaming from the underlying client.

        NOTE: cancellation\_token is ignored for cached results.

    *property* model\_info*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*[#](#autogen_ext.models.cache.ChatCompletionCache.model_info "Link to this definition")

    remaining\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.models.cache.ChatCompletionCache.remaining_tokens "Link to this definition")

    total\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.models.cache.ChatCompletionCache.total_usage "Link to this definition")

### autogen\_ext.models.openai[#](#module-autogen_ext.models.openai "Link to this heading")

*class* OpenAIChatCompletionClient(*\*\*kwargs: [Unpack](https://docs.python.org/3/library/typing.html#typing.Unpack "(in Python v3.13)")[OpenAIClientConfiguration]*)[#](#autogen_ext.models.openai.OpenAIChatCompletionClient "Link to this definition")
:   Bases: [`BaseOpenAIChatCompletionClient`](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient "autogen_ext.models.openai._openai_client.BaseOpenAIChatCompletionClient"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[[`OpenAIClientConfigurationConfigModel`](#autogen_ext.models.openai.OpenAIClientConfigurationConfigModel "autogen_ext.models.openai.config.OpenAIClientConfigurationConfigModel")]

    Chat completion client for OpenAI hosted models.

    To use this client, you must install the openai extra:

    ```
    pip install "autogen-ext[openai]"

    ```

    You can also use this client for OpenAI-compatible ChatCompletion endpoints.
    **Using this client for non-OpenAI models is not tested or guaranteed.**

    For non-OpenAI models, please first take a look at our [community extensions](https://microsoft.github.io/autogen/dev/user-guide/extensions-user-guide/index.html)
    for additional model clients.

    Parameters:
    :   * **model** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – Which OpenAI model to use.
        * **api\_key** (*optional**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The API key to use. **Required if ‘OPENAI\_API\_KEY’ is not found in the environment variables.**
        * **organization** (*optional**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The organization ID to use.
        * **base\_url** (*optional**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The base URL to use. **Required if the model is not hosted on OpenAI.**
        * **timeout** – (optional, float): The timeout for the request in seconds.
        * **max\_retries** (*optional**,* [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The maximum number of retries to attempt.
        * **model\_info** (*optional**,* [*ModelInfo*](#autogen_core.models.ModelInfo "autogen_core.models.ModelInfo")) – The capabilities of the model. **Required if the model name is not a valid OpenAI model.**
        * **frequency\_penalty** (*optional**,* [*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)"))
        * **logit\_bias** – (optional, dict[str, int]):
        * **max\_tokens** (*optional**,* [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)"))
        * **n** (*optional**,* [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)"))
        * **presence\_penalty** (*optional**,* [*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)"))
        * **response\_format** (*optional**,* *Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]*) –

          the format of the response. Possible options are:

          ```
          # Text response, this is the default.
          {"type": "text"}

          ```

          ```
          # JSON response, make sure to instruct the model to return JSON.
          {"type": "json_object"}

          ```

          ```
          # Structured output response, with a pre-defined JSON schema.
          {
              "type": "json_schema",
              "json_schema": {
                  "name": "name of the schema, must be an identifier.",
                  "description": "description for the model.",
                  # You can convert a Pydantic (v2) model to JSON schema
                  # using the `model_json_schema()` method.
                  "schema": "<the JSON schema itself>",
                  # Whether to enable strict schema adherence when
                  # generating the output. If set to true, the model will
                  # always follow the exact schema defined in the
                  # `schema` field. Only a subset of JSON Schema is
                  # supported when `strict` is `true`.
                  # To learn more, read
                  # https://platform.openai.com/docs/guides/structured-outputs.
                  "strict": False,  # or True
              },
          }

          ```

          It is recommended to use the json\_output parameter in
          [`create()`](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create "autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create") or
          [`create_stream()`](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create_stream "autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create_stream")
          methods instead of response\_format for structured output.
          The json\_output parameter is more flexible and allows you to
          specify a Pydantic model class directly.
        * **seed** (*optional**,* [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)"))
        * **stop** (*optional**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *|* *List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]*)
        * **temperature** (*optional**,* [*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)"))
        * **top\_p** (*optional**,* [*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)"))
        * **user** (*optional**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"))
        * **default\_headers** (*optional**,* [*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")*[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]*) – Custom headers; useful for authentication or other custom requirements.
        * **add\_name\_prefixes** (*optional**,* [*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")) – Whether to prepend the source value
          to each [`UserMessage`](#autogen_core.models.UserMessage "autogen_core.models.UserMessage") content. E.g.,
          “this is content” becomes “Reviewer said: this is content.”
          This can be useful for models that do not support the name field in
          message. Defaults to False.
        * **stream\_options** (*optional**,* [*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")) – Additional options for streaming. Currently only include\_usage is supported.

    Examples

    The following code snippet shows how to use the client with an OpenAI model:

    ```
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_core.models import UserMessage

    openai_client = OpenAIChatCompletionClient(
        model="gpt-4o-2024-08-06",
        # api_key="sk-...", # Optional if you have an OPENAI_API_KEY environment variable set.
    )

    result = await openai_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
    print(result)

    # Close the client when done.
    # await openai_client.close()

    ```

    To use the client with a non-OpenAI model, you need to provide the base URL of the model and the model info.
    For example, to use Ollama, you can use the following code snippet:

    ```
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_core.models import ModelFamily

    custom_model_client = OpenAIChatCompletionClient(
        model="deepseek-r1:1.5b",
        base_url="http://localhost:11434/v1",
        api_key="placeholder",
        model_info={
            "vision": False,
            "function_calling": False,
            "json_output": False,
            "family": ModelFamily.R1,
            "structured_output": True,
        },
    )

    # Close the client when done.
    # await custom_model_client.close()

    ```

    To use streaming mode, you can use the following code snippet:

    ```
    import asyncio
    from autogen_core.models import UserMessage
    from autogen_ext.models.openai import OpenAIChatCompletionClient

    async def main() -> None:
        # Similar for AzureOpenAIChatCompletionClient.
        model_client = OpenAIChatCompletionClient(model="gpt-4o")  # assuming OPENAI_API_KEY is set in the environment.

        messages = [UserMessage(content="Write a very short story about a dragon.", source="user")]

        # Create a stream.
        stream = model_client.create_stream(messages=messages)

        # Iterate over the stream and print the responses.
        print("Streamed responses:")
        async for response in stream:
            if isinstance(response, str):
                # A partial response is a string.
                print(response, flush=True, end="")
            else:
                # The last response is a CreateResult object with the complete message.
                print("\n\n------------\n")
                print("The complete response:", flush=True)
                print(response.content, flush=True)

        # Close the client when done.
        await model_client.close()

    asyncio.run(main())

    ```

    To use structured output as well as function calling, you can use the following code snippet:

    ```
    import asyncio
    from typing import Literal

    from autogen_core.models import (
        AssistantMessage,
        FunctionExecutionResult,
        FunctionExecutionResultMessage,
        SystemMessage,
        UserMessage,
    )
    from autogen_core.tools import FunctionTool
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from pydantic import BaseModel

    # Define the structured output format.
    class AgentResponse(BaseModel):
        thoughts: str
        response: Literal["happy", "sad", "neutral"]

    # Define the function to be called as a tool.
    def sentiment_analysis(text: str) -> str:
        """Given a text, return the sentiment."""
        return "happy" if "happy" in text else "sad" if "sad" in text else "neutral"

    # Create a FunctionTool instance with `strict=True`,
    # which is required for structured output mode.
    tool = FunctionTool(sentiment_analysis, description="Sentiment Analysis", strict=True)

    async def main() -> None:
        # Create an OpenAIChatCompletionClient instance.
        model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")

        # Generate a response using the tool.
        response1 = await model_client.create(
            messages=[
                SystemMessage(content="Analyze input text sentiment using the tool provided."),
                UserMessage(content="I am happy.", source="user"),
            ],
            tools=[tool],
        )
        print(response1.content)
        # Should be a list of tool calls.
        # [FunctionCall(name="sentiment_analysis", arguments={"text": "I am happy."}, ...)]

        assert isinstance(response1.content, list)
        response2 = await model_client.create(
            messages=[
                SystemMessage(content="Analyze input text sentiment using the tool provided."),
                UserMessage(content="I am happy.", source="user"),
                AssistantMessage(content=response1.content, source="assistant"),
                FunctionExecutionResultMessage(
                    content=[FunctionExecutionResult(content="happy", call_id=response1.content[0].id, is_error=False, name="sentiment_analysis")]
                ),
            ],
            # Use the structured output format.
            json_output=AgentResponse,
        )
        print(response2.content)
        # Should be a structured output.
        # {"thoughts": "The user is happy.", "response": "happy"}

        # Close the client when done.
        await model_client.close()

    asyncio.run(main())

    ```

    To load the client from a configuration, you can use the load\_component method:

    ```
    from autogen_core.models import ChatCompletionClient

    config = {
        "provider": "OpenAIChatCompletionClient",
        "config": {"model": "gpt-4o", "api_key": "REPLACE_WITH_YOUR_API_KEY"},
    }

    client = ChatCompletionClient.load_component(config)

    ```

    To view the full list of available configuration options, see the [`OpenAIClientConfigurationConfigModel`](#autogen_ext.models.openai.OpenAIClientConfigurationConfigModel "autogen_ext.models.openai.OpenAIClientConfigurationConfigModel") class.

    component\_type*: ClassVar[ComponentType]* *= 'model'*[#](#autogen_ext.models.openai.OpenAIChatCompletionClient.component_type "Link to this definition")
    :   The logical type of the component.

    component\_config\_schema[#](#autogen_ext.models.openai.OpenAIChatCompletionClient.component_config_schema "Link to this definition")
    :   alias of [`OpenAIClientConfigurationConfigModel`](#autogen_ext.models.openai.OpenAIClientConfigurationConfigModel "autogen_ext.models.openai.config.OpenAIClientConfigurationConfigModel")

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.models.openai.OpenAIChatCompletionClient'*[#](#autogen_ext.models.openai.OpenAIChatCompletionClient.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    \_to\_config() → [OpenAIClientConfigurationConfigModel](#autogen_ext.models.openai.OpenAIClientConfigurationConfigModel "autogen_ext.models.openai.config.OpenAIClientConfigurationConfigModel")[#](#autogen_ext.models.openai.OpenAIChatCompletionClient._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    *classmethod* \_from\_config(*config: [OpenAIClientConfigurationConfigModel](#autogen_ext.models.openai.OpenAIClientConfigurationConfigModel "autogen_ext.models.openai.config.OpenAIClientConfigurationConfigModel")*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_ext.models.openai.OpenAIChatCompletionClient._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

*class* AzureOpenAIChatCompletionClient(*\*\*kwargs: [Unpack](https://docs.python.org/3/library/typing.html#typing.Unpack "(in Python v3.13)")[AzureOpenAIClientConfiguration]*)[#](#autogen_ext.models.openai.AzureOpenAIChatCompletionClient "Link to this definition")
:   Bases: [`BaseOpenAIChatCompletionClient`](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient "autogen_ext.models.openai._openai_client.BaseOpenAIChatCompletionClient"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[[`AzureOpenAIClientConfigurationConfigModel`](#autogen_ext.models.openai.AzureOpenAIClientConfigurationConfigModel "autogen_ext.models.openai.config.AzureOpenAIClientConfigurationConfigModel")]

    Chat completion client for Azure OpenAI hosted models.

    To use this client, you must install the azure and openai extensions:

    ```
    pip install "autogen-ext[openai,azure]"

    ```

    Parameters:
    :   * **model** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – Which OpenAI model to use.
        * **azure\_endpoint** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The endpoint for the Azure model. **Required for Azure models.**
        * **azure\_deployment** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – Deployment name for the Azure model. **Required for Azure models.**
        * **api\_version** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The API version to use. **Required for Azure models.**
        * **azure\_ad\_token** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The Azure AD token to use. Provide this or azure\_ad\_token\_provider for token-based authentication.
        * **azure\_ad\_token\_provider** (*optional**,* *Callable**[**[**]**,* *Awaitable**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]**]* *|* [*AzureTokenProvider*](#autogen_ext.auth.azure.AzureTokenProvider "autogen_ext.auth.azure.AzureTokenProvider")) – The Azure AD token provider to use. Provide this or azure\_ad\_token for token-based authentication.
        * **api\_key** (*optional**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The API key to use, use this if you are using key based authentication. It is optional if you are using Azure AD token based authentication or AZURE\_OPENAI\_API\_KEY environment variable.
        * **timeout** – (optional, float): The timeout for the request in seconds.
        * **max\_retries** (*optional**,* [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The maximum number of retries to attempt.
        * **model\_info** (*optional**,* [*ModelInfo*](#autogen_core.models.ModelInfo "autogen_core.models.ModelInfo")) – The capabilities of the model. **Required if the model name is not a valid OpenAI model.**
        * **frequency\_penalty** (*optional**,* [*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)"))
        * **logit\_bias** – (optional, dict[str, int]):
        * **max\_tokens** (*optional**,* [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)"))
        * **n** (*optional**,* [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)"))
        * **presence\_penalty** (*optional**,* [*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)"))
        * **response\_format** (*optional**,* *Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]*) –

          the format of the response. Possible options are:

          ```
          # Text response, this is the default.
          {"type": "text"}

          ```

          ```
          # JSON response, make sure to instruct the model to return JSON.
          {"type": "json_object"}

          ```

          ```
          # Structured output response, with a pre-defined JSON schema.
          {
              "type": "json_schema",
              "json_schema": {
                  "name": "name of the schema, must be an identifier.",
                  "description": "description for the model.",
                  # You can convert a Pydantic (v2) model to JSON schema
                  # using the `model_json_schema()` method.
                  "schema": "<the JSON schema itself>",
                  # Whether to enable strict schema adherence when
                  # generating the output. If set to true, the model will
                  # always follow the exact schema defined in the
                  # `schema` field. Only a subset of JSON Schema is
                  # supported when `strict` is `true`.
                  # To learn more, read
                  # https://platform.openai.com/docs/guides/structured-outputs.
                  "strict": False,  # or True
              },
          }

          ```

          It is recommended to use the json\_output parameter in
          [`create()`](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create "autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create") or
          [`create_stream()`](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create_stream "autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create_stream")
          methods instead of response\_format for structured output.
          The json\_output parameter is more flexible and allows you to
          specify a Pydantic model class directly.
        * **seed** (*optional**,* [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)"))
        * **stop** (*optional**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *|* *List**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]*)
        * **temperature** (*optional**,* [*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)"))
        * **top\_p** (*optional**,* [*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)"))
        * **user** (*optional**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"))
        * **default\_headers** (*optional**,* [*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")*[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]*) – Custom headers; useful for authentication or other custom requirements.

    To use the client, you need to provide your deployment name, Azure Cognitive Services endpoint, and api version.
    For authentication, you can either provide an API key or an Azure Active Directory (AAD) token credential.

    The following code snippet shows how to use AAD authentication.
    The identity used must be assigned the [Cognitive Services OpenAI User](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control#cognitive-services-openai-user) role.

    ```
    from autogen_ext.auth.azure import AzureTokenProvider
    from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
    from azure.identity import DefaultAzureCredential

    # Create the token provider
    token_provider = AzureTokenProvider(
        DefaultAzureCredential(),
        "https://cognitiveservices.azure.com/.default",
    )

    az_model_client = AzureOpenAIChatCompletionClient(
        azure_deployment="{your-azure-deployment}",
        model="{model-name, such as gpt-4o}",
        api_version="2024-06-01",
        azure_endpoint="https://{your-custom-endpoint}.openai.azure.com/",
        azure_ad_token_provider=token_provider,  # Optional if you choose key-based authentication.
        # api_key="sk-...", # For key-based authentication.
    )

    ```

    See other usage examples in the [`OpenAIChatCompletionClient`](#autogen_ext.models.openai.OpenAIChatCompletionClient "autogen_ext.models.openai.OpenAIChatCompletionClient") class.

    To load the client that uses identity based aith from a configuration, you can use the load\_component method:

    ```
    from autogen_core.models import ChatCompletionClient

    config = {
        "provider": "AzureOpenAIChatCompletionClient",
        "config": {
            "model": "gpt-4o-2024-05-13",
            "azure_endpoint": "https://{your-custom-endpoint}.openai.azure.com/",
            "azure_deployment": "{your-azure-deployment}",
            "api_version": "2024-06-01",
            "azure_ad_token_provider": {
                "provider": "autogen_ext.auth.azure.AzureTokenProvider",
                "config": {
                    "provider_kind": "DefaultAzureCredential",
                    "scopes": ["https://cognitiveservices.azure.com/.default"],
                },
            },
        },
    }

    client = ChatCompletionClient.load_component(config)

    ```

    To view the full list of available configuration options, see the [`AzureOpenAIClientConfigurationConfigModel`](#autogen_ext.models.openai.AzureOpenAIClientConfigurationConfigModel "autogen_ext.models.openai.AzureOpenAIClientConfigurationConfigModel") class.

    Note

    Right now only DefaultAzureCredential is supported with no additional args passed to it.

    See [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity#chat-completions) for how to use the Azure client directly or for more info.

    component\_type*: ClassVar[ComponentType]* *= 'model'*[#](#autogen_ext.models.openai.AzureOpenAIChatCompletionClient.component_type "Link to this definition")
    :   The logical type of the component.

    component\_config\_schema[#](#autogen_ext.models.openai.AzureOpenAIChatCompletionClient.component_config_schema "Link to this definition")
    :   alias of [`AzureOpenAIClientConfigurationConfigModel`](#autogen_ext.models.openai.AzureOpenAIClientConfigurationConfigModel "autogen_ext.models.openai.config.AzureOpenAIClientConfigurationConfigModel")

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.models.openai.AzureOpenAIChatCompletionClient'*[#](#autogen_ext.models.openai.AzureOpenAIChatCompletionClient.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    \_to\_config() → [AzureOpenAIClientConfigurationConfigModel](#autogen_ext.models.openai.AzureOpenAIClientConfigurationConfigModel "autogen_ext.models.openai.config.AzureOpenAIClientConfigurationConfigModel")[#](#autogen_ext.models.openai.AzureOpenAIChatCompletionClient._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    *classmethod* \_from\_config(*config: [AzureOpenAIClientConfigurationConfigModel](#autogen_ext.models.openai.AzureOpenAIClientConfigurationConfigModel "autogen_ext.models.openai.config.AzureOpenAIClientConfigurationConfigModel")*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_ext.models.openai.AzureOpenAIChatCompletionClient._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

*class* BaseOpenAIChatCompletionClient(*client: AsyncOpenAI | AsyncAzureOpenAI*, *\**, *create\_args: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*, *model\_capabilities: [ModelCapabilities](#autogen_core.models.ModelCapabilities "autogen_core.models._model_client.ModelCapabilities") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *model\_info: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *add\_name\_prefixes: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*)[#](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient "Link to this definition")
:   Bases: [`ChatCompletionClient`](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")

    *classmethod* create\_from\_config(*config: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")[#](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create_from_config "Link to this definition")

    *async* create(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult")[#](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create "Link to this definition")
    :   Creates a single response from the model.

        Parameters:
        :   * **messages** (*Sequence**[**LLMMessage**]*) – The messages to send to the model.
            * **tools** (*Sequence**[*[*Tool*](#autogen_core.tools.Tool "autogen_core.tools.Tool") *|* [*ToolSchema*](#autogen_core.tools.ToolSchema "autogen_core.tools.ToolSchema")*]**,* *optional*) – The tools to use with the model. Defaults to [].
            * **json\_output** (*Optional**[*[*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") *|* [*type*](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")*[**BaseModel**]**]**,* *optional*) – Whether to use JSON mode, structured output, or neither.
              Defaults to None. If set to a [Pydantic BaseModel](https://docs.pydantic.dev/latest/usage/models/#model) type,
              it will be used as the output type for structured output.
              If set to a boolean, it will be used to determine whether to use JSON mode or not.
              If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
            * **extra\_create\_args** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]**,* *optional*) – Extra arguments to pass to the underlying client. Defaults to {}.
            * **cancellation\_token** (*Optional**[*[*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken")*]**,* *optional*) – A token for cancellation. Defaults to None.

        Returns:
        :   **CreateResult** – The result of the model call.

    *async* create\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *max\_consecutive\_empty\_chunk\_tolerance: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 0*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.create_stream "Link to this definition")
    :   Create a stream of string chunks from the model ending with a [`CreateResult`](#autogen_core.models.CreateResult "autogen_core.models.CreateResult").

        Extends [`autogen_core.models.ChatCompletionClient.create_stream()`](#autogen_core.models.ChatCompletionClient.create_stream "autogen_core.models.ChatCompletionClient.create_stream") to support OpenAI API.

        In streaming, the default behaviour is not return token usage counts.
        See: [OpenAI API reference for possible args](https://platform.openai.com/docs/api-reference/chat/create).

        You can set extra\_create\_args={“stream\_options”: {“include\_usage”: True}}
        (if supported by the accessed API) to
        return a final chunk with usage set to a [`RequestUsage`](#autogen_core.models.RequestUsage "autogen_core.models.RequestUsage") object
        with prompt and completion token counts,
        all preceding chunks will have usage as None.
        See: [OpenAI API reference for stream options](https://platform.openai.com/docs/api-reference/chat/create#chat-create-stream_options).

        Other examples of supported arguments that can be included in extra\_create\_args:
        :   * temperature (float): Controls the randomness of the output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
            * max\_tokens (int): The maximum number of tokens to generate in the completion.
            * top\_p (float): An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top\_p probability mass.
            * frequency\_penalty (float): A value between -2.0 and 2.0 that penalizes new tokens based on their existing frequency in the text so far, decreasing the likelihood of repeated phrases.
            * presence\_penalty (float): A value between -2.0 and 2.0 that penalizes new tokens based on whether they appear in the text so far, encouraging the model to talk about new topics.

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.close "Link to this definition")

    actual\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.actual_usage "Link to this definition")

    total\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.total_usage "Link to this definition")

    count\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.count_tokens "Link to this definition")

    remaining\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.remaining_tokens "Link to this definition")

    *property* capabilities*: [ModelCapabilities](#autogen_core.models.ModelCapabilities "autogen_core.models._model_client.ModelCapabilities")*[#](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.capabilities "Link to this definition")

    *property* model\_info*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*[#](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient.model_info "Link to this definition")

*pydantic model* AzureOpenAIClientConfigurationConfigModel[#](#autogen_ext.models.openai.AzureOpenAIClientConfigurationConfigModel "Link to this definition")
:   Bases: [`BaseOpenAIClientConfigurationConfigModel`](#autogen_ext.models.openai.BaseOpenAIClientConfigurationConfigModel "autogen_ext.models.openai.config.BaseOpenAIClientConfigurationConfigModel")

    Show JSON schema

    ```
    {
       "title": "AzureOpenAIClientConfigurationConfigModel",
       "type": "object",
       "properties": {
          "frequency_penalty": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Frequency Penalty"
          },
          "logit_bias": {
             "anyOf": [
                {
                   "additionalProperties": {
                      "type": "integer"
                   },
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Logit Bias"
          },
          "max_tokens": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Max Tokens"
          },
          "n": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "N"
          },
          "presence_penalty": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Presence Penalty"
          },
          "response_format": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ResponseFormat"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "seed": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Seed"
          },
          "stop": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "items": {
                      "type": "string"
                   },
                   "type": "array"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Stop"
          },
          "temperature": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Temperature"
          },
          "top_p": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Top P"
          },
          "user": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "User"
          },
          "stream_options": {
             "anyOf": [
                {
                   "$ref": "#/$defs/StreamOptions"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "model": {
             "title": "Model",
             "type": "string"
          },
          "api_key": {
             "anyOf": [
                {
                   "format": "password",
                   "type": "string",
                   "writeOnly": true
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Api Key"
          },
          "timeout": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Timeout"
          },
          "max_retries": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Max Retries"
          },
          "model_capabilities": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ModelCapabilities"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "model_info": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ModelInfo"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "add_name_prefixes": {
             "anyOf": [
                {
                   "type": "boolean"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Add Name Prefixes"
          },
          "default_headers": {
             "anyOf": [
                {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Default Headers"
          },
          "azure_endpoint": {
             "title": "Azure Endpoint",
             "type": "string"
          },
          "azure_deployment": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Azure Deployment"
          },
          "api_version": {
             "title": "Api Version",
             "type": "string"
          },
          "azure_ad_token": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Azure Ad Token"
          },
          "azure_ad_token_provider": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ComponentModel"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          }
       },
       "$defs": {
          "ComponentModel": {
             "description": "Model class for a component. Contains all information required to instantiate a component.",
             "properties": {
                "provider": {
                   "title": "Provider",
                   "type": "string"
                },
                "component_type": {
                   "anyOf": [
                      {
                         "enum": [
                            "model",
                            "agent",
                            "tool",
                            "termination",
                            "token_provider"
                         ],
                         "type": "string"
                      },
                      {
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Component Type"
                },
                "version": {
                   "anyOf": [
                      {
                         "type": "integer"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Version"
                },
                "component_version": {
                   "anyOf": [
                      {
                         "type": "integer"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Component Version"
                },
                "description": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Description"
                },
                "label": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Label"
                },
                "config": {
                   "title": "Config",
                   "type": "object"
                }
             },
             "required": [
                "provider",
                "config"
             ],
             "title": "ComponentModel",
             "type": "object"
          },
          "JSONSchema": {
             "properties": {
                "name": {
                   "title": "Name",
                   "type": "string"
                },
                "description": {
                   "title": "Description",
                   "type": "string"
                },
                "schema": {
                   "title": "Schema",
                   "type": "object"
                },
                "strict": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "title": "Strict"
                }
             },
             "required": [
                "name"
             ],
             "title": "JSONSchema",
             "type": "object"
          },
          "ModelCapabilities": {
             "deprecated": true,
             "properties": {
                "vision": {
                   "title": "Vision",
                   "type": "boolean"
                },
                "function_calling": {
                   "title": "Function Calling",
                   "type": "boolean"
                },
                "json_output": {
                   "title": "Json Output",
                   "type": "boolean"
                }
             },
             "required": [
                "vision",
                "function_calling",
                "json_output"
             ],
             "title": "ModelCapabilities",
             "type": "object"
          },
          "ModelInfo": {
             "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
             "properties": {
                "vision": {
                   "title": "Vision",
                   "type": "boolean"
                },
                "function_calling": {
                   "title": "Function Calling",
                   "type": "boolean"
                },
                "json_output": {
                   "title": "Json Output",
                   "type": "boolean"
                },
                "family": {
                   "anyOf": [
                      {
                         "enum": [
                            "gpt-4o",
                            "o1",
                            "o3",
                            "gpt-4",
                            "gpt-35",
                            "r1",
                            "gemini-1.5-flash",
                            "gemini-1.5-pro",
                            "gemini-2.0-flash",
                            "claude-3-haiku",
                            "claude-3-sonnet",
                            "claude-3-opus",
                            "claude-3.5-haiku",
                            "claude-3.5-sonnet",
                            "unknown"
                         ],
                         "type": "string"
                      },
                      {
                         "type": "string"
                      }
                   ],
                   "title": "Family"
                },
                "structured_output": {
                   "title": "Structured Output",
                   "type": "boolean"
                }
             },
             "required": [
                "vision",
                "function_calling",
                "json_output",
                "family",
                "structured_output"
             ],
             "title": "ModelInfo",
             "type": "object"
          },
          "ResponseFormat": {
             "properties": {
                "type": {
                   "enum": [
                      "text",
                      "json_object",
                      "json_schema"
                   ],
                   "title": "Type",
                   "type": "string"
                },
                "json_schema": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/JSONSchema"
                      },
                      {
                         "type": "null"
                      }
                   ]
                }
             },
             "required": [
                "type",
                "json_schema"
             ],
             "title": "ResponseFormat",
             "type": "object"
          },
          "StreamOptions": {
             "properties": {
                "include_usage": {
                   "title": "Include Usage",
                   "type": "boolean"
                }
             },
             "required": [
                "include_usage"
             ],
             "title": "StreamOptions",
             "type": "object"
          }
       },
       "required": [
          "model",
          "azure_endpoint",
          "api_version"
       ]
    }

    ```

    Fields:
    :   * `api_version (str)`
        * `azure_ad_token (str | None)`
        * `azure_ad_token_provider (autogen_core._component_config.ComponentModel | None)`
        * `azure_deployment (str | None)`
        * `azure_endpoint (str)`

    *field* azure\_endpoint*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.models.openai.AzureOpenAIClientConfigurationConfigModel.azure_endpoint "Link to this definition")

    *field* azure\_deployment*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.AzureOpenAIClientConfigurationConfigModel.azure_deployment "Link to this definition")

    *field* api\_version*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.models.openai.AzureOpenAIClientConfigurationConfigModel.api_version "Link to this definition")

    *field* azure\_ad\_token*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.AzureOpenAIClientConfigurationConfigModel.azure_ad_token "Link to this definition")

    *field* azure\_ad\_token\_provider*: [ComponentModel](#autogen_core.ComponentModel "autogen_core._component_config.ComponentModel") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.AzureOpenAIClientConfigurationConfigModel.azure_ad_token_provider "Link to this definition")

*pydantic model* OpenAIClientConfigurationConfigModel[#](#autogen_ext.models.openai.OpenAIClientConfigurationConfigModel "Link to this definition")
:   Bases: [`BaseOpenAIClientConfigurationConfigModel`](#autogen_ext.models.openai.BaseOpenAIClientConfigurationConfigModel "autogen_ext.models.openai.config.BaseOpenAIClientConfigurationConfigModel")

    Show JSON schema

    ```
    {
       "title": "OpenAIClientConfigurationConfigModel",
       "type": "object",
       "properties": {
          "frequency_penalty": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Frequency Penalty"
          },
          "logit_bias": {
             "anyOf": [
                {
                   "additionalProperties": {
                      "type": "integer"
                   },
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Logit Bias"
          },
          "max_tokens": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Max Tokens"
          },
          "n": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "N"
          },
          "presence_penalty": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Presence Penalty"
          },
          "response_format": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ResponseFormat"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "seed": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Seed"
          },
          "stop": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "items": {
                      "type": "string"
                   },
                   "type": "array"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Stop"
          },
          "temperature": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Temperature"
          },
          "top_p": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Top P"
          },
          "user": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "User"
          },
          "stream_options": {
             "anyOf": [
                {
                   "$ref": "#/$defs/StreamOptions"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "model": {
             "title": "Model",
             "type": "string"
          },
          "api_key": {
             "anyOf": [
                {
                   "format": "password",
                   "type": "string",
                   "writeOnly": true
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Api Key"
          },
          "timeout": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Timeout"
          },
          "max_retries": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Max Retries"
          },
          "model_capabilities": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ModelCapabilities"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "model_info": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ModelInfo"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "add_name_prefixes": {
             "anyOf": [
                {
                   "type": "boolean"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Add Name Prefixes"
          },
          "default_headers": {
             "anyOf": [
                {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Default Headers"
          },
          "organization": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Organization"
          },
          "base_url": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Base Url"
          }
       },
       "$defs": {
          "JSONSchema": {
             "properties": {
                "name": {
                   "title": "Name",
                   "type": "string"
                },
                "description": {
                   "title": "Description",
                   "type": "string"
                },
                "schema": {
                   "title": "Schema",
                   "type": "object"
                },
                "strict": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "title": "Strict"
                }
             },
             "required": [
                "name"
             ],
             "title": "JSONSchema",
             "type": "object"
          },
          "ModelCapabilities": {
             "deprecated": true,
             "properties": {
                "vision": {
                   "title": "Vision",
                   "type": "boolean"
                },
                "function_calling": {
                   "title": "Function Calling",
                   "type": "boolean"
                },
                "json_output": {
                   "title": "Json Output",
                   "type": "boolean"
                }
             },
             "required": [
                "vision",
                "function_calling",
                "json_output"
             ],
             "title": "ModelCapabilities",
             "type": "object"
          },
          "ModelInfo": {
             "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
             "properties": {
                "vision": {
                   "title": "Vision",
                   "type": "boolean"
                },
                "function_calling": {
                   "title": "Function Calling",
                   "type": "boolean"
                },
                "json_output": {
                   "title": "Json Output",
                   "type": "boolean"
                },
                "family": {
                   "anyOf": [
                      {
                         "enum": [
                            "gpt-4o",
                            "o1",
                            "o3",
                            "gpt-4",
                            "gpt-35",
                            "r1",
                            "gemini-1.5-flash",
                            "gemini-1.5-pro",
                            "gemini-2.0-flash",
                            "claude-3-haiku",
                            "claude-3-sonnet",
                            "claude-3-opus",
                            "claude-3.5-haiku",
                            "claude-3.5-sonnet",
                            "unknown"
                         ],
                         "type": "string"
                      },
                      {
                         "type": "string"
                      }
                   ],
                   "title": "Family"
                },
                "structured_output": {
                   "title": "Structured Output",
                   "type": "boolean"
                }
             },
             "required": [
                "vision",
                "function_calling",
                "json_output",
                "family",
                "structured_output"
             ],
             "title": "ModelInfo",
             "type": "object"
          },
          "ResponseFormat": {
             "properties": {
                "type": {
                   "enum": [
                      "text",
                      "json_object",
                      "json_schema"
                   ],
                   "title": "Type",
                   "type": "string"
                },
                "json_schema": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/JSONSchema"
                      },
                      {
                         "type": "null"
                      }
                   ]
                }
             },
             "required": [
                "type",
                "json_schema"
             ],
             "title": "ResponseFormat",
             "type": "object"
          },
          "StreamOptions": {
             "properties": {
                "include_usage": {
                   "title": "Include Usage",
                   "type": "boolean"
                }
             },
             "required": [
                "include_usage"
             ],
             "title": "StreamOptions",
             "type": "object"
          }
       },
       "required": [
          "model"
       ]
    }

    ```

    Fields:
    :   * `base_url (str | None)`
        * `organization (str | None)`

    *field* organization*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.OpenAIClientConfigurationConfigModel.organization "Link to this definition")

    *field* base\_url*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.OpenAIClientConfigurationConfigModel.base_url "Link to this definition")

*pydantic model* BaseOpenAIClientConfigurationConfigModel[#](#autogen_ext.models.openai.BaseOpenAIClientConfigurationConfigModel "Link to this definition")
:   Bases: [`CreateArgumentsConfigModel`](#autogen_ext.models.openai.CreateArgumentsConfigModel "autogen_ext.models.openai.config.CreateArgumentsConfigModel")

    Show JSON schema

    ```
    {
       "title": "BaseOpenAIClientConfigurationConfigModel",
       "type": "object",
       "properties": {
          "frequency_penalty": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Frequency Penalty"
          },
          "logit_bias": {
             "anyOf": [
                {
                   "additionalProperties": {
                      "type": "integer"
                   },
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Logit Bias"
          },
          "max_tokens": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Max Tokens"
          },
          "n": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "N"
          },
          "presence_penalty": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Presence Penalty"
          },
          "response_format": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ResponseFormat"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "seed": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Seed"
          },
          "stop": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "items": {
                      "type": "string"
                   },
                   "type": "array"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Stop"
          },
          "temperature": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Temperature"
          },
          "top_p": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Top P"
          },
          "user": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "User"
          },
          "stream_options": {
             "anyOf": [
                {
                   "$ref": "#/$defs/StreamOptions"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "model": {
             "title": "Model",
             "type": "string"
          },
          "api_key": {
             "anyOf": [
                {
                   "format": "password",
                   "type": "string",
                   "writeOnly": true
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Api Key"
          },
          "timeout": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Timeout"
          },
          "max_retries": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Max Retries"
          },
          "model_capabilities": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ModelCapabilities"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "model_info": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ModelInfo"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "add_name_prefixes": {
             "anyOf": [
                {
                   "type": "boolean"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Add Name Prefixes"
          },
          "default_headers": {
             "anyOf": [
                {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Default Headers"
          }
       },
       "$defs": {
          "JSONSchema": {
             "properties": {
                "name": {
                   "title": "Name",
                   "type": "string"
                },
                "description": {
                   "title": "Description",
                   "type": "string"
                },
                "schema": {
                   "title": "Schema",
                   "type": "object"
                },
                "strict": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "title": "Strict"
                }
             },
             "required": [
                "name"
             ],
             "title": "JSONSchema",
             "type": "object"
          },
          "ModelCapabilities": {
             "deprecated": true,
             "properties": {
                "vision": {
                   "title": "Vision",
                   "type": "boolean"
                },
                "function_calling": {
                   "title": "Function Calling",
                   "type": "boolean"
                },
                "json_output": {
                   "title": "Json Output",
                   "type": "boolean"
                }
             },
             "required": [
                "vision",
                "function_calling",
                "json_output"
             ],
             "title": "ModelCapabilities",
             "type": "object"
          },
          "ModelInfo": {
             "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
             "properties": {
                "vision": {
                   "title": "Vision",
                   "type": "boolean"
                },
                "function_calling": {
                   "title": "Function Calling",
                   "type": "boolean"
                },
                "json_output": {
                   "title": "Json Output",
                   "type": "boolean"
                },
                "family": {
                   "anyOf": [
                      {
                         "enum": [
                            "gpt-4o",
                            "o1",
                            "o3",
                            "gpt-4",
                            "gpt-35",
                            "r1",
                            "gemini-1.5-flash",
                            "gemini-1.5-pro",
                            "gemini-2.0-flash",
                            "claude-3-haiku",
                            "claude-3-sonnet",
                            "claude-3-opus",
                            "claude-3.5-haiku",
                            "claude-3.5-sonnet",
                            "unknown"
                         ],
                         "type": "string"
                      },
                      {
                         "type": "string"
                      }
                   ],
                   "title": "Family"
                },
                "structured_output": {
                   "title": "Structured Output",
                   "type": "boolean"
                }
             },
             "required": [
                "vision",
                "function_calling",
                "json_output",
                "family",
                "structured_output"
             ],
             "title": "ModelInfo",
             "type": "object"
          },
          "ResponseFormat": {
             "properties": {
                "type": {
                   "enum": [
                      "text",
                      "json_object",
                      "json_schema"
                   ],
                   "title": "Type",
                   "type": "string"
                },
                "json_schema": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/JSONSchema"
                      },
                      {
                         "type": "null"
                      }
                   ]
                }
             },
             "required": [
                "type",
                "json_schema"
             ],
             "title": "ResponseFormat",
             "type": "object"
          },
          "StreamOptions": {
             "properties": {
                "include_usage": {
                   "title": "Include Usage",
                   "type": "boolean"
                }
             },
             "required": [
                "include_usage"
             ],
             "title": "StreamOptions",
             "type": "object"
          }
       },
       "required": [
          "model"
       ]
    }

    ```

    Fields:
    :   * `add_name_prefixes (bool | None)`
        * `api_key (pydantic.types.SecretStr | None)`
        * `default_headers (Dict[str, str] | None)`
        * `max_retries (int | None)`
        * `model (str)`
        * `model_capabilities (autogen_core.models._model_client.ModelCapabilities | None)`
        * `model_info (autogen_core.models._model_client.ModelInfo | None)`
        * `timeout (float | None)`

    *field* model*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.models.openai.BaseOpenAIClientConfigurationConfigModel.model "Link to this definition")

    *field* api\_key*: SecretStr | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.BaseOpenAIClientConfigurationConfigModel.api_key "Link to this definition")

    *field* timeout*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.BaseOpenAIClientConfigurationConfigModel.timeout "Link to this definition")

    *field* max\_retries*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.BaseOpenAIClientConfigurationConfigModel.max_retries "Link to this definition")

    *field* model\_capabilities*: [ModelCapabilities](#autogen_core.models.ModelCapabilities "autogen_core.models._model_client.ModelCapabilities") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.BaseOpenAIClientConfigurationConfigModel.model_capabilities "Link to this definition")

    *field* model\_info*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.BaseOpenAIClientConfigurationConfigModel.model_info "Link to this definition")

    *field* add\_name\_prefixes*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.BaseOpenAIClientConfigurationConfigModel.add_name_prefixes "Link to this definition")

    *field* default\_headers*: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.BaseOpenAIClientConfigurationConfigModel.default_headers "Link to this definition")

*pydantic model* CreateArgumentsConfigModel[#](#autogen_ext.models.openai.CreateArgumentsConfigModel "Link to this definition")
:   Bases: `BaseModel`

    Show JSON schema

    ```
    {
       "title": "CreateArgumentsConfigModel",
       "type": "object",
       "properties": {
          "frequency_penalty": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Frequency Penalty"
          },
          "logit_bias": {
             "anyOf": [
                {
                   "additionalProperties": {
                      "type": "integer"
                   },
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Logit Bias"
          },
          "max_tokens": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Max Tokens"
          },
          "n": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "N"
          },
          "presence_penalty": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Presence Penalty"
          },
          "response_format": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ResponseFormat"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "seed": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Seed"
          },
          "stop": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "items": {
                      "type": "string"
                   },
                   "type": "array"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Stop"
          },
          "temperature": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Temperature"
          },
          "top_p": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Top P"
          },
          "user": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "User"
          },
          "stream_options": {
             "anyOf": [
                {
                   "$ref": "#/$defs/StreamOptions"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          }
       },
       "$defs": {
          "JSONSchema": {
             "properties": {
                "name": {
                   "title": "Name",
                   "type": "string"
                },
                "description": {
                   "title": "Description",
                   "type": "string"
                },
                "schema": {
                   "title": "Schema",
                   "type": "object"
                },
                "strict": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "title": "Strict"
                }
             },
             "required": [
                "name"
             ],
             "title": "JSONSchema",
             "type": "object"
          },
          "ResponseFormat": {
             "properties": {
                "type": {
                   "enum": [
                      "text",
                      "json_object",
                      "json_schema"
                   ],
                   "title": "Type",
                   "type": "string"
                },
                "json_schema": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/JSONSchema"
                      },
                      {
                         "type": "null"
                      }
                   ]
                }
             },
             "required": [
                "type",
                "json_schema"
             ],
             "title": "ResponseFormat",
             "type": "object"
          },
          "StreamOptions": {
             "properties": {
                "include_usage": {
                   "title": "Include Usage",
                   "type": "boolean"
                }
             },
             "required": [
                "include_usage"
             ],
             "title": "StreamOptions",
             "type": "object"
          }
       }
    }

    ```

    Fields:
    :   * `frequency_penalty (float | None)`
        * `logit_bias (Dict[str, int] | None)`
        * `max_tokens (int | None)`
        * `n (int | None)`
        * `presence_penalty (float | None)`
        * `response_format (autogen_ext.models.openai.config.ResponseFormat | None)`
        * `seed (int | None)`
        * `stop (str | List[str] | None)`
        * `stream_options (autogen_ext.models.openai.config.StreamOptions | None)`
        * `temperature (float | None)`
        * `top_p (float | None)`
        * `user (str | None)`

    *field* frequency\_penalty*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.CreateArgumentsConfigModel.frequency_penalty "Link to this definition")

    *field* logit\_bias*: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.CreateArgumentsConfigModel.logit_bias "Link to this definition")

    *field* max\_tokens*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.CreateArgumentsConfigModel.max_tokens "Link to this definition")

    *field* n*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.CreateArgumentsConfigModel.n "Link to this definition")

    *field* presence\_penalty*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.CreateArgumentsConfigModel.presence_penalty "Link to this definition")

    *field* response\_format*: ResponseFormat | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.CreateArgumentsConfigModel.response_format "Link to this definition")

    *field* seed*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.CreateArgumentsConfigModel.seed "Link to this definition")

    *field* stop*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.CreateArgumentsConfigModel.stop "Link to this definition")

    *field* temperature*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.CreateArgumentsConfigModel.temperature "Link to this definition")

    *field* top\_p*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.CreateArgumentsConfigModel.top_p "Link to this definition")

    *field* user*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.CreateArgumentsConfigModel.user "Link to this definition")

    *field* stream\_options*: StreamOptions | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.openai.CreateArgumentsConfigModel.stream_options "Link to this definition")

### autogen\_ext.models.replay[#](#module-autogen_ext.models.replay "Link to this heading")

*class* ReplayChatCompletionClient(*chat\_completions: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult")]*, *model\_info: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.models.replay.ReplayChatCompletionClient "Link to this definition")
:   Bases: [`ChatCompletionClient`](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`ReplayChatCompletionClientConfig`]

    A mock chat completion client that replays predefined responses using an index-based approach.

    This class simulates a chat completion client by replaying a predefined list of responses. It supports both single completion and streaming responses. The responses can be either strings or CreateResult objects. The client now uses an index-based approach to access the responses, allowing for resetting the state.

    Note

    The responses can be either strings or CreateResult objects.

    Parameters:
    :   **chat\_completions** (*Sequence**[**Union**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* [*CreateResult*](#autogen_core.models.CreateResult "autogen_core.models.CreateResult")*]**]*) – A list of predefined responses to replay.

    Raises:
    :   [**ValueError**](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.13)")**(****"No more mock responses available"****)** – If the list of provided outputs are exhausted.

    Examples:

    Simple chat completion client to return pre-defined responses.

    > ```
    > from autogen_core.models import UserMessage
    > from autogen_ext.models.replay import ReplayChatCompletionClient
    >
    >
    > async def example():
    >     chat_completions = [
    >         "Hello, how can I assist you today?",
    >         "I'm happy to help with any questions you have.",
    >         "Is there anything else I can assist you with?",
    >     ]
    >     client = ReplayChatCompletionClient(chat_completions)
    >     messages = [UserMessage(content="What can you do?", source="user")]
    >     response = await client.create(messages)
    >     print(response.content)  # Output: "Hello, how can I assist you today?"
    >
    > ```

    Simple streaming chat completion client to return pre-defined responses

    > ```
    > import asyncio
    > from autogen_core.models import UserMessage
    > from autogen_ext.models.replay import ReplayChatCompletionClient
    >
    >
    > async def example():
    >     chat_completions = [
    >         "Hello, how can I assist you today?",
    >         "I'm happy to help with any questions you have.",
    >         "Is there anything else I can assist you with?",
    >     ]
    >     client = ReplayChatCompletionClient(chat_completions)
    >     messages = [UserMessage(content="What can you do?", source="user")]
    >
    >     async for token in client.create_stream(messages):
    >         print(token, end="")  # Output: "Hello, how can I assist you today?"
    >
    >     async for token in client.create_stream(messages):
    >         print(token, end="")  # Output: "I'm happy to help with any questions you have."
    >
    >     asyncio.run(example())
    >
    > ```

    Using .reset to reset the chat client state

    > ```
    > import asyncio
    > from autogen_core.models import UserMessage
    > from autogen_ext.models.replay import ReplayChatCompletionClient
    >
    >
    > async def example():
    >     chat_completions = [
    >         "Hello, how can I assist you today?",
    >     ]
    >     client = ReplayChatCompletionClient(chat_completions)
    >     messages = [UserMessage(content="What can you do?", source="user")]
    >     response = await client.create(messages)
    >     print(response.content)  # Output: "Hello, how can I assist you today?"
    >
    >     response = await client.create(messages)  # Raises ValueError("No more mock responses available")
    >
    >     client.reset()  # Reset the client state (current index of message and token usages)
    >     response = await client.create(messages)
    >     print(response.content)  # Output: "Hello, how can I assist you today?" again
    >
    >
    > asyncio.run(example())
    >
    > ```

    *classmethod* \_from\_config(*config: ReplayChatCompletionClientConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_ext.models.replay.ReplayChatCompletionClient._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → ReplayChatCompletionClientConfig[#](#autogen_ext.models.replay.ReplayChatCompletionClient._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    actual\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.models.replay.ReplayChatCompletionClient.actual_usage "Link to this definition")

    *property* capabilities*: [ModelCapabilities](#autogen_core.models.ModelCapabilities "autogen_core.models._model_client.ModelCapabilities")*[#](#autogen_ext.models.replay.ReplayChatCompletionClient.capabilities "Link to this definition")
    :   Return mock capabilities.

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.models.replay.ReplayChatCompletionClient.close "Link to this definition")

    component\_config\_schema[#](#autogen_ext.models.replay.ReplayChatCompletionClient.component_config_schema "Link to this definition")
    :   alias of `ReplayChatCompletionClientConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.models.replay.ReplayChatCompletionClient'*[#](#autogen_ext.models.replay.ReplayChatCompletionClient.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    component\_type*: ClassVar[ComponentType]* *= 'replay\_chat\_completion\_client'*[#](#autogen_ext.models.replay.ReplayChatCompletionClient.component_type "Link to this definition")
    :   The logical type of the component.

    count\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.models.replay.ReplayChatCompletionClient.count_tokens "Link to this definition")

    *async* create(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult")[#](#autogen_ext.models.replay.ReplayChatCompletionClient.create "Link to this definition")
    :   Return the next completion from the list.

    *property* create\_calls*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]]*[#](#autogen_ext.models.replay.ReplayChatCompletionClient.create_calls "Link to this definition")
    :   Return the arguments of the calls made to the create method.

    *async* create\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_ext.models.replay.ReplayChatCompletionClient.create_stream "Link to this definition")
    :   Return the next completion as a stream.

    *property* model\_info*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*[#](#autogen_ext.models.replay.ReplayChatCompletionClient.model_info "Link to this definition")

    remaining\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.models.replay.ReplayChatCompletionClient.remaining_tokens "Link to this definition")

    reset() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.models.replay.ReplayChatCompletionClient.reset "Link to this definition")
    :   Reset the client state and usage to its initial state.

    set\_cached\_bool\_value(*value: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.models.replay.ReplayChatCompletionClient.set_cached_bool_value "Link to this definition")

    total\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.models.replay.ReplayChatCompletionClient.total_usage "Link to this definition")

### autogen\_ext.models.azure[#](#module-autogen_ext.models.azure "Link to this heading")

*class* AzureAIChatCompletionClient(*\*\*kwargs: [Unpack](https://docs.python.org/3/library/typing.html#typing.Unpack "(in Python v3.13)")[[AzureAIChatCompletionClientConfig](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig "autogen_ext.models.azure.config.AzureAIChatCompletionClientConfig")]*)[#](#autogen_ext.models.azure.AzureAIChatCompletionClient "Link to this definition")
:   Bases: [`ChatCompletionClient`](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")

    Chat completion client for models hosted on Azure AI Foundry or GitHub Models.
    See [here](https://learn.microsoft.com/en-us/azure/ai-studio/reference/reference-model-inference-chat-completions) for more info.

    Parameters:
    :   * **endpoint** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The endpoint to use. **Required.**
        * **credential** (*union**,* *AzureKeyCredential**,* *AsyncTokenCredential*) – The credentials to use. **Required**
        * **model\_info** ([*ModelInfo*](#autogen_core.models.ModelInfo "autogen_core.models.ModelInfo")) – The model family and capabilities of the model. **Required.**
        * **model** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The name of the model. **Required if model is hosted on GitHub Models.**
        * **frequency\_penalty** – (optional,float)
        * **presence\_penalty** – (optional,float)
        * **temperature** – (optional,float)
        * **top\_p** – (optional,float)
        * **max\_tokens** – (optional,int)
        * **response\_format** – (optional, literal[“text”, “json\_object”])
        * **stop** – (optional,List[str])
        * **tools** – (optional,List[ChatCompletionsToolDefinition])
        * **tool\_choice** – (optional,Union[str, ChatCompletionsToolChoicePreset, ChatCompletionsNamedToolChoice]])
        * **seed** – (optional,int)
        * **model\_extras** – (optional,Dict[str, Any])

    To use this client, you must install the azure extra:

    ```
    pip install "autogen-ext[azure]"

    ```

    The following code snippet shows how to use the client with GitHub Models:

    ```
    import asyncio
    import os
    from azure.core.credentials import AzureKeyCredential
    from autogen_ext.models.azure import AzureAIChatCompletionClient
    from autogen_core.models import UserMessage

    async def main():
        client = AzureAIChatCompletionClient(
            model="Phi-4",
            endpoint="https://models.inference.ai.azure.com",
            # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.
            # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens
            credential=AzureKeyCredential(os.environ["GITHUB_TOKEN"]),
            model_info={
                "json_output": False,
                "function_calling": False,
                "vision": False,
                "family": "unknown",
                "structured_output": False,
            },
        )

        result = await client.create([UserMessage(content="What is the capital of France?", source="user")])
        print(result)

        # Close the client.
        await client.close()

    if __name__ == "__main__":
        asyncio.run(main())

    ```

    To use streaming, you can use the create\_stream method:

    ```
    import asyncio
    import os

    from autogen_core.models import UserMessage
    from autogen_ext.models.azure import AzureAIChatCompletionClient
    from azure.core.credentials import AzureKeyCredential

    async def main():
        client = AzureAIChatCompletionClient(
            model="Phi-4",
            endpoint="https://models.inference.ai.azure.com",
            # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.
            # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens
            credential=AzureKeyCredential(os.environ["GITHUB_TOKEN"]),
            model_info={
                "json_output": False,
                "function_calling": False,
                "vision": False,
                "family": "unknown",
                "structured_output": False,
            },
        )

        # Create a stream.
        stream = client.create_stream([UserMessage(content="Write a poem about the ocean", source="user")])
        async for chunk in stream:
            print(chunk, end="", flush=True)
        print()

        # Close the client.
        await client.close()

    if __name__ == "__main__":
        asyncio.run(main())

    ```

    actual\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.models.azure.AzureAIChatCompletionClient.actual_usage "Link to this definition")

    add\_usage(*usage: [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.models.azure.AzureAIChatCompletionClient.add_usage "Link to this definition")

    *property* capabilities*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClient.capabilities "Link to this definition")

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.models.azure.AzureAIChatCompletionClient.close "Link to this definition")

    count\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.models.azure.AzureAIChatCompletionClient.count_tokens "Link to this definition")

    *async* create(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult")[#](#autogen_ext.models.azure.AzureAIChatCompletionClient.create "Link to this definition")
    :   Creates a single response from the model.

        Parameters:
        :   * **messages** (*Sequence**[**LLMMessage**]*) – The messages to send to the model.
            * **tools** (*Sequence**[*[*Tool*](#autogen_core.tools.Tool "autogen_core.tools.Tool") *|* [*ToolSchema*](#autogen_core.tools.ToolSchema "autogen_core.tools.ToolSchema")*]**,* *optional*) – The tools to use with the model. Defaults to [].
            * **json\_output** (*Optional**[*[*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") *|* [*type*](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")*[**BaseModel**]**]**,* *optional*) – Whether to use JSON mode, structured output, or neither.
              Defaults to None. If set to a [Pydantic BaseModel](https://docs.pydantic.dev/latest/usage/models/#model) type,
              it will be used as the output type for structured output.
              If set to a boolean, it will be used to determine whether to use JSON mode or not.
              If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
            * **extra\_create\_args** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]**,* *optional*) – Extra arguments to pass to the underlying client. Defaults to {}.
            * **cancellation\_token** (*Optional**[*[*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken")*]**,* *optional*) – A token for cancellation. Defaults to None.

        Returns:
        :   **CreateResult** – The result of the model call.

    *async* create\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_ext.models.azure.AzureAIChatCompletionClient.create_stream "Link to this definition")
    :   Creates a stream of string chunks from the model ending with a CreateResult.

        Parameters:
        :   * **messages** (*Sequence**[**LLMMessage**]*) – The messages to send to the model.
            * **tools** (*Sequence**[*[*Tool*](#autogen_core.tools.Tool "autogen_core.tools.Tool") *|* [*ToolSchema*](#autogen_core.tools.ToolSchema "autogen_core.tools.ToolSchema")*]**,* *optional*) – The tools to use with the model. Defaults to [].
            * **json\_output** (*Optional**[*[*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") *|* [*type*](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")*[**BaseModel**]**]**,* *optional*) –

              Whether to use JSON mode, structured output, or neither.
              Defaults to None. If set to a [Pydantic BaseModel](https://docs.pydantic.dev/latest/usage/models/#model) type,
              it will be used as the output type for structured output.
              If set to a boolean, it will be used to determine whether to use JSON mode or not.
              If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
            * **extra\_create\_args** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]**,* *optional*) – Extra arguments to pass to the underlying client. Defaults to {}.
            * **cancellation\_token** (*Optional**[*[*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken")*]**,* *optional*) – A token for cancellation. Defaults to None.

        Returns:
        :   **AsyncGenerator[Union[str, CreateResult], None]** – A generator that yields string chunks and ends with a `CreateResult`.

    *property* model\_info*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClient.model_info "Link to this definition")

    remaining\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.models.azure.AzureAIChatCompletionClient.remaining_tokens "Link to this definition")

    total\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.models.azure.AzureAIChatCompletionClient.total_usage "Link to this definition")

*class* AzureAIChatCompletionClientConfig[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig "Link to this definition")
:   Bases: `AzureAIClientArguments`, `AzureAICreateArguments`

    credential*: AzureKeyCredential | AsyncTokenCredential*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.credential "Link to this definition")

    endpoint*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.endpoint "Link to this definition")

    frequency\_penalty*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.frequency_penalty "Link to this definition")

    max\_tokens*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.max_tokens "Link to this definition")

    model*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.model "Link to this definition")

    model\_extras*: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.model_extras "Link to this definition")

    model\_info*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.model_info "Link to this definition")

    presence\_penalty*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.presence_penalty "Link to this definition")

    response\_format*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['text', 'json\_object'] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.response_format "Link to this definition")

    seed*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.seed "Link to this definition")

    stop*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.stop "Link to this definition")

    temperature*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.temperature "Link to this definition")

    tool\_choice*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | ChatCompletionsToolChoicePreset | ChatCompletionsNamedToolChoice | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.tool_choice "Link to this definition")

    tools*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[ChatCompletionsToolDefinition] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.tools "Link to this definition")

    top\_p*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig.top_p "Link to this definition")

### autogen\_ext.models.anthropic[#](#module-autogen_ext.models.anthropic "Link to this heading")

*class* AnthropicChatCompletionClient(*\*\*kwargs: [Unpack](https://docs.python.org/3/library/typing.html#typing.Unpack "(in Python v3.13)")[[AnthropicClientConfiguration](#autogen_ext.models.anthropic.AnthropicClientConfiguration "autogen_ext.models.anthropic.config.AnthropicClientConfiguration")]*)[#](#autogen_ext.models.anthropic.AnthropicChatCompletionClient "Link to this definition")
:   Bases: [`BaseAnthropicChatCompletionClient`](#autogen_ext.models.anthropic.BaseAnthropicChatCompletionClient "autogen_ext.models.anthropic._anthropic_client.BaseAnthropicChatCompletionClient"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[[`AnthropicClientConfigurationConfigModel`](#autogen_ext.models.anthropic.AnthropicClientConfigurationConfigModel "autogen_ext.models.anthropic.config.AnthropicClientConfigurationConfigModel")]

    Chat completion client for Anthropic’s Claude models.

    Parameters:
    :   * **model** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The Claude model to use (e.g., “claude-3-sonnet-20240229”, “claude-3-opus-20240229”)
        * **api\_key** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – Anthropic API key. Required if not in environment variables.
        * **base\_url** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – Override the default API endpoint.
        * **max\_tokens** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*,* *optional*) – Maximum tokens in the response. Default is 4096.
        * **temperature** ([*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*,* *optional*) – Controls randomness. Lower is more deterministic. Default is 1.0.
        * **top\_p** ([*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")*,* *optional*) – Controls diversity via nucleus sampling. Default is 1.0.
        * **top\_k** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*,* *optional*) – Controls diversity via top-k sampling. Default is -1 (disabled).
        * **model\_info** ([*ModelInfo*](#autogen_core.models.ModelInfo "autogen_core.models.ModelInfo")*,* *optional*) – The capabilities of the model. Required if using a custom model.

    To use this client, you must install the Anthropic extension:

    ```
    pip install "autogen-ext[anthropic]"

    ```

    Example:

    ```
    import asyncio
    from autogen_ext.models.anthropic import AnthropicChatCompletionClient
    from autogen_core.models import UserMessage

    async def main():
        anthropic_client = AnthropicChatCompletionClient(
            model="claude-3-sonnet-20240229",
            api_key="your-api-key",  # Optional if ANTHROPIC_API_KEY is set in environment
        )

        result = await anthropic_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
        print(result)

    if __name__ == "__main__":
        asyncio.run(main())

    ```

    To load the client from a configuration:

    ```
    from autogen_core.models import ChatCompletionClient

    config = {
        "provider": "AnthropicChatCompletionClient",
        "config": {"model": "claude-3-sonnet-20240229"},
    }

    client = ChatCompletionClient.load_component(config)

    ```

    *classmethod* \_from\_config(*config: [AnthropicClientConfigurationConfigModel](#autogen_ext.models.anthropic.AnthropicClientConfigurationConfigModel "autogen_ext.models.anthropic.config.AnthropicClientConfigurationConfigModel")*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_ext.models.anthropic.AnthropicChatCompletionClient._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → [AnthropicClientConfigurationConfigModel](#autogen_ext.models.anthropic.AnthropicClientConfigurationConfigModel "autogen_ext.models.anthropic.config.AnthropicClientConfigurationConfigModel")[#](#autogen_ext.models.anthropic.AnthropicChatCompletionClient._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_ext.models.anthropic.AnthropicChatCompletionClient.component_config_schema "Link to this definition")
    :   alias of [`AnthropicClientConfigurationConfigModel`](#autogen_ext.models.anthropic.AnthropicClientConfigurationConfigModel "autogen_ext.models.anthropic.config.AnthropicClientConfigurationConfigModel")

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.models.anthropic.AnthropicChatCompletionClient'*[#](#autogen_ext.models.anthropic.AnthropicChatCompletionClient.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    component\_type*: ClassVar[ComponentType]* *= 'model'*[#](#autogen_ext.models.anthropic.AnthropicChatCompletionClient.component_type "Link to this definition")
    :   The logical type of the component.

*class* AnthropicClientConfiguration[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration "Link to this definition")
:   Bases: `BaseAnthropicClientConfiguration`

    api\_key*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.api_key "Link to this definition")

    base\_url*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.base_url "Link to this definition")

    default\_headers*: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.default_headers "Link to this definition")

    max\_retries*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.max_retries "Link to this definition")

    max\_tokens*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.max_tokens "Link to this definition")

    metadata*: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.metadata "Link to this definition")

    model*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.model "Link to this definition")

    model\_capabilities*: [ModelCapabilities](#autogen_core.models.ModelCapabilities "autogen_core.models._model_client.ModelCapabilities")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.model_capabilities "Link to this definition")

    model\_info*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.model_info "Link to this definition")

    response\_format*: ResponseFormat | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.response_format "Link to this definition")

    stop\_sequences*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.stop_sequences "Link to this definition")

    temperature*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.temperature "Link to this definition")

    timeout*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.timeout "Link to this definition")

    tool\_choice*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['auto', 'any', 'none'] | [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.tool_choice "Link to this definition")

    tools*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.tools "Link to this definition")

    top\_k*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.top_k "Link to this definition")

    top\_p*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*[#](#autogen_ext.models.anthropic.AnthropicClientConfiguration.top_p "Link to this definition")

*pydantic model* AnthropicClientConfigurationConfigModel[#](#autogen_ext.models.anthropic.AnthropicClientConfigurationConfigModel "Link to this definition")
:   Bases: `BaseAnthropicClientConfigurationConfigModel`

    Show JSON schema

    ```
    {
       "title": "AnthropicClientConfigurationConfigModel",
       "type": "object",
       "properties": {
          "model": {
             "title": "Model",
             "type": "string"
          },
          "max_tokens": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": 4096,
             "title": "Max Tokens"
          },
          "temperature": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": 1.0,
             "title": "Temperature"
          },
          "top_p": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Top P"
          },
          "top_k": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Top K"
          },
          "stop_sequences": {
             "anyOf": [
                {
                   "items": {
                      "type": "string"
                   },
                   "type": "array"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Stop Sequences"
          },
          "response_format": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ResponseFormat"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "metadata": {
             "anyOf": [
                {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Metadata"
          },
          "api_key": {
             "anyOf": [
                {
                   "format": "password",
                   "type": "string",
                   "writeOnly": true
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Api Key"
          },
          "base_url": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Base Url"
          },
          "model_capabilities": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ModelCapabilities"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "model_info": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ModelInfo"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "timeout": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Timeout"
          },
          "max_retries": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Max Retries"
          },
          "default_headers": {
             "anyOf": [
                {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Default Headers"
          },
          "tools": {
             "anyOf": [
                {
                   "items": {
                      "type": "object"
                   },
                   "type": "array"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Tools"
          },
          "tool_choice": {
             "anyOf": [
                {
                   "enum": [
                      "auto",
                      "any",
                      "none"
                   ],
                   "type": "string"
                },
                {
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Tool Choice"
          }
       },
       "$defs": {
          "ModelCapabilities": {
             "deprecated": true,
             "properties": {
                "vision": {
                   "title": "Vision",
                   "type": "boolean"
                },
                "function_calling": {
                   "title": "Function Calling",
                   "type": "boolean"
                },
                "json_output": {
                   "title": "Json Output",
                   "type": "boolean"
                }
             },
             "required": [
                "vision",
                "function_calling",
                "json_output"
             ],
             "title": "ModelCapabilities",
             "type": "object"
          },
          "ModelInfo": {
             "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
             "properties": {
                "vision": {
                   "title": "Vision",
                   "type": "boolean"
                },
                "function_calling": {
                   "title": "Function Calling",
                   "type": "boolean"
                },
                "json_output": {
                   "title": "Json Output",
                   "type": "boolean"
                },
                "family": {
                   "anyOf": [
                      {
                         "enum": [
                            "gpt-4o",
                            "o1",
                            "o3",
                            "gpt-4",
                            "gpt-35",
                            "r1",
                            "gemini-1.5-flash",
                            "gemini-1.5-pro",
                            "gemini-2.0-flash",
                            "claude-3-haiku",
                            "claude-3-sonnet",
                            "claude-3-opus",
                            "claude-3.5-haiku",
                            "claude-3.5-sonnet",
                            "unknown"
                         ],
                         "type": "string"
                      },
                      {
                         "type": "string"
                      }
                   ],
                   "title": "Family"
                },
                "structured_output": {
                   "title": "Structured Output",
                   "type": "boolean"
                }
             },
             "required": [
                "vision",
                "function_calling",
                "json_output",
                "family",
                "structured_output"
             ],
             "title": "ModelInfo",
             "type": "object"
          },
          "ResponseFormat": {
             "properties": {
                "type": {
                   "enum": [
                      "text",
                      "json_object"
                   ],
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "type"
             ],
             "title": "ResponseFormat",
             "type": "object"
          }
       },
       "required": [
          "model"
       ]
    }

    ```

    Fields:
    :   * `tool_choice (Literal['auto', 'any', 'none'] | Dict[str, Any] | None)`
        * `tools (List[Dict[str, Any]] | None)`

    *field* tool\_choice*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['auto', 'any', 'none'] | [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.anthropic.AnthropicClientConfigurationConfigModel.tool_choice "Link to this definition")

    *field* tools*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.anthropic.AnthropicClientConfigurationConfigModel.tools "Link to this definition")

*class* BaseAnthropicChatCompletionClient(*client: AsyncAnthropic*, *\**, *create\_args: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*, *model\_info: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.models.anthropic.BaseAnthropicChatCompletionClient "Link to this definition")
:   Bases: [`ChatCompletionClient`](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")

    actual\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.models.anthropic.BaseAnthropicChatCompletionClient.actual_usage "Link to this definition")

    *property* capabilities*: [ModelCapabilities](#autogen_core.models.ModelCapabilities "autogen_core.models._model_client.ModelCapabilities")*[#](#autogen_ext.models.anthropic.BaseAnthropicChatCompletionClient.capabilities "Link to this definition")

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.models.anthropic.BaseAnthropicChatCompletionClient.close "Link to this definition")

    count\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.models.anthropic.BaseAnthropicChatCompletionClient.count_tokens "Link to this definition")
    :   Estimate the number of tokens used by messages and tools.

        Note: This is an estimation based on common tokenization patterns and may not perfectly
        match Anthropic’s exact token counting for Claude models.

    *async* create(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult")[#](#autogen_ext.models.anthropic.BaseAnthropicChatCompletionClient.create "Link to this definition")
    :   Creates a single response from the model.

        Parameters:
        :   * **messages** (*Sequence**[**LLMMessage**]*) – The messages to send to the model.
            * **tools** (*Sequence**[*[*Tool*](#autogen_core.tools.Tool "autogen_core.tools.Tool") *|* [*ToolSchema*](#autogen_core.tools.ToolSchema "autogen_core.tools.ToolSchema")*]**,* *optional*) – The tools to use with the model. Defaults to [].
            * **json\_output** (*Optional**[*[*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") *|* [*type*](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")*[**BaseModel**]**]**,* *optional*) – Whether to use JSON mode, structured output, or neither.
              Defaults to None. If set to a [Pydantic BaseModel](https://docs.pydantic.dev/latest/usage/models/#model) type,
              it will be used as the output type for structured output.
              If set to a boolean, it will be used to determine whether to use JSON mode or not.
              If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
            * **extra\_create\_args** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]**,* *optional*) – Extra arguments to pass to the underlying client. Defaults to {}.
            * **cancellation\_token** (*Optional**[*[*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken")*]**,* *optional*) – A token for cancellation. Defaults to None.

        Returns:
        :   **CreateResult** – The result of the model call.

    *async* create\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *max\_consecutive\_empty\_chunk\_tolerance: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 0*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_ext.models.anthropic.BaseAnthropicChatCompletionClient.create_stream "Link to this definition")
    :   Creates an AsyncGenerator that yields a stream of completions based on the provided messages and tools.

    *property* model\_info*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*[#](#autogen_ext.models.anthropic.BaseAnthropicChatCompletionClient.model_info "Link to this definition")

    remaining\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.models.anthropic.BaseAnthropicChatCompletionClient.remaining_tokens "Link to this definition")
    :   Calculate the remaining tokens based on the model’s token limit.

    total\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.models.anthropic.BaseAnthropicChatCompletionClient.total_usage "Link to this definition")

*pydantic model* CreateArgumentsConfigModel[#](#autogen_ext.models.anthropic.CreateArgumentsConfigModel "Link to this definition")
:   Bases: `BaseModel`

    Show JSON schema

    ```
    {
       "title": "CreateArgumentsConfigModel",
       "type": "object",
       "properties": {
          "model": {
             "title": "Model",
             "type": "string"
          },
          "max_tokens": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": 4096,
             "title": "Max Tokens"
          },
          "temperature": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": 1.0,
             "title": "Temperature"
          },
          "top_p": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Top P"
          },
          "top_k": {
             "anyOf": [
                {
                   "type": "integer"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Top K"
          },
          "stop_sequences": {
             "anyOf": [
                {
                   "items": {
                      "type": "string"
                   },
                   "type": "array"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Stop Sequences"
          },
          "response_format": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ResponseFormat"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "metadata": {
             "anyOf": [
                {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Metadata"
          }
       },
       "$defs": {
          "ResponseFormat": {
             "properties": {
                "type": {
                   "enum": [
                      "text",
                      "json_object"
                   ],
                   "title": "Type",
                   "type": "string"
                }
             },
             "required": [
                "type"
             ],
             "title": "ResponseFormat",
             "type": "object"
          }
       },
       "required": [
          "model"
       ]
    }

    ```

    Fields:
    :   * `max_tokens (int | None)`
        * `metadata (Dict[str, str] | None)`
        * `model (str)`
        * `response_format (autogen_ext.models.anthropic.config.ResponseFormat | None)`
        * `stop_sequences (List[str] | None)`
        * `temperature (float | None)`
        * `top_k (int | None)`
        * `top_p (float | None)`

    *field* max\_tokens*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= 4096*[#](#autogen_ext.models.anthropic.CreateArgumentsConfigModel.max_tokens "Link to this definition")

    *field* metadata*: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.anthropic.CreateArgumentsConfigModel.metadata "Link to this definition")

    *field* model*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.models.anthropic.CreateArgumentsConfigModel.model "Link to this definition")

    *field* response\_format*: ResponseFormat | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.anthropic.CreateArgumentsConfigModel.response_format "Link to this definition")

    *field* stop\_sequences*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.anthropic.CreateArgumentsConfigModel.stop_sequences "Link to this definition")

    *field* temperature*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= 1.0*[#](#autogen_ext.models.anthropic.CreateArgumentsConfigModel.temperature "Link to this definition")

    *field* top\_k*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.anthropic.CreateArgumentsConfigModel.top_k "Link to this definition")

    *field* top\_p*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.anthropic.CreateArgumentsConfigModel.top_p "Link to this definition")

### autogen\_ext.models.semantic\_kernel[#](#module-autogen_ext.models.semantic_kernel "Link to this heading")

*class* SKChatCompletionAdapter(*sk\_client: ChatCompletionClientBase*, *kernel: Kernel | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *prompt\_settings: PromptExecutionSettings | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *model\_info: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *service\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.models.semantic_kernel.SKChatCompletionAdapter "Link to this definition")
:   Bases: [`ChatCompletionClient`](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")

    SKChatCompletionAdapter is an adapter that allows using Semantic Kernel model clients
    as Autogen ChatCompletion clients. This makes it possible to seamlessly integrate
    Semantic Kernel connectors (e.g., Azure OpenAI, Google Gemini, Ollama, etc.) into
    Autogen agents that rely on a ChatCompletionClient interface.

    By leveraging this adapter, you can:

    * Pass in a Kernel and any supported Semantic Kernel ChatCompletionClientBase connector.
    * Provide tools (via Autogen Tool or ToolSchema) for function calls during chat completion.
    * Stream responses or retrieve them in a single request.
    * Provide prompt settings to control the chat completion behavior either globally through the constructor
      :   or on a per-request basis through the extra\_create\_args dictionary.

    The list of extras that can be installed:

    * semantic-kernel-anthropic: Install this extra to use Anthropic models.
    * semantic-kernel-google: Install this extra to use Google Gemini models.
    * semantic-kernel-ollama: Install this extra to use Ollama models.
    * semantic-kernel-mistralai: Install this extra to use MistralAI models.
    * semantic-kernel-aws: Install this extra to use AWS models.
    * semantic-kernel-hugging-face: Install this extra to use Hugging Face models.

    Parameters:
    :   * **sk\_client** (*ChatCompletionClientBase*) – The Semantic Kernel client to wrap (e.g., AzureChatCompletion, GoogleAIChatCompletion, OllamaChatCompletion).
        * **kernel** (*Optional**[**Kernel**]*) – The Semantic Kernel instance to use for executing requests. If not provided, one must be passed
          in the extra\_create\_args for each request.
        * **prompt\_settings** (*Optional**[**PromptExecutionSettings**]*) – Default prompt execution settings to use. Can be overridden per request.
        * **model\_info** (*Optional**[*[*ModelInfo*](#autogen_core.models.ModelInfo "autogen_core.models.ModelInfo")*]*) – Information about the model’s capabilities.
        * **service\_id** (*Optional**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]*) – Optional service identifier.

    Examples

    Anthropic models with function calling:

    ```
    pip install "autogen-ext[semantic-kernel-anthropic]"

    ```

    ```
    import asyncio
    import os

    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.ui import Console
    from autogen_core.models import ModelFamily, UserMessage
    from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
    from semantic_kernel import Kernel
    from semantic_kernel.connectors.ai.anthropic import AnthropicChatCompletion, AnthropicChatPromptExecutionSettings
    from semantic_kernel.memory.null_memory import NullMemory

    async def get_weather(city: str) -> str:
        """Get the weather for a city."""
        return f"The weather in {city} is 75 degrees."

    async def main() -> None:
        sk_client = AnthropicChatCompletion(
            ai_model_id="claude-3-5-sonnet-20241022",
            api_key=os.environ["ANTHROPIC_API_KEY"],
            service_id="my-service-id",  # Optional; for targeting specific services within Semantic Kernel
        )
        settings = AnthropicChatPromptExecutionSettings(
            temperature=0.2,
        )

        model_client = SKChatCompletionAdapter(
            sk_client,
            kernel=Kernel(memory=NullMemory()),
            prompt_settings=settings,
            model_info={
                "function_calling": True,
                "json_output": True,
                "vision": True,
                "family": ModelFamily.CLAUDE_3_5_SONNET,
                "structured_output": True,
            },
        )

        # Call the model directly.
        response = await model_client.create([UserMessage(content="What is the capital of France?", source="test")])
        print(response)

        # Create an assistant agent with the model client.
        assistant = AssistantAgent(
            "assistant", model_client=model_client, system_message="You are a helpful assistant.", tools=[get_weather]
        )
        # Call the assistant with a task.
        await Console(assistant.run_stream(task="What is the weather in Paris and London?"))

    asyncio.run(main())

    ```

    Google Gemini models with function calling:

    ```
    pip install "autogen-ext[semantic-kernel-google]"

    ```

    ```
    import asyncio
    import os

    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.ui import Console
    from autogen_core.models import UserMessage, ModelFamily
    from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
    from semantic_kernel import Kernel
    from semantic_kernel.connectors.ai.google.google_ai import (
        GoogleAIChatCompletion,
        GoogleAIChatPromptExecutionSettings,
    )
    from semantic_kernel.memory.null_memory import NullMemory

    def get_weather(city: str) -> str:
        """Get the weather for a city."""
        return f"The weather in {city} is 75 degrees."

    async def main() -> None:
        sk_client = GoogleAIChatCompletion(
            gemini_model_id="gemini-2.0-flash",
            api_key=os.environ["GEMINI_API_KEY"],
        )
        settings = GoogleAIChatPromptExecutionSettings(
            temperature=0.2,
        )

        kernel = Kernel(memory=NullMemory())

        model_client = SKChatCompletionAdapter(
            sk_client,
            kernel=kernel,
            prompt_settings=settings,
            model_info={
                "family": ModelFamily.GEMINI_2_0_FLASH,
                "function_calling": True,
                "json_output": True,
                "vision": True,
                "structured_output": True,
            },
        )

        # Call the model directly.
        model_result = await model_client.create(
            messages=[UserMessage(content="What is the capital of France?", source="User")]
        )
        print(model_result)

        # Create an assistant agent with the model client.
        assistant = AssistantAgent(
            "assistant", model_client=model_client, tools=[get_weather], system_message="You are a helpful assistant."
        )
        # Call the assistant with a task.
        stream = assistant.run_stream(task="What is the weather in Paris and London?")
        await Console(stream)

    asyncio.run(main())

    ```

    Ollama models:

    ```
    pip install "autogen-ext[semantic-kernel-ollama]"

    ```

    ```
    import asyncio

    from autogen_agentchat.agents import AssistantAgent
    from autogen_core.models import UserMessage
    from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
    from semantic_kernel import Kernel
    from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion, OllamaChatPromptExecutionSettings
    from semantic_kernel.memory.null_memory import NullMemory

    async def main() -> None:
        sk_client = OllamaChatCompletion(
            host="http://localhost:11434",
            ai_model_id="llama3.2:latest",
        )
        ollama_settings = OllamaChatPromptExecutionSettings(
            options={"temperature": 0.5},
        )

        model_client = SKChatCompletionAdapter(
            sk_client, kernel=Kernel(memory=NullMemory()), prompt_settings=ollama_settings
        )

        # Call the model directly.
        model_result = await model_client.create(
            messages=[UserMessage(content="What is the capital of France?", source="User")]
        )
        print(model_result)

        # Create an assistant agent with the model client.
        assistant = AssistantAgent("assistant", model_client=model_client)
        # Call the assistant with a task.
        result = await assistant.run(task="What is the capital of France?")
        print(result)

    asyncio.run(main())

    ```

    actual\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.models.semantic_kernel.SKChatCompletionAdapter.actual_usage "Link to this definition")

    *property* capabilities*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*[#](#autogen_ext.models.semantic_kernel.SKChatCompletionAdapter.capabilities "Link to this definition")

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.models.semantic_kernel.SKChatCompletionAdapter.close "Link to this definition")

    count\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.models.semantic_kernel.SKChatCompletionAdapter.count_tokens "Link to this definition")

    *async* create(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult")[#](#autogen_ext.models.semantic_kernel.SKChatCompletionAdapter.create "Link to this definition")
    :   Create a chat completion using the Semantic Kernel client.

        The extra\_create\_args dictionary can include two special keys:

        1. “kernel” (optional):
           :   An instance of `semantic_kernel.Kernel` used to execute the request.
               If not provided either in constructor or extra\_create\_args, a ValueError is raised.
        2. “prompt\_execution\_settings” (optional):
           :   An instance of a `PromptExecutionSettings` subclass corresponding to the
               underlying Semantic Kernel client (e.g., AzureChatPromptExecutionSettings,
               GoogleAIChatPromptExecutionSettings). If not provided, the adapter’s default
               prompt settings will be used.

        Parameters:
        :   * **messages** – The list of LLM messages to send.
            * **tools** – The tools that may be invoked during the chat.
            * **json\_output** – Whether the model is expected to return JSON.
            * **extra\_create\_args** – Additional arguments to control the chat completion behavior.
            * **cancellation\_token** – Token allowing cancellation of the request.

        Returns:
        :   **CreateResult** – The result of the chat completion.

    *async* create\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_ext.models.semantic_kernel.SKChatCompletionAdapter.create_stream "Link to this definition")
    :   Create a streaming chat completion using the Semantic Kernel client.

        The extra\_create\_args dictionary can include two special keys:

        1. “kernel” (optional):
           :   An instance of `semantic_kernel.Kernel` used to execute the request.
               If not provided either in constructor or extra\_create\_args, a ValueError is raised.
        2. “prompt\_execution\_settings” (optional):
           :   An instance of a `PromptExecutionSettings` subclass corresponding to the
               underlying Semantic Kernel client (e.g., AzureChatPromptExecutionSettings,
               GoogleAIChatPromptExecutionSettings). If not provided, the adapter’s default
               prompt settings will be used.

        Parameters:
        :   * **messages** – The list of LLM messages to send.
            * **tools** – The tools that may be invoked during the chat.
            * **json\_output** – Whether the model is expected to return JSON.
            * **extra\_create\_args** – Additional arguments to control the chat completion behavior.
            * **cancellation\_token** – Token allowing cancellation of the request.

        Yields:
        :   *Union[str, CreateResult]* – Either a string chunk of the response or a CreateResult containing function calls.

    *property* model\_info*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*[#](#autogen_ext.models.semantic_kernel.SKChatCompletionAdapter.model_info "Link to this definition")

    remaining\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.models.semantic_kernel.SKChatCompletionAdapter.remaining_tokens "Link to this definition")

    total\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.models.semantic_kernel.SKChatCompletionAdapter.total_usage "Link to this definition")

### autogen\_ext.models.ollama[#](#module-autogen_ext.models.ollama "Link to this heading")

*pydantic model* BaseOllamaClientConfigurationConfigModel[#](#autogen_ext.models.ollama.BaseOllamaClientConfigurationConfigModel "Link to this definition")
:   Bases: [`CreateArgumentsConfigModel`](#autogen_ext.models.ollama.CreateArgumentsConfigModel "autogen_ext.models.ollama.config.CreateArgumentsConfigModel")

    Show JSON schema

    ```
    {
       "title": "BaseOllamaClientConfigurationConfigModel",
       "type": "object",
       "properties": {
          "model": {
             "title": "Model",
             "type": "string"
          },
          "host": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Host"
          },
          "response_format": {
             "default": null,
             "title": "Response Format"
          },
          "follow_redirects": {
             "default": true,
             "title": "Follow Redirects",
             "type": "boolean"
          },
          "timeout": {
             "default": null,
             "title": "Timeout"
          },
          "headers": {
             "anyOf": [
                {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Headers"
          },
          "model_capabilities": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ModelCapabilities"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "model_info": {
             "anyOf": [
                {
                   "$ref": "#/$defs/ModelInfo"
                },
                {
                   "type": "null"
                }
             ],
             "default": null
          },
          "options": {
             "anyOf": [
                {
                   "type": "object"
                },
                {
                   "$ref": "#/$defs/Options"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Options"
          }
       },
       "$defs": {
          "ModelCapabilities": {
             "deprecated": true,
             "properties": {
                "vision": {
                   "title": "Vision",
                   "type": "boolean"
                },
                "function_calling": {
                   "title": "Function Calling",
                   "type": "boolean"
                },
                "json_output": {
                   "title": "Json Output",
                   "type": "boolean"
                }
             },
             "required": [
                "vision",
                "function_calling",
                "json_output"
             ],
             "title": "ModelCapabilities",
             "type": "object"
          },
          "ModelInfo": {
             "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
             "properties": {
                "vision": {
                   "title": "Vision",
                   "type": "boolean"
                },
                "function_calling": {
                   "title": "Function Calling",
                   "type": "boolean"
                },
                "json_output": {
                   "title": "Json Output",
                   "type": "boolean"
                },
                "family": {
                   "anyOf": [
                      {
                         "enum": [
                            "gpt-4o",
                            "o1",
                            "o3",
                            "gpt-4",
                            "gpt-35",
                            "r1",
                            "gemini-1.5-flash",
                            "gemini-1.5-pro",
                            "gemini-2.0-flash",
                            "claude-3-haiku",
                            "claude-3-sonnet",
                            "claude-3-opus",
                            "claude-3.5-haiku",
                            "claude-3.5-sonnet",
                            "unknown"
                         ],
                         "type": "string"
                      },
                      {
                         "type": "string"
                      }
                   ],
                   "title": "Family"
                },
                "structured_output": {
                   "title": "Structured Output",
                   "type": "boolean"
                }
             },
             "required": [
                "vision",
                "function_calling",
                "json_output",
                "family",
                "structured_output"
             ],
             "title": "ModelInfo",
             "type": "object"
          },
          "Options": {
             "properties": {
                "numa": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Numa"
                },
                "num_ctx": {
                   "anyOf": [
                      {
                         "type": "integer"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Num Ctx"
                },
                "num_batch": {
                   "anyOf": [
                      {
                         "type": "integer"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Num Batch"
                },
                "num_gpu": {
                   "anyOf": [
                      {
                         "type": "integer"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Num Gpu"
                },
                "main_gpu": {
                   "anyOf": [
                      {
                         "type": "integer"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Main Gpu"
                },
                "low_vram": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Low Vram"
                },
                "f16_kv": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "F16 Kv"
                },
                "logits_all": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Logits All"
                },
                "vocab_only": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Vocab Only"
                },
                "use_mmap": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Use Mmap"
                },
                "use_mlock": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Use Mlock"
                },
                "embedding_only": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Embedding Only"
                },
                "num_thread": {
                   "anyOf": [
                      {
                         "type": "integer"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Num Thread"
                },
                "num_keep": {
                   "anyOf": [
                      {
                         "type": "integer"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Num Keep"
                },
                "seed": {
                   "anyOf": [
                      {
                         "type": "integer"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Seed"
                },
                "num_predict": {
                   "anyOf": [
                      {
                         "type": "integer"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Num Predict"
                },
                "top_k": {
                   "anyOf": [
                      {
                         "type": "integer"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Top K"
                },
                "top_p": {
                   "anyOf": [
                      {
                         "type": "number"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Top P"
                },
                "tfs_z": {
                   "anyOf": [
                      {
                         "type": "number"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Tfs Z"
                },
                "typical_p": {
                   "anyOf": [
                      {
                         "type": "number"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Typical P"
                },
                "repeat_last_n": {
                   "anyOf": [
                      {
                         "type": "integer"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Repeat Last N"
                },
                "temperature": {
                   "anyOf": [
                      {
                         "type": "number"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Temperature"
                },
                "repeat_penalty": {
                   "anyOf": [
                      {
                         "type": "number"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Repeat Penalty"
                },
                "presence_penalty": {
                   "anyOf": [
                      {
                         "type": "number"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Presence Penalty"
                },
                "frequency_penalty": {
                   "anyOf": [
                      {
                         "type": "number"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Frequency Penalty"
                },
                "mirostat": {
                   "anyOf": [
                      {
                         "type": "integer"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Mirostat"
                },
                "mirostat_tau": {
                   "anyOf": [
                      {
                         "type": "number"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Mirostat Tau"
                },
                "mirostat_eta": {
                   "anyOf": [
                      {
                         "type": "number"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Mirostat Eta"
                },
                "penalize_newline": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Penalize Newline"
                },
                "stop": {
                   "anyOf": [
                      {
                         "items": {
                            "type": "string"
                         },
                         "type": "array"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Stop"
                }
             },
             "title": "Options",
             "type": "object"
          }
       },
       "required": [
          "model"
       ]
    }

    ```

    Fields:
    :   * `follow_redirects (bool)`
        * `headers (Mapping[str, str] | None)`
        * `model_capabilities (autogen_core.models._model_client.ModelCapabilities | None)`
        * `model_info (autogen_core.models._model_client.ModelInfo | None)`
        * `options (Mapping[str, Any] | ollama._types.Options | None)`
        * `timeout (Any)`

    *field* follow\_redirects*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")* *= True*[#](#autogen_ext.models.ollama.BaseOllamaClientConfigurationConfigModel.follow_redirects "Link to this definition")

    *field* headers*: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.ollama.BaseOllamaClientConfigurationConfigModel.headers "Link to this definition")

    *field* model\_capabilities*: [ModelCapabilities](#autogen_core.models.ModelCapabilities "autogen_core.models._model_client.ModelCapabilities") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.ollama.BaseOllamaClientConfigurationConfigModel.model_capabilities "Link to this definition")

    *field* model\_info*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.ollama.BaseOllamaClientConfigurationConfigModel.model_info "Link to this definition")

    *field* options*: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | Options | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.ollama.BaseOllamaClientConfigurationConfigModel.options "Link to this definition")

    *field* timeout*: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")* *= None*[#](#autogen_ext.models.ollama.BaseOllamaClientConfigurationConfigModel.timeout "Link to this definition")

*pydantic model* CreateArgumentsConfigModel[#](#autogen_ext.models.ollama.CreateArgumentsConfigModel "Link to this definition")
:   Bases: `BaseModel`

    Show JSON schema

    ```
    {
       "title": "CreateArgumentsConfigModel",
       "type": "object",
       "properties": {
          "model": {
             "title": "Model",
             "type": "string"
          },
          "host": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Host"
          },
          "response_format": {
             "default": null,
             "title": "Response Format"
          }
       },
       "required": [
          "model"
       ]
    }

    ```

    Fields:
    :   * `host (str | None)`
        * `model (str)`
        * `response_format (Any)`

    *field* host*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.models.ollama.CreateArgumentsConfigModel.host "Link to this definition")

    *field* model*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.models.ollama.CreateArgumentsConfigModel.model "Link to this definition")

    *field* response\_format*: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")* *= None*[#](#autogen_ext.models.ollama.CreateArgumentsConfigModel.response_format "Link to this definition")

*class* OllamaChatCompletionClient(*\*\*kwargs: [Unpack](https://docs.python.org/3/library/typing.html#typing.Unpack "(in Python v3.13)")[BaseOllamaClientConfiguration]*)[#](#autogen_ext.models.ollama.OllamaChatCompletionClient "Link to this definition")
:   Bases: `BaseOllamaChatCompletionClient`, [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[[`BaseOllamaClientConfigurationConfigModel`](#autogen_ext.models.ollama.BaseOllamaClientConfigurationConfigModel "autogen_ext.models.ollama.config.BaseOllamaClientConfigurationConfigModel")]

    Chat completion client for Ollama hosted models.

    Ollama must be installed and the appropriate model pulled.

    Parameters:
    :   * **model** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – Which Ollama model to use.
        * **host** (*optional**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – Model host url.
        * **response\_format** (*optional**,* *pydantic.BaseModel*) – The format of the response. If provided, the response will be parsed into this format as json.
        * **options** (*optional**,* *Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]* *|* *Options*) – Additional options to pass to the Ollama client.
        * **model\_info** (*optional**,* [*ModelInfo*](#autogen_core.models.ModelInfo "autogen_core.models.ModelInfo")) – The capabilities of the model. **Required if the model is not listed in the ollama model info.**

    Note

    Only models with 200k+ downloads (as of Jan 21, 2025), + phi4, deepseek-r1 have pre-defined model infos. See [this file](https://github.com/microsoft/autogen/blob/main/python/packages/autogen-ext/src/autogen_ext/models/ollama/_model_info.py) for the full list. An entry for one model encompases all parameter variants of that model.

    To use this client, you must install the ollama extension:

    ```
    pip install "autogen-ext[ollama]"

    ```

    The following code snippet shows how to use the client with an Ollama model:

    ```
    from autogen_ext.models.ollama import OllamaChatCompletionClient
    from autogen_core.models import UserMessage

    ollama_client = OllamaChatCompletionClient(
        model="llama3",
    )

    result = await ollama_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
    print(result)

    ```

    To load the client from a configuration, you can use the load\_component method:

    ```
    from autogen_core.models import ChatCompletionClient

    config = {
        "provider": "OllamaChatCompletionClient",
        "config": {"model": "llama3"},
    }

    client = ChatCompletionClient.load_component(config)

    ```

    To output structured data, you can use the response\_format argument:

    ```
    from autogen_ext.models.ollama import OllamaChatCompletionClient
    from autogen_core.models import UserMessage
    from pydantic import BaseModel

    class StructuredOutput(BaseModel):
        first_name: str
        last_name: str

    ollama_client = OllamaChatCompletionClient(
        model="llama3",
        response_format=StructuredOutput,
    )
    result = await ollama_client.create([UserMessage(content="Who was the first man on the moon?", source="user")])  # type: ignore
    print(result)

    ```

    Note

    Tool usage in ollama is stricter than in its OpenAI counterparts. While OpenAI accepts a map of [str, Any], Ollama requires a map of [str, Property] where Property is a typed object containing `type` and `description` fields. Therefore, only the keys `type` and `description` will be converted from the properties blob in the tool schema.

    To view the full list of available configuration options, see the `OllamaClientConfigurationConfigModel` class.

    *classmethod* \_from\_config(*config: [BaseOllamaClientConfigurationConfigModel](#autogen_ext.models.ollama.BaseOllamaClientConfigurationConfigModel "autogen_ext.models.ollama.config.BaseOllamaClientConfigurationConfigModel")*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_ext.models.ollama.OllamaChatCompletionClient._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → [BaseOllamaClientConfigurationConfigModel](#autogen_ext.models.ollama.BaseOllamaClientConfigurationConfigModel "autogen_ext.models.ollama.config.BaseOllamaClientConfigurationConfigModel")[#](#autogen_ext.models.ollama.OllamaChatCompletionClient._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_ext.models.ollama.OllamaChatCompletionClient.component_config_schema "Link to this definition")
    :   alias of [`BaseOllamaClientConfigurationConfigModel`](#autogen_ext.models.ollama.BaseOllamaClientConfigurationConfigModel "autogen_ext.models.ollama.config.BaseOllamaClientConfigurationConfigModel")

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.models.ollama.OllamaChatCompletionClient'*[#](#autogen_ext.models.ollama.OllamaChatCompletionClient.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    component\_type*: ClassVar[ComponentType]* *= 'model'*[#](#autogen_ext.models.ollama.OllamaChatCompletionClient.component_type "Link to this definition")
    :   The logical type of the component.

### autogen\_ext.models.llama\_cpp[#](#module-autogen_ext.models.llama_cpp "Link to this heading")

*class* LlamaCppChatCompletionClient(*model\_info: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *\*\*kwargs: [Unpack](https://docs.python.org/3/library/typing.html#typing.Unpack "(in Python v3.13)")[LlamaCppParams]*)[#](#autogen_ext.models.llama_cpp.LlamaCppChatCompletionClient "Link to this definition")
:   Bases: [`ChatCompletionClient`](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")

    Chat completion client for LlamaCpp models.
    To use this client, you must install the llama-cpp extra:

    ```
    pip install "autogen-ext[llama-cpp]"

    ```

    This client allows you to interact with LlamaCpp models, either by specifying a local model path or by downloading a model from Hugging Face Hub.

    Parameters:
    :   * **model\_info** (*optional**,* [*ModelInfo*](#autogen_core.models.ModelInfo "autogen_core.models.ModelInfo")) – The information about the model. Defaults to [`DEFAULT_MODEL_INFO`](#autogen_ext.models.llama_cpp.LlamaCppChatCompletionClient.DEFAULT_MODEL_INFO "autogen_ext.models.llama_cpp.LlamaCppChatCompletionClient.DEFAULT_MODEL_INFO").
        * **model\_path** (*optional**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The path to the LlamaCpp model file. Required if repo\_id and filename are not provided.
        * **repo\_id** (*optional**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The Hugging Face Hub repository ID. Required if model\_path is not provided.
        * **filename** (*optional**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The filename of the model within the Hugging Face Hub repository. Required if model\_path is not provided.
        * **n\_gpu\_layers** (*optional**,* [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The number of layers to put on the GPU.
        * **n\_ctx** (*optional**,* [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The context size.
        * **n\_batch** (*optional**,* [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The batch size.
        * **verbose** (*optional**,* [*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")) – Whether to print verbose output.
        * **\*\*kwargs** – Additional parameters to pass to the Llama class.

    Examples

    The following code snippet shows how to use the client with a local model file:

    ```
    import asyncio

    from autogen_core.models import UserMessage
    from autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient

    async def main():
        llama_client = LlamaCppChatCompletionClient(model_path="/path/to/your/model.gguf")
        result = await llama_client.create([UserMessage(content="What is the capital of France?", source="user")])
        print(result)

    asyncio.run(main())

    ```

    The following code snippet shows how to use the client with a model from Hugging Face Hub:

    ```
    import asyncio

    from autogen_core.models import UserMessage
    from autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient

    async def main():
        llama_client = LlamaCppChatCompletionClient(
            repo_id="unsloth/phi-4-GGUF", filename="phi-4-Q2_K_L.gguf", n_gpu_layers=-1, seed=1337, n_ctx=5000
        )
        result = await llama_client.create([UserMessage(content="What is the capital of France?", source="user")])
        print(result)

    asyncio.run(main())

    ```

    DEFAULT\_MODEL\_INFO*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")* *= {'family': 'unknown', 'function\_calling': True, 'json\_output': True, 'structured\_output': True, 'vision': False}*[#](#autogen_ext.models.llama_cpp.LlamaCppChatCompletionClient.DEFAULT_MODEL_INFO "Link to this definition")

    *async* create(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult")[#](#autogen_ext.models.llama_cpp.LlamaCppChatCompletionClient.create "Link to this definition")
    :   Creates a single response from the model.

        Parameters:
        :   * **messages** (*Sequence**[**LLMMessage**]*) – The messages to send to the model.
            * **tools** (*Sequence**[*[*Tool*](#autogen_core.tools.Tool "autogen_core.tools.Tool") *|* [*ToolSchema*](#autogen_core.tools.ToolSchema "autogen_core.tools.ToolSchema")*]**,* *optional*) – The tools to use with the model. Defaults to [].
            * **json\_output** (*Optional**[*[*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") *|* [*type*](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")*[**BaseModel**]**]**,* *optional*) – Whether to use JSON mode, structured output, or neither.
              Defaults to None. If set to a [Pydantic BaseModel](https://docs.pydantic.dev/latest/usage/models/#model) type,
              it will be used as the output type for structured output.
              If set to a boolean, it will be used to determine whether to use JSON mode or not.
              If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
            * **extra\_create\_args** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]**,* *optional*) – Extra arguments to pass to the underlying client. Defaults to {}.
            * **cancellation\_token** (*Optional**[*[*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken")*]**,* *optional*) – A token for cancellation. Defaults to None.

        Returns:
        :   **CreateResult** – The result of the model call.

    *async* create\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_ext.models.llama_cpp.LlamaCppChatCompletionClient.create_stream "Link to this definition")
    :   Creates a stream of string chunks from the model ending with a CreateResult.

        Parameters:
        :   * **messages** (*Sequence**[**LLMMessage**]*) – The messages to send to the model.
            * **tools** (*Sequence**[*[*Tool*](#autogen_core.tools.Tool "autogen_core.tools.Tool") *|* [*ToolSchema*](#autogen_core.tools.ToolSchema "autogen_core.tools.ToolSchema")*]**,* *optional*) – The tools to use with the model. Defaults to [].
            * **json\_output** (*Optional**[*[*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") *|* [*type*](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")*[**BaseModel**]**]**,* *optional*) –

              Whether to use JSON mode, structured output, or neither.
              Defaults to None. If set to a [Pydantic BaseModel](https://docs.pydantic.dev/latest/usage/models/#model) type,
              it will be used as the output type for structured output.
              If set to a boolean, it will be used to determine whether to use JSON mode or not.
              If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
            * **extra\_create\_args** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]**,* *optional*) – Extra arguments to pass to the underlying client. Defaults to {}.
            * **cancellation\_token** (*Optional**[*[*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken")*]**,* *optional*) – A token for cancellation. Defaults to None.

        Returns:
        :   **AsyncGenerator[Union[str, CreateResult], None]** – A generator that yields string chunks and ends with a `CreateResult`.

    actual\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.models.llama_cpp.LlamaCppChatCompletionClient.actual_usage "Link to this definition")

    *property* capabilities*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*[#](#autogen_ext.models.llama_cpp.LlamaCppChatCompletionClient.capabilities "Link to this definition")

    count\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage")]*, *\*\*kwargs: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.models.llama_cpp.LlamaCppChatCompletionClient.count_tokens "Link to this definition")

    *property* model\_info*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*[#](#autogen_ext.models.llama_cpp.LlamaCppChatCompletionClient.model_info "Link to this definition")

    remaining\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage")]*, *\*\*kwargs: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.models.llama_cpp.LlamaCppChatCompletionClient.remaining_tokens "Link to this definition")

    total\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.models.llama_cpp.LlamaCppChatCompletionClient.total_usage "Link to this definition")

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.models.llama_cpp.LlamaCppChatCompletionClient.close "Link to this definition")
    :   Close the LlamaCpp client.

### autogen\_ext.tools.code\_execution[#](#module-autogen_ext.tools.code_execution "Link to this heading")

*pydantic model* CodeExecutionInput[#](#autogen_ext.tools.code_execution.CodeExecutionInput "Link to this definition")
:   Bases: `BaseModel`

    Show JSON schema

    ```
    {
       "title": "CodeExecutionInput",
       "type": "object",
       "properties": {
          "code": {
             "description": "The contents of the Python code block that should be executed",
             "title": "Code",
             "type": "string"
          }
       },
       "required": [
          "code"
       ]
    }

    ```

    Fields:
    :   * `code (str)`

    *field* code*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.tools.code_execution.CodeExecutionInput.code "Link to this definition")
    :   The contents of the Python code block that should be executed

*pydantic model* CodeExecutionResult[#](#autogen_ext.tools.code_execution.CodeExecutionResult "Link to this definition")
:   Bases: `BaseModel`

    Show JSON schema

    ```
    {
       "title": "CodeExecutionResult",
       "type": "object",
       "properties": {
          "success": {
             "title": "Success",
             "type": "boolean"
          },
          "output": {
             "title": "Output",
             "type": "string"
          }
       },
       "required": [
          "success",
          "output"
       ]
    }

    ```

    Fields:
    :   * `output (str)`
        * `success (bool)`

    *field* output*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.tools.code_execution.CodeExecutionResult.output "Link to this definition")

    *field* success*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")* *[Required]*[#](#autogen_ext.tools.code_execution.CodeExecutionResult.success "Link to this definition")

    ser\_model() → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.tools.code_execution.CodeExecutionResult.ser_model "Link to this definition")

*class* PythonCodeExecutionTool(*executor: [CodeExecutor](#autogen_core.code_executor.CodeExecutor "autogen_core.code_executor._base.CodeExecutor")*)[#](#autogen_ext.tools.code_execution.PythonCodeExecutionTool "Link to this definition")
:   Bases: [`BaseTool`](#autogen_core.tools.BaseTool "autogen_core.tools._base.BaseTool")[[`CodeExecutionInput`](#autogen_ext.tools.code_execution.CodeExecutionInput "autogen_ext.tools.code_execution._code_execution.CodeExecutionInput"), [`CodeExecutionResult`](#autogen_ext.tools.code_execution.CodeExecutionResult "autogen_ext.tools.code_execution._code_execution.CodeExecutionResult")], [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`PythonCodeExecutionToolConfig`]

    A tool that executes Python code in a code executor and returns output.

    Example executors:

    * [`autogen_ext.code_executors.local.LocalCommandLineCodeExecutor`](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor "autogen_ext.code_executors.local.LocalCommandLineCodeExecutor")
    * [`autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor`](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor "autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor")
    * [`autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor`](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor "autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor")

    Example usage:

    ```
    pip install -U "autogen-agentchat" "autogen-ext[openai]" "yfinance" "matplotlib"

    ```

    ```
    import asyncio
    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.ui import Console
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
    from autogen_ext.tools.code_execution import PythonCodeExecutionTool

    async def main() -> None:
        tool = PythonCodeExecutionTool(LocalCommandLineCodeExecutor(work_dir="coding"))
        agent = AssistantAgent(
            "assistant", OpenAIChatCompletionClient(model="gpt-4o"), tools=[tool], reflect_on_tool_use=True
        )
        await Console(
            agent.run_stream(
                task="Create a plot of MSFT stock prices in 2024 and save it to a file. Use yfinance and matplotlib."
            )
        )

    asyncio.run(main())

    ```

    Parameters:
    :   **executor** ([*CodeExecutor*](#autogen_core.code_executor.CodeExecutor "autogen_core.code_executor.CodeExecutor")) – The code executor that will be used to execute the code blocks.

    component\_config\_schema[#](#autogen_ext.tools.code_execution.PythonCodeExecutionTool.component_config_schema "Link to this definition")
    :   alias of `PythonCodeExecutionToolConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.tools.code\_execution.PythonCodeExecutionTool'*[#](#autogen_ext.tools.code_execution.PythonCodeExecutionTool.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* run(*args: [CodeExecutionInput](#autogen_ext.tools.code_execution.CodeExecutionInput "autogen_ext.tools.code_execution._code_execution.CodeExecutionInput")*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [CodeExecutionResult](#autogen_ext.tools.code_execution.CodeExecutionResult "autogen_ext.tools.code_execution._code_execution.CodeExecutionResult")[#](#autogen_ext.tools.code_execution.PythonCodeExecutionTool.run "Link to this definition")

### autogen\_ext.tools.graphrag[#](#module-autogen_ext.tools.graphrag "Link to this heading")

*pydantic model* GlobalContextConfig[#](#autogen_ext.tools.graphrag.GlobalContextConfig "Link to this definition")
:   Bases: `ContextConfig`

    Show JSON schema

    ```
    {
       "title": "GlobalContextConfig",
       "type": "object",
       "properties": {
          "max_data_tokens": {
             "default": 12000,
             "title": "Max Data Tokens",
             "type": "integer"
          },
          "use_community_summary": {
             "default": false,
             "title": "Use Community Summary",
             "type": "boolean"
          },
          "shuffle_data": {
             "default": true,
             "title": "Shuffle Data",
             "type": "boolean"
          },
          "include_community_rank": {
             "default": true,
             "title": "Include Community Rank",
             "type": "boolean"
          },
          "min_community_rank": {
             "default": 0,
             "title": "Min Community Rank",
             "type": "integer"
          },
          "community_rank_name": {
             "default": "rank",
             "title": "Community Rank Name",
             "type": "string"
          },
          "include_community_weight": {
             "default": true,
             "title": "Include Community Weight",
             "type": "boolean"
          },
          "community_weight_name": {
             "default": "occurrence weight",
             "title": "Community Weight Name",
             "type": "string"
          },
          "normalize_community_weight": {
             "default": true,
             "title": "Normalize Community Weight",
             "type": "boolean"
          }
       }
    }

    ```

    Fields:
    :   * `community_rank_name (str)`
        * `community_weight_name (str)`
        * `include_community_rank (bool)`
        * `include_community_weight (bool)`
        * `max_data_tokens (int)`
        * `min_community_rank (int)`
        * `normalize_community_weight (bool)`
        * `shuffle_data (bool)`
        * `use_community_summary (bool)`

    *field* community\_rank\_name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'rank'*[#](#autogen_ext.tools.graphrag.GlobalContextConfig.community_rank_name "Link to this definition")

    *field* community\_weight\_name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'occurrence weight'*[#](#autogen_ext.tools.graphrag.GlobalContextConfig.community_weight_name "Link to this definition")

    *field* include\_community\_rank*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")* *= True*[#](#autogen_ext.tools.graphrag.GlobalContextConfig.include_community_rank "Link to this definition")

    *field* include\_community\_weight*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")* *= True*[#](#autogen_ext.tools.graphrag.GlobalContextConfig.include_community_weight "Link to this definition")

    *field* max\_data\_tokens*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")* *= 12000*[#](#autogen_ext.tools.graphrag.GlobalContextConfig.max_data_tokens "Link to this definition")

    *field* min\_community\_rank*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")* *= 0*[#](#autogen_ext.tools.graphrag.GlobalContextConfig.min_community_rank "Link to this definition")

    *field* normalize\_community\_weight*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")* *= True*[#](#autogen_ext.tools.graphrag.GlobalContextConfig.normalize_community_weight "Link to this definition")

    *field* shuffle\_data*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")* *= True*[#](#autogen_ext.tools.graphrag.GlobalContextConfig.shuffle_data "Link to this definition")

    *field* use\_community\_summary*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")* *= False*[#](#autogen_ext.tools.graphrag.GlobalContextConfig.use_community_summary "Link to this definition")

*pydantic model* GlobalDataConfig[#](#autogen_ext.tools.graphrag.GlobalDataConfig "Link to this definition")
:   Bases: `DataConfig`

    Show JSON schema

    ```
    {
       "title": "GlobalDataConfig",
       "type": "object",
       "properties": {
          "input_dir": {
             "title": "Input Dir",
             "type": "string"
          },
          "entity_table": {
             "default": "create_final_nodes",
             "title": "Entity Table",
             "type": "string"
          },
          "entity_embedding_table": {
             "default": "create_final_entities",
             "title": "Entity Embedding Table",
             "type": "string"
          },
          "community_level": {
             "default": 2,
             "title": "Community Level",
             "type": "integer"
          },
          "community_table": {
             "default": "create_final_communities",
             "title": "Community Table",
             "type": "string"
          },
          "community_report_table": {
             "default": "create_final_community_reports",
             "title": "Community Report Table",
             "type": "string"
          }
       },
       "required": [
          "input_dir"
       ]
    }

    ```

    Fields:
    :   * `community_report_table (str)`
        * `community_table (str)`

    *field* community\_report\_table*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'create\_final\_community\_reports'*[#](#autogen_ext.tools.graphrag.GlobalDataConfig.community_report_table "Link to this definition")

    *field* community\_table*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'create\_final\_communities'*[#](#autogen_ext.tools.graphrag.GlobalDataConfig.community_table "Link to this definition")

*class* GlobalSearchTool(*token\_encoder: Encoding*, *llm: BaseLLM*, *data\_config: [GlobalDataConfig](#autogen_ext.tools.graphrag.GlobalDataConfig "autogen_ext.tools.graphrag._config.GlobalDataConfig")*, *context\_config: [GlobalContextConfig](#autogen_ext.tools.graphrag.GlobalContextConfig "autogen_ext.tools.graphrag._config.GlobalContextConfig") = \_default\_context\_config*, *mapreduce\_config: [MapReduceConfig](#autogen_ext.tools.graphrag.MapReduceConfig "autogen_ext.tools.graphrag._config.MapReduceConfig") = \_default\_mapreduce\_config*)[#](#autogen_ext.tools.graphrag.GlobalSearchTool "Link to this definition")
:   Bases: [`BaseTool`](#autogen_core.tools.BaseTool "autogen_core.tools._base.BaseTool")[[`GlobalSearchToolArgs`](#autogen_ext.tools.graphrag.GlobalSearchToolArgs "autogen_ext.tools.graphrag._global_search.GlobalSearchToolArgs"), [`GlobalSearchToolReturn`](#autogen_ext.tools.graphrag.GlobalSearchToolReturn "autogen_ext.tools.graphrag._global_search.GlobalSearchToolReturn")]

    Enables running GraphRAG global search queries as an AutoGen tool.

    This tool allows you to perform semantic search over a corpus of documents using the GraphRAG framework.
    The search combines graph-based document relationships with semantic embeddings to find relevant information.

    Note

    This tool requires the `graphrag` extra for the `autogen-ext` package.

    To install:

    ```
    pip install -U "autogen-agentchat" "autogen-ext[graphrag]"

    ```

    Before using this tool, you must complete the GraphRAG setup and indexing process:

    1. Follow the GraphRAG documentation to initialize your project and settings
    2. Configure and tune your prompts for the specific use case
    3. Run the indexing process to generate the required data files
    4. Ensure you have the settings.yaml file from the setup process

    Please refer to the [GraphRAG documentation](<https://microsoft.github.io/graphrag/>)
    for detailed instructions on completing these prerequisite steps.

    Example usage with AssistantAgent:

    ```
    import asyncio
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_agentchat.ui import Console
    from autogen_ext.tools.graphrag import GlobalSearchTool
    from autogen_agentchat.agents import AssistantAgent

    async def main():
        # Initialize the OpenAI client
        openai_client = OpenAIChatCompletionClient(
            model="gpt-4o-mini",
            api_key="<api-key>",
        )

        # Set up global search tool
        global_tool = GlobalSearchTool.from_settings(settings_path="./settings.yaml")

        # Create assistant agent with the global search tool
        assistant_agent = AssistantAgent(
            name="search_assistant",
            tools=[global_tool],
            model_client=openai_client,
            system_message=(
                "You are a tool selector AI assistant using the GraphRAG framework. "
                "Your primary task is to determine the appropriate search tool to call based on the user's query. "
                "For broader, abstract questions requiring a comprehensive understanding of the dataset, call the 'global_search' function."
            ),
        )

        # Run a sample query
        query = "What is the overall sentiment of the community reports?"
        await Console(assistant_agent.run_stream(task=query))

    if __name__ == "__main__":
        asyncio.run(main())

    ```

    *classmethod* from\_settings(*settings\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)")*) → [GlobalSearchTool](#autogen_ext.tools.graphrag.GlobalSearchTool "autogen_ext.tools.graphrag._global_search.GlobalSearchTool")[#](#autogen_ext.tools.graphrag.GlobalSearchTool.from_settings "Link to this definition")
    :   Create a GlobalSearchTool instance from GraphRAG settings file.

        Parameters:
        :   **settings\_path** – Path to the GraphRAG settings.yaml file

        Returns:
        :   **An initialized GlobalSearchTool instance**

    *async* run(*args: [GlobalSearchToolArgs](#autogen_ext.tools.graphrag.GlobalSearchToolArgs "autogen_ext.tools.graphrag._global_search.GlobalSearchToolArgs")*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [GlobalSearchToolReturn](#autogen_ext.tools.graphrag.GlobalSearchToolReturn "autogen_ext.tools.graphrag._global_search.GlobalSearchToolReturn")[#](#autogen_ext.tools.graphrag.GlobalSearchTool.run "Link to this definition")

*pydantic model* GlobalSearchToolArgs[#](#autogen_ext.tools.graphrag.GlobalSearchToolArgs "Link to this definition")
:   Bases: `BaseModel`

    Show JSON schema

    ```
    {
       "title": "GlobalSearchToolArgs",
       "type": "object",
       "properties": {
          "query": {
             "description": "The user query to perform global search on.",
             "title": "Query",
             "type": "string"
          }
       },
       "required": [
          "query"
       ]
    }

    ```

    Fields:
    :   * `query (str)`

    *field* query*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.tools.graphrag.GlobalSearchToolArgs.query "Link to this definition")
    :   The user query to perform global search on.

*pydantic model* GlobalSearchToolReturn[#](#autogen_ext.tools.graphrag.GlobalSearchToolReturn "Link to this definition")
:   Bases: `BaseModel`

    Show JSON schema

    ```
    {
       "title": "GlobalSearchToolReturn",
       "type": "object",
       "properties": {
          "answer": {
             "title": "Answer",
             "type": "string"
          }
       },
       "required": [
          "answer"
       ]
    }

    ```

    Fields:
    :   * `answer (str)`

    *field* answer*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.tools.graphrag.GlobalSearchToolReturn.answer "Link to this definition")

*pydantic model* LocalContextConfig[#](#autogen_ext.tools.graphrag.LocalContextConfig "Link to this definition")
:   Bases: `ContextConfig`

    Show JSON schema

    ```
    {
       "title": "LocalContextConfig",
       "type": "object",
       "properties": {
          "max_data_tokens": {
             "default": 8000,
             "title": "Max Data Tokens",
             "type": "integer"
          },
          "text_unit_prop": {
             "default": 0.5,
             "title": "Text Unit Prop",
             "type": "number"
          },
          "community_prop": {
             "default": 0.25,
             "title": "Community Prop",
             "type": "number"
          },
          "include_entity_rank": {
             "default": true,
             "title": "Include Entity Rank",
             "type": "boolean"
          },
          "rank_description": {
             "default": "number of relationships",
             "title": "Rank Description",
             "type": "string"
          },
          "include_relationship_weight": {
             "default": true,
             "title": "Include Relationship Weight",
             "type": "boolean"
          },
          "relationship_ranking_attribute": {
             "default": "rank",
             "title": "Relationship Ranking Attribute",
             "type": "string"
          }
       }
    }

    ```

    Fields:
    :   * `community_prop (float)`
        * `include_entity_rank (bool)`
        * `include_relationship_weight (bool)`
        * `rank_description (str)`
        * `relationship_ranking_attribute (str)`
        * `text_unit_prop (float)`

    *field* community\_prop*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")* *= 0.25*[#](#autogen_ext.tools.graphrag.LocalContextConfig.community_prop "Link to this definition")

    *field* include\_entity\_rank*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")* *= True*[#](#autogen_ext.tools.graphrag.LocalContextConfig.include_entity_rank "Link to this definition")

    *field* include\_relationship\_weight*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")* *= True*[#](#autogen_ext.tools.graphrag.LocalContextConfig.include_relationship_weight "Link to this definition")

    *field* rank\_description*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'number of relationships'*[#](#autogen_ext.tools.graphrag.LocalContextConfig.rank_description "Link to this definition")

    *field* relationship\_ranking\_attribute*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'rank'*[#](#autogen_ext.tools.graphrag.LocalContextConfig.relationship_ranking_attribute "Link to this definition")

    *field* text\_unit\_prop*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")* *= 0.5*[#](#autogen_ext.tools.graphrag.LocalContextConfig.text_unit_prop "Link to this definition")

*pydantic model* LocalDataConfig[#](#autogen_ext.tools.graphrag.LocalDataConfig "Link to this definition")
:   Bases: `DataConfig`

    Show JSON schema

    ```
    {
       "title": "LocalDataConfig",
       "type": "object",
       "properties": {
          "input_dir": {
             "title": "Input Dir",
             "type": "string"
          },
          "entity_table": {
             "default": "create_final_nodes",
             "title": "Entity Table",
             "type": "string"
          },
          "entity_embedding_table": {
             "default": "create_final_entities",
             "title": "Entity Embedding Table",
             "type": "string"
          },
          "community_level": {
             "default": 2,
             "title": "Community Level",
             "type": "integer"
          },
          "relationship_table": {
             "default": "create_final_relationships",
             "title": "Relationship Table",
             "type": "string"
          },
          "text_unit_table": {
             "default": "create_final_text_units",
             "title": "Text Unit Table",
             "type": "string"
          }
       },
       "required": [
          "input_dir"
       ]
    }

    ```

    Fields:
    :   * `relationship_table (str)`
        * `text_unit_table (str)`

    *field* relationship\_table*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'create\_final\_relationships'*[#](#autogen_ext.tools.graphrag.LocalDataConfig.relationship_table "Link to this definition")

    *field* text\_unit\_table*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'create\_final\_text\_units'*[#](#autogen_ext.tools.graphrag.LocalDataConfig.text_unit_table "Link to this definition")

*class* LocalSearchTool(*token\_encoder: Encoding*, *llm: BaseLLM*, *embedder: BaseTextEmbedding*, *data\_config: [LocalDataConfig](#autogen_ext.tools.graphrag.LocalDataConfig "autogen_ext.tools.graphrag._config.LocalDataConfig")*, *context\_config: [LocalContextConfig](#autogen_ext.tools.graphrag.LocalContextConfig "autogen_ext.tools.graphrag._config.LocalContextConfig") = \_default\_context\_config*, *search\_config: [SearchConfig](#autogen_ext.tools.graphrag.SearchConfig "autogen_ext.tools.graphrag._config.SearchConfig") = \_default\_search\_config*)[#](#autogen_ext.tools.graphrag.LocalSearchTool "Link to this definition")
:   Bases: [`BaseTool`](#autogen_core.tools.BaseTool "autogen_core.tools._base.BaseTool")[[`LocalSearchToolArgs`](#autogen_ext.tools.graphrag.LocalSearchToolArgs "autogen_ext.tools.graphrag._local_search.LocalSearchToolArgs"), [`LocalSearchToolReturn`](#autogen_ext.tools.graphrag.LocalSearchToolReturn "autogen_ext.tools.graphrag._local_search.LocalSearchToolReturn")]

    Enables running GraphRAG local search queries as an AutoGen tool.

    This tool allows you to perform semantic search over a corpus of documents using the GraphRAG framework.
    The search combines local document context with semantic embeddings to find relevant information.

    Note

    This tool requires the `graphrag` extra for the `autogen-ext` package.
    To install:

    ```
    pip install -U "autogen-agentchat" "autogen-ext[graphrag]"

    ```

    Before using this tool, you must complete the GraphRAG setup and indexing process:

    1. Follow the GraphRAG documentation to initialize your project and settings
    2. Configure and tune your prompts for the specific use case
    3. Run the indexing process to generate the required data files
    4. Ensure you have the settings.yaml file from the setup process

    Please refer to the [GraphRAG documentation](<https://microsoft.github.io/graphrag/>)
    for detailed instructions on completing these prerequisite steps.

    Example usage with AssistantAgent:

    ```
    import asyncio
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_agentchat.ui import Console
    from autogen_ext.tools.graphrag import LocalSearchTool
    from autogen_agentchat.agents import AssistantAgent

    async def main():
        # Initialize the OpenAI client
        openai_client = OpenAIChatCompletionClient(
            model="gpt-4o-mini",
            api_key="<api-key>",
        )

        # Set up local search tool
        local_tool = LocalSearchTool.from_settings(settings_path="./settings.yaml")

        # Create assistant agent with the local search tool
        assistant_agent = AssistantAgent(
            name="search_assistant",
            tools=[local_tool],
            model_client=openai_client,
            system_message=(
                "You are a tool selector AI assistant using the GraphRAG framework. "
                "Your primary task is to determine the appropriate search tool to call based on the user's query. "
                "For specific, detailed information about particular entities or relationships, call the 'local_search' function."
            ),
        )

        # Run a sample query
        query = "What does the station-master say about Dr. Becher?"
        await Console(assistant_agent.run_stream(task=query))

    if __name__ == "__main__":
        asyncio.run(main())

    ```

    Parameters:
    :   * **token\_encoder** (*tiktoken.Encoding*) – The tokenizer used for text encoding
        * **llm** (*BaseLLM*) – The language model to use for search
        * **embedder** (*BaseTextEmbedding*) – The text embedding model to use
        * **data\_config** (*DataConfig*) – Configuration for data source locations and settings
        * **context\_config** ([*LocalContextConfig*](#autogen_ext.tools.graphrag.LocalContextConfig "autogen_ext.tools.graphrag.LocalContextConfig")*,* *optional*) – Configuration for context building. Defaults to default config.
        * **search\_config** ([*SearchConfig*](#autogen_ext.tools.graphrag.SearchConfig "autogen_ext.tools.graphrag.SearchConfig")*,* *optional*) – Configuration for search operations. Defaults to default config.

    *classmethod* from\_settings(*settings\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)")*) → [LocalSearchTool](#autogen_ext.tools.graphrag.LocalSearchTool "autogen_ext.tools.graphrag._local_search.LocalSearchTool")[#](#autogen_ext.tools.graphrag.LocalSearchTool.from_settings "Link to this definition")
    :   Create a LocalSearchTool instance from GraphRAG settings file.

        Parameters:
        :   **settings\_path** – Path to the GraphRAG settings.yaml file

        Returns:
        :   **An initialized LocalSearchTool instance**

    *async* run(*args: [LocalSearchToolArgs](#autogen_ext.tools.graphrag.LocalSearchToolArgs "autogen_ext.tools.graphrag._local_search.LocalSearchToolArgs")*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [LocalSearchToolReturn](#autogen_ext.tools.graphrag.LocalSearchToolReturn "autogen_ext.tools.graphrag._local_search.LocalSearchToolReturn")[#](#autogen_ext.tools.graphrag.LocalSearchTool.run "Link to this definition")

*pydantic model* LocalSearchToolArgs[#](#autogen_ext.tools.graphrag.LocalSearchToolArgs "Link to this definition")
:   Bases: `BaseModel`

    Show JSON schema

    ```
    {
       "title": "LocalSearchToolArgs",
       "type": "object",
       "properties": {
          "query": {
             "description": "The user query to perform local search on.",
             "title": "Query",
             "type": "string"
          }
       },
       "required": [
          "query"
       ]
    }

    ```

    Fields:
    :   * `query (str)`

    *field* query*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.tools.graphrag.LocalSearchToolArgs.query "Link to this definition")
    :   The user query to perform local search on.

*pydantic model* LocalSearchToolReturn[#](#autogen_ext.tools.graphrag.LocalSearchToolReturn "Link to this definition")
:   Bases: `BaseModel`

    Show JSON schema

    ```
    {
       "title": "LocalSearchToolReturn",
       "type": "object",
       "properties": {
          "answer": {
             "description": "The answer to the user query.",
             "title": "Answer",
             "type": "string"
          }
       },
       "required": [
          "answer"
       ]
    }

    ```

    Fields:
    :   * `answer (str)`

    *field* answer*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.tools.graphrag.LocalSearchToolReturn.answer "Link to this definition")
    :   The answer to the user query.

*pydantic model* MapReduceConfig[#](#autogen_ext.tools.graphrag.MapReduceConfig "Link to this definition")
:   Bases: `BaseModel`

    Show JSON schema

    ```
    {
       "title": "MapReduceConfig",
       "type": "object",
       "properties": {
          "map_max_tokens": {
             "default": 1000,
             "title": "Map Max Tokens",
             "type": "integer"
          },
          "map_temperature": {
             "default": 0.0,
             "title": "Map Temperature",
             "type": "number"
          },
          "reduce_max_tokens": {
             "default": 2000,
             "title": "Reduce Max Tokens",
             "type": "integer"
          },
          "reduce_temperature": {
             "default": 0.0,
             "title": "Reduce Temperature",
             "type": "number"
          },
          "allow_general_knowledge": {
             "default": false,
             "title": "Allow General Knowledge",
             "type": "boolean"
          },
          "json_mode": {
             "default": false,
             "title": "Json Mode",
             "type": "boolean"
          },
          "response_type": {
             "default": "multiple paragraphs",
             "title": "Response Type",
             "type": "string"
          }
       }
    }

    ```

    Fields:
    :   * `allow_general_knowledge (bool)`
        * `json_mode (bool)`
        * `map_max_tokens (int)`
        * `map_temperature (float)`
        * `reduce_max_tokens (int)`
        * `reduce_temperature (float)`
        * `response_type (str)`

    *field* allow\_general\_knowledge*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")* *= False*[#](#autogen_ext.tools.graphrag.MapReduceConfig.allow_general_knowledge "Link to this definition")

    *field* json\_mode*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")* *= False*[#](#autogen_ext.tools.graphrag.MapReduceConfig.json_mode "Link to this definition")

    *field* map\_max\_tokens*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")* *= 1000*[#](#autogen_ext.tools.graphrag.MapReduceConfig.map_max_tokens "Link to this definition")

    *field* map\_temperature*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")* *= 0.0*[#](#autogen_ext.tools.graphrag.MapReduceConfig.map_temperature "Link to this definition")

    *field* reduce\_max\_tokens*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")* *= 2000*[#](#autogen_ext.tools.graphrag.MapReduceConfig.reduce_max_tokens "Link to this definition")

    *field* reduce\_temperature*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")* *= 0.0*[#](#autogen_ext.tools.graphrag.MapReduceConfig.reduce_temperature "Link to this definition")

    *field* response\_type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'multiple paragraphs'*[#](#autogen_ext.tools.graphrag.MapReduceConfig.response_type "Link to this definition")

*pydantic model* SearchConfig[#](#autogen_ext.tools.graphrag.SearchConfig "Link to this definition")
:   Bases: `BaseModel`

    Show JSON schema

    ```
    {
       "title": "SearchConfig",
       "type": "object",
       "properties": {
          "max_tokens": {
             "default": 1500,
             "title": "Max Tokens",
             "type": "integer"
          },
          "temperature": {
             "default": 0.0,
             "title": "Temperature",
             "type": "number"
          },
          "response_type": {
             "default": "multiple paragraphs",
             "title": "Response Type",
             "type": "string"
          }
       }
    }

    ```

    Fields:
    :   * `max_tokens (int)`
        * `response_type (str)`
        * `temperature (float)`

    *field* max\_tokens*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")* *= 1500*[#](#autogen_ext.tools.graphrag.SearchConfig.max_tokens "Link to this definition")

    *field* response\_type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'multiple paragraphs'*[#](#autogen_ext.tools.graphrag.SearchConfig.response_type "Link to this definition")

    *field* temperature*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")* *= 0.0*[#](#autogen_ext.tools.graphrag.SearchConfig.temperature "Link to this definition")

### autogen\_ext.tools.http[#](#module-autogen_ext.tools.http "Link to this heading")

*class* HttpTool(*name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *host: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *port: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*, *json\_schema: [dict](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*, *headers: [dict](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'HTTP tool'*, *path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = '/'*, *scheme: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['http', 'https'] = 'http'*, *method: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['GET', 'POST', 'PUT', 'DELETE', 'PATCH'] = 'POST'*, *return\_type: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['text', 'json'] = 'text'*)[#](#autogen_ext.tools.http.HttpTool "Link to this definition")
:   Bases: [`BaseTool`](#autogen_core.tools.BaseTool "autogen_core.tools._base.BaseTool")[`BaseModel`, [`Any`](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")], [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`HttpToolConfig`]

    A wrapper for using an HTTP server as a tool.

    Parameters:
    :   * **name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The name of the tool.
        * **description** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – A description of the tool.
        * **scheme** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The scheme to use for the request. Must be either “http” or “https”.
        * **host** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The host to send the request to.
        * **port** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The port to send the request to.
        * **path** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The path to send the request to. Defaults to “/”.
          Can include path parameters like “/{param1}/{param2}” which will be templated from input args.
        * **method** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The HTTP method to use, will default to POST if not provided.
          Must be one of “GET”, “POST”, “PUT”, “DELETE”, “PATCH”.
        * **headers** ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")*[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]**,* *optional*) – A dictionary of headers to send with the request.
        * **json\_schema** ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")*[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]*) – A JSON Schema object defining the expected parameters for the tool.
          Path parameters must also be included in the schema and must be strings.
        * **return\_type** (*Literal**[**"text"**,* *"json"**]**,* *optional*) – The type of response to return from the tool.
          Defaults to “text”.

    Note

    This tool requires the `http-tool` extra for the `autogen-ext` package.

    To install:

    ```
    pip install -U "autogen-agentchat" "autogen-ext[http-tool]"

    ```

    Example

    Simple use case:

    ```
    import asyncio

    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.messages import TextMessage
    from autogen_core import CancellationToken
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_ext.tools.http import HttpTool

    # Define a JSON schema for a base64 decode tool
    base64_schema = {
        "type": "object",
        "properties": {
            "value": {"type": "string", "description": "The base64 value to decode"},
        },
        "required": ["value"],
    }

    # Create an HTTP tool for the httpbin API
    base64_tool = HttpTool(
        name="base64_decode",
        description="base64 decode a value",
        scheme="https",
        host="httpbin.org",
        port=443,
        path="/base64/{value}",
        method="GET",
        json_schema=base64_schema,
    )

    async def main():
        # Create an assistant with the base64 tool
        model = OpenAIChatCompletionClient(model="gpt-4")
        assistant = AssistantAgent("base64_assistant", model_client=model, tools=[base64_tool])

        # The assistant can now use the base64 tool to decode the string
        response = await assistant.on_messages(
            [TextMessage(content="Can you base64 decode the value 'YWJjZGU=', please?", source="user")],
            CancellationToken(),
        )
        print(response.chat_message.content)

    asyncio.run(main())

    ```

    *classmethod* \_from\_config(*config: HttpToolConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_ext.tools.http.HttpTool._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → HttpToolConfig[#](#autogen_ext.tools.http.HttpTool._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_ext.tools.http.HttpTool.component_config_schema "Link to this definition")
    :   alias of `HttpToolConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.tools.http.HttpTool'*[#](#autogen_ext.tools.http.HttpTool.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    component\_type*: ClassVar[ComponentType]* *= 'tool'*[#](#autogen_ext.tools.http.HttpTool.component_type "Link to this definition")
    :   The logical type of the component.

    *async* run(*args: BaseModel*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_ext.tools.http.HttpTool.run "Link to this definition")
    :   Execute the HTTP tool with the given arguments.

        Parameters:
        :   * **args** – The validated input arguments
            * **cancellation\_token** – Token for cancelling the operation

        Returns:
        :   **The response body from the HTTP call in JSON format**

        Raises:
        :   [**Exception**](https://docs.python.org/3/library/exceptions.html#Exception "(in Python v3.13)") – If tool execution fails

### autogen\_ext.tools.langchain[#](#module-autogen_ext.tools.langchain "Link to this heading")

*class* LangChainToolAdapter(*langchain\_tool: LangChainTool*)[#](#autogen_ext.tools.langchain.LangChainToolAdapter "Link to this definition")
:   Bases: [`BaseTool`](#autogen_core.tools.BaseTool "autogen_core.tools._base.BaseTool")[`BaseModel`, [`Any`](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]

    Allows you to wrap a LangChain tool and make it available to AutoGen.

    Note

    This class requires the `langchain` extra for the `autogen-ext` package.

    ```
    pip install -U "autogen-ext[langchain]"

    ```

    Parameters:
    :   **langchain\_tool** (*LangChainTool*) – A LangChain tool to wrap

    Examples

    Use the PythonAstREPLTool from the langchain\_experimental package to
    create a tool that allows you to interact with a Pandas DataFrame.

    ```
    import asyncio
    import pandas as pd
    from langchain_experimental.tools.python.tool import PythonAstREPLTool
    from autogen_ext.tools.langchain import LangChainToolAdapter
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_agentchat.messages import TextMessage
    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.ui import Console
    from autogen_core import CancellationToken

    async def main() -> None:
        df = pd.read_csv("https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv")  # type: ignore
        tool = LangChainToolAdapter(PythonAstREPLTool(locals={"df": df}))
        model_client = OpenAIChatCompletionClient(model="gpt-4o")
        agent = AssistantAgent(
            "assistant",
            tools=[tool],
            model_client=model_client,
            system_message="Use the `df` variable to access the dataset.",
        )
        await Console(
            agent.on_messages_stream(
                [TextMessage(content="What's the average age of the passengers?", source="user")], CancellationToken()
            )
        )

    asyncio.run(main())

    ```

    This example demonstrates how to use the SQLDatabaseToolkit from the langchain\_community
    package to interact with an SQLite database.
    It uses the `RoundRobinGroupChat` to iterate the single agent over multiple steps.
    If you want to one step at a time, you can just call run\_stream method of the
    [`AssistantAgent`](#autogen_agentchat.agents.AssistantAgent "autogen_agentchat.agents.AssistantAgent") class directly.

    ```
    import asyncio
    import sqlite3

    import requests
    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.conditions import TextMentionTermination
    from autogen_agentchat.teams import RoundRobinGroupChat
    from autogen_agentchat.ui import Console
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_ext.tools.langchain import LangChainToolAdapter
    from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
    from langchain_community.utilities.sql_database import SQLDatabase
    from langchain_openai import ChatOpenAI
    from sqlalchemy import Engine, create_engine
    from sqlalchemy.pool import StaticPool

    def get_engine_for_chinook_db() -> Engine:
        url = "https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql"
        response = requests.get(url)
        sql_script = response.text
        connection = sqlite3.connect(":memory:", check_same_thread=False)
        connection.executescript(sql_script)
        return create_engine(
            "sqlite://",
            creator=lambda: connection,
            poolclass=StaticPool,
            connect_args={"check_same_thread": False},
        )

    async def main() -> None:
        # Create the engine and database wrapper.
        engine = get_engine_for_chinook_db()
        db = SQLDatabase(engine)

        # Create the toolkit.
        llm = ChatOpenAI(temperature=0)
        toolkit = SQLDatabaseToolkit(db=db, llm=llm)

        # Create the LangChain tool adapter for every tool in the toolkit.
        tools = [LangChainToolAdapter(tool) for tool in toolkit.get_tools()]

        # Create the chat completion client.
        model_client = OpenAIChatCompletionClient(model="gpt-4o")

        # Create the assistant agent.
        agent = AssistantAgent(
            "assistant",
            model_client=model_client,
            tools=tools,  # type: ignore
            model_client_stream=True,
            system_message="Respond with 'TERMINATE' if the task is completed.",
        )

        # Create termination condition.
        termination = TextMentionTermination("TERMINATE")

        # Create a round-robin group chat to iterate the single agent over multiple steps.
        chat = RoundRobinGroupChat([agent], termination_condition=termination)

        # Run the chat.
        await Console(chat.run_stream(task="Show some tables in the database"))

    if __name__ == "__main__":
        asyncio.run(main())

    ```

    *async* run(*args: BaseModel*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_ext.tools.langchain.LangChainToolAdapter.run "Link to this definition")

### autogen\_ext.tools.mcp[#](#module-autogen_ext.tools.mcp "Link to this heading")

*class* SseMcpToolAdapter(*server\_params: [SseServerParams](#autogen_ext.tools.mcp.SseServerParams "autogen_ext.tools.mcp._config.SseServerParams")*, *tool: Tool*)[#](#autogen_ext.tools.mcp.SseMcpToolAdapter "Link to this definition")
:   Bases: `McpToolAdapter`[[`SseServerParams`](#autogen_ext.tools.mcp.SseServerParams "autogen_ext.tools.mcp._config.SseServerParams")], [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`SseMcpToolAdapterConfig`]

    Allows you to wrap an MCP tool running over Server-Sent Events (SSE) and make it available to AutoGen.

    This adapter enables using MCP-compatible tools that communicate over HTTP with SSE
    with AutoGen agents. Common use cases include integrating with remote MCP services,
    cloud-based tools, and web APIs that implement the Model Context Protocol (MCP).

    Note

    To use this class, you need to install mcp extra for the autogen-ext package.

    ```
    pip install -U "autogen-ext[mcp]"

    ```

    Parameters:
    :   * **server\_params** (*SseServerParameters*) – Parameters for the MCP server connection,
          including URL, headers, and timeouts
        * **tool** ([*Tool*](#autogen_core.tools.Tool "autogen_core.tools.Tool")) – The MCP tool to wrap

    Examples

    Use a remote translation service that implements MCP over SSE to create tools
    that allow AutoGen agents to perform translations:

    ```
    import asyncio
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_ext.tools.mcp import SseMcpToolAdapter, SseServerParams
    from autogen_agentchat.agents import AssistantAgent
    from autogen_agentchat.ui import Console
    from autogen_core import CancellationToken

    async def main() -> None:
        # Create server params for the remote MCP service
        server_params = SseServerParams(
            url="https://api.example.com/mcp",
            headers={"Authorization": "Bearer your-api-key", "Content-Type": "application/json"},
            timeout=30,  # Connection timeout in seconds
        )

        # Get the translation tool from the server
        adapter = await SseMcpToolAdapter.from_server_params(server_params, "translate")

        # Create an agent that can use the translation tool
        model_client = OpenAIChatCompletionClient(model="gpt-4")
        agent = AssistantAgent(
            name="translator",
            model_client=model_client,
            tools=[adapter],
            system_message="You are a helpful translation assistant.",
        )

        # Let the agent translate some text
        await Console(
            agent.run_stream(task="Translate 'Hello, how are you?' to Spanish", cancellation_token=CancellationToken())
        )

    if __name__ == "__main__":
        asyncio.run(main())

    ```

    component\_config\_schema[#](#autogen_ext.tools.mcp.SseMcpToolAdapter.component_config_schema "Link to this definition")
    :   alias of `SseMcpToolAdapterConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.tools.mcp.SseMcpToolAdapter'*[#](#autogen_ext.tools.mcp.SseMcpToolAdapter.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

*pydantic model* SseServerParams[#](#autogen_ext.tools.mcp.SseServerParams "Link to this definition")
:   Bases: `BaseModel`

    Parameters for connecting to an MCP server over SSE.

    Show JSON schema

    ```
    {
       "title": "SseServerParams",
       "description": "Parameters for connecting to an MCP server over SSE.",
       "type": "object",
       "properties": {
          "url": {
             "title": "Url",
             "type": "string"
          },
          "headers": {
             "anyOf": [
                {
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Headers"
          },
          "timeout": {
             "default": 5,
             "title": "Timeout",
             "type": "number"
          },
          "sse_read_timeout": {
             "default": 300,
             "title": "Sse Read Timeout",
             "type": "number"
          }
       },
       "required": [
          "url"
       ]
    }

    ```

    Fields:
    :   * `headers (dict[str, Any] | None)`
        * `sse_read_timeout (float)`
        * `timeout (float)`
        * `url (str)`

    *field* headers*: [dict](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.tools.mcp.SseServerParams.headers "Link to this definition")

    *field* sse\_read\_timeout*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")* *= 300*[#](#autogen_ext.tools.mcp.SseServerParams.sse_read_timeout "Link to this definition")

    *field* timeout*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")* *= 5*[#](#autogen_ext.tools.mcp.SseServerParams.timeout "Link to this definition")

    *field* url*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.tools.mcp.SseServerParams.url "Link to this definition")

*class* StdioMcpToolAdapter(*server\_params: [StdioServerParams](#autogen_ext.tools.mcp.StdioServerParams "autogen_ext.tools.mcp._config.StdioServerParams")*, *tool: Tool*)[#](#autogen_ext.tools.mcp.StdioMcpToolAdapter "Link to this definition")
:   Bases: `McpToolAdapter`[[`StdioServerParams`](#autogen_ext.tools.mcp.StdioServerParams "autogen_ext.tools.mcp._config.StdioServerParams")], [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`StdioMcpToolAdapterConfig`]

    Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.

    This adapter enables using MCP-compatible tools that communicate over standard input/output
    with AutoGen agents. Common use cases include wrapping command-line tools and local services
    that implement the Model Context Protocol (MCP).

    Note

    To use this class, you need to install mcp extra for the autogen-ext package.

    ```
    pip install -U "autogen-ext[mcp]"

    ```

    Parameters:
    :   * **server\_params** ([*StdioServerParams*](#autogen_ext.tools.mcp.StdioServerParams "autogen_ext.tools.mcp.StdioServerParams")) – Parameters for the MCP server connection,
          including command to run and its arguments
        * **tool** ([*Tool*](#autogen_core.tools.Tool "autogen_core.tools.Tool")) – The MCP tool to wrap

    See [`mcp_server_tools()`](#autogen_ext.tools.mcp.mcp_server_tools "autogen_ext.tools.mcp.mcp_server_tools") for examples.

    component\_config\_schema[#](#autogen_ext.tools.mcp.StdioMcpToolAdapter.component_config_schema "Link to this definition")
    :   alias of `StdioMcpToolAdapterConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.tools.mcp.StdioMcpToolAdapter'*[#](#autogen_ext.tools.mcp.StdioMcpToolAdapter.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

*pydantic model* StdioServerParams[#](#autogen_ext.tools.mcp.StdioServerParams "Link to this definition")
:   Bases: `StdioServerParameters`

    Parameters for connecting to an MCP server over STDIO.

    Show JSON schema

    ```
    {
       "title": "StdioServerParams",
       "description": "Parameters for connecting to an MCP server over STDIO.",
       "type": "object",
       "properties": {
          "command": {
             "title": "Command",
             "type": "string"
          },
          "args": {
             "items": {
                "type": "string"
             },
             "title": "Args",
             "type": "array"
          },
          "env": {
             "anyOf": [
                {
                   "additionalProperties": {
                      "type": "string"
                   },
                   "type": "object"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Env"
          },
          "encoding": {
             "default": "utf-8",
             "title": "Encoding",
             "type": "string"
          },
          "encoding_error_handler": {
             "default": "strict",
             "enum": [
                "strict",
                "ignore",
                "replace"
             ],
             "title": "Encoding Error Handler",
             "type": "string"
          }
       },
       "required": [
          "command"
       ]
    }

    ```

    Fields:
    :   * `args (list[str])`
        * `command (str)`
        * `encoding (str)`
        * `encoding_error_handler (Literal['strict', 'ignore', 'replace'])`
        * `env (dict[str, str] | None)`

    *field* args*: [list](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]* *[Optional]*[#](#autogen_ext.tools.mcp.StdioServerParams.args "Link to this definition")
    :   Command line arguments to pass to the executable.

    *field* command*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.tools.mcp.StdioServerParams.command "Link to this definition")
    :   The executable to run to start the server.

    *field* encoding*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'utf-8'*[#](#autogen_ext.tools.mcp.StdioServerParams.encoding "Link to this definition")
    :   The text encoding used when sending/receiving messages to the server

        defaults to utf-8

    *field* encoding\_error\_handler*: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['strict', 'ignore', 'replace']* *= 'strict'*[#](#autogen_ext.tools.mcp.StdioServerParams.encoding_error_handler "Link to this definition")
    :   The text encoding error handler.

        See <https://docs.python.org/3/library/codecs.html#codec-base-classes> for
        explanations of possible values

    *field* env*: [dict](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.tools.mcp.StdioServerParams.env "Link to this definition")
    :   The environment to use when spawning the process.

        If not specified, the result of get\_default\_environment() will be used.

*async* mcp\_server\_tools(*server\_params: [StdioServerParams](#autogen_ext.tools.mcp.StdioServerParams "autogen_ext.tools.mcp._config.StdioServerParams") | [SseServerParams](#autogen_ext.tools.mcp.SseServerParams "autogen_ext.tools.mcp._config.SseServerParams")*) → [list](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.13)")[[StdioMcpToolAdapter](#autogen_ext.tools.mcp.StdioMcpToolAdapter "autogen_ext.tools.mcp._stdio.StdioMcpToolAdapter") | [SseMcpToolAdapter](#autogen_ext.tools.mcp.SseMcpToolAdapter "autogen_ext.tools.mcp._sse.SseMcpToolAdapter")][#](#autogen_ext.tools.mcp.mcp_server_tools "Link to this definition")
:   Creates a list of MCP tool adapters that can be used with AutoGen agents.

    This factory function connects to an MCP server and returns adapters for all available tools.
    The adapters can be directly assigned to an AutoGen agent’s tools list.

    Note

    To use this function, you need to install mcp extra for the autogen-ext package.

    ```
    pip install -U "autogen-ext[mcp]"

    ```

    Parameters:
    :   **server\_params** (*McpServerParams*) – Connection parameters for the MCP server.
        Can be either StdioServerParams for command-line tools or
        SseServerParams for HTTP/SSE services.

    Returns:
    :   **list[StdioMcpToolAdapter | SseMcpToolAdapter]** – A list of tool adapters ready to use
        with AutoGen agents.

    Examples

    **Local file system MCP service over standard I/O example:**

    Install the filesystem server package from npm (requires Node.js 16+ and npm).

    ```
    npm install -g @modelcontextprotocol/server-filesystem

    ```

    Create an agent that can use all tools from the local filesystem MCP server.

    ```
    import asyncio
    from pathlib import Path
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools
    from autogen_agentchat.agents import AssistantAgent
    from autogen_core import CancellationToken

    async def main() -> None:
        # Setup server params for local filesystem access
        desktop = str(Path.home() / "Desktop")
        server_params = StdioServerParams(
            command="npx.cmd", args=["-y", "@modelcontextprotocol/server-filesystem", desktop]
        )

        # Get all available tools from the server
        tools = await mcp_server_tools(server_params)

        # Create an agent that can use all the tools
        agent = AssistantAgent(
            name="file_manager",
            model_client=OpenAIChatCompletionClient(model="gpt-4"),
            tools=tools,  # type: ignore
        )

        # The agent can now use any of the filesystem tools
        await agent.run(task="Create a file called test.txt with some content", cancellation_token=CancellationToken())

    if __name__ == "__main__":
        asyncio.run(main())

    ```

    **Local fetch MCP service over standard I/O example:**

    Install the mcp-server-fetch package.

    ```
    pip install mcp-server-fetch

    ```

    Create an agent that can use the fetch tool from the local MCP server.

    ```
    import asyncio

    from autogen_agentchat.agents import AssistantAgent
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools

    async def main() -> None:
        # Get the fetch tool from mcp-server-fetch.
        fetch_mcp_server = StdioServerParams(command="uvx", args=["mcp-server-fetch"])
        tools = await mcp_server_tools(fetch_mcp_server)

        # Create an agent that can use the fetch tool.
        model_client = OpenAIChatCompletionClient(model="gpt-4o")
        agent = AssistantAgent(name="fetcher", model_client=model_client, tools=tools, reflect_on_tool_use=True)  # type: ignore

        # Let the agent fetch the content of a URL and summarize it.
        result = await agent.run(task="Summarize the content of https://en.wikipedia.org/wiki/Seattle")
        print(result.messages[-1].content)

    asyncio.run(main())

    ```

    **Remote MCP service over SSE example:**

    ```
    from autogen_ext.tools.mcp import SseServerParams, mcp_server_tools

    async def main() -> None:
        # Setup server params for remote service
        server_params = SseServerParams(url="https://api.example.com/mcp", headers={"Authorization": "Bearer token"})

        # Get all available tools
        tools = await mcp_server_tools(server_params)

        # Create an agent with all tools
        agent = AssistantAgent(name="tool_user", model_client=OpenAIChatCompletionClient(model="gpt-4"), tools=tools)  # type: ignore

    ```

    For more examples and detailed usage, see the samples directory in the package repository.

### autogen\_ext.tools.semantic\_kernel[#](#module-autogen_ext.tools.semantic_kernel "Link to this heading")

*pydantic model* KernelFunctionFromTool[#](#autogen_ext.tools.semantic_kernel.KernelFunctionFromTool "Link to this definition")
:   Bases: `KernelFunctionFromMethod`

    Show JSON schema

    ```
    {
       "title": "KernelFunctionFromTool",
       "type": "object",
       "properties": {
          "metadata": {
             "$ref": "#/$defs/KernelFunctionMetadata"
          },
          "invocation_duration_histogram": {
             "default": null,
             "title": "Invocation Duration Histogram"
          },
          "streaming_duration_histogram": {
             "default": null,
             "title": "Streaming Duration Histogram"
          },
          "method": {
             "default": null,
             "title": "Method"
          },
          "stream_method": {
             "default": null,
             "title": "Stream Method"
          }
       },
       "$defs": {
          "KernelFunctionMetadata": {
             "description": "The kernel function metadata.",
             "properties": {
                "name": {
                   "pattern": "^[0-9A-Za-z_]+$",
                   "title": "Name",
                   "type": "string"
                },
                "plugin_name": {
                   "anyOf": [
                      {
                         "pattern": "^[0-9A-Za-z_]+$",
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Plugin Name"
                },
                "description": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Description"
                },
                "parameters": {
                   "items": {
                      "$ref": "#/$defs/KernelParameterMetadata"
                   },
                   "title": "Parameters",
                   "type": "array"
                },
                "is_prompt": {
                   "title": "Is Prompt",
                   "type": "boolean"
                },
                "is_asynchronous": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": true,
                   "title": "Is Asynchronous"
                },
                "return_parameter": {
                   "anyOf": [
                      {
                         "$ref": "#/$defs/KernelParameterMetadata"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null
                },
                "additional_properties": {
                   "anyOf": [
                      {
                         "type": "object"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Additional Properties"
                }
             },
             "required": [
                "name",
                "is_prompt"
             ],
             "title": "KernelFunctionMetadata",
             "type": "object"
          },
          "KernelParameterMetadata": {
             "description": "The kernel parameter metadata.",
             "properties": {
                "name": {
                   "anyOf": [
                      {
                         "pattern": "^[0-9A-Za-z_]+$",
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "title": "Name"
                },
                "description": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Description"
                },
                "default_value": {
                   "anyOf": [
                      {},
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Default Value"
                },
                "type": {
                   "anyOf": [
                      {
                         "type": "string"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": "str",
                   "title": "Type"
                },
                "is_required": {
                   "anyOf": [
                      {
                         "type": "boolean"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": false,
                   "title": "Is Required"
                },
                "type_object": {
                   "anyOf": [
                      {},
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Type Object"
                },
                "schema_data": {
                   "anyOf": [
                      {
                         "type": "object"
                      },
                      {
                         "type": "null"
                      }
                   ],
                   "default": null,
                   "title": "Schema Data"
                },
                "include_in_function_choices": {
                   "default": true,
                   "title": "Include In Function Choices",
                   "type": "boolean"
                }
             },
             "required": [
                "name"
             ],
             "title": "KernelParameterMetadata",
             "type": "object"
          }
       },
       "required": [
          "metadata"
       ]
    }

    ```

    Fields:
    :   * `method (collections.abc.Callable[[...], Any])`
        * `stream_method (collections.abc.Callable[[...], Any] | None)`

    *field* method*: [Callable](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable "(in Python v3.13)")[[...], [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]* *[Required]*[#](#autogen_ext.tools.semantic_kernel.KernelFunctionFromTool.method "Link to this definition")

    *field* stream\_method*: [Callable](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable "(in Python v3.13)")[[...], [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.tools.semantic_kernel.KernelFunctionFromTool.stream_method "Link to this definition")

### autogen\_ext.code\_executors.local[#](#module-autogen_ext.code_executors.local "Link to this heading")

*class* LocalCommandLineCodeExecutor(*timeout: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 60*, *work\_dir: [Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)") | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = Path('.')*, *functions: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[FunctionWithRequirements](#autogen_core.code_executor.FunctionWithRequirements "autogen_core.code_executor._func_with_reqs.FunctionWithRequirements")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), A] | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[...], [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [FunctionWithRequirementsStr](#autogen_core.code_executor.FunctionWithRequirementsStr "autogen_core.code_executor._func_with_reqs.FunctionWithRequirementsStr")] = []*, *functions\_module: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'functions'*, *virtual\_env\_context: [SimpleNamespace](https://docs.python.org/3/library/types.html#types.SimpleNamespace "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor "Link to this definition")
:   Bases: [`CodeExecutor`](#autogen_core.code_executor.CodeExecutor "autogen_core.code_executor._base.CodeExecutor"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`LocalCommandLineCodeExecutorConfig`]

    A code executor class that executes code through a local command line
    environment.

    Danger

    This will execute code on the local machine. If being used with LLM generated code, caution should be used.

    Each code block is saved as a file and executed in a separate process in
    the working directory, and a unique file is generated and saved in the
    working directory for each code block.
    The code blocks are executed in the order they are received.
    Command line code is sanitized using regular expression match against a list of dangerous commands in order to prevent self-destructive
    commands from being executed which may potentially affect the users environment.
    Currently the only supported languages is Python and shell scripts.
    For Python code, use the language “python” for the code block.
    For shell scripts, use the language “bash”, “shell”, or “sh” for the code
    block.

    Note

    On Windows, the event loop policy must be set to WindowsProactorEventLoopPolicy to avoid issues with subprocesses.

    ```
    import sys
    import asyncio

    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

    ```

    Parameters:
    :   * **timeout** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The timeout for the execution of any single code block. Default is 60.
        * **work\_dir** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The working directory for the code execution. If None,
          a default working directory will be used. The default working
          directory is the current directory “.”.
        * **functions** (*List**[**Union**[*[*FunctionWithRequirements*](#autogen_core.code_executor.FunctionWithRequirements "autogen_core.code_executor.FunctionWithRequirements")*[**Any**,* *A**]**,* *Callable**[**...**,* *Any**]**]**]*) – A list of functions that are available to the code executor. Default is an empty list.
        * **functions\_module** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The name of the module that will be created to store the functions. Defaults to “functions”.
        * **virtual\_env\_context** (*Optional**[**SimpleNamespace**]**,* *optional*) – The virtual environment context. Defaults to None.

    Example:

    How to use LocalCommandLineCodeExecutor with a virtual environment different from the one used to run the autogen application:
    Set up a virtual environment using the venv module, and pass its context to the initializer of LocalCommandLineCodeExecutor. This way, the executor will run code within the new environment.

    > ```
    > import venv
    > from pathlib import Path
    > import asyncio
    >
    > from autogen_core import CancellationToken
    > from autogen_core.code_executor import CodeBlock
    > from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
    >
    >
    > async def example():
    >     work_dir = Path("coding")
    >     work_dir.mkdir(exist_ok=True)
    >
    >     venv_dir = work_dir / ".venv"
    >     venv_builder = venv.EnvBuilder(with_pip=True)
    >     venv_builder.create(venv_dir)
    >     venv_context = venv_builder.ensure_directories(venv_dir)
    >
    >     local_executor = LocalCommandLineCodeExecutor(work_dir=work_dir, virtual_env_context=venv_context)
    >     await local_executor.execute_code_blocks(
    >         code_blocks=[
    >             CodeBlock(language="bash", code="pip install matplotlib"),
    >         ],
    >         cancellation_token=CancellationToken(),
    >     )
    >
    >
    > asyncio.run(example())
    >
    > ```

    FUNCTION\_PROMPT\_TEMPLATE*: [ClassVar](https://docs.python.org/3/library/typing.html#typing.ClassVar "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]* *= 'You have access to the following user defined functions. They can be accessed from the module called `$module\_name` by their function names.\n\nFor example, if there was a function called `foo` you could import it by writing `from $module\_name import foo`\n\n$functions'*[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor.FUNCTION_PROMPT_TEMPLATE "Link to this definition")

    SUPPORTED\_LANGUAGES*: [ClassVar](https://docs.python.org/3/library/typing.html#typing.ClassVar "(in Python v3.13)")[[List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]]* *= ['bash', 'shell', 'sh', 'pwsh', 'powershell', 'ps1', 'python']*[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor.SUPPORTED_LANGUAGES "Link to this definition")

    *classmethod* \_from\_config(*config: LocalCommandLineCodeExecutorConfig*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → LocalCommandLineCodeExecutorConfig[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor.component_config_schema "Link to this definition")
    :   alias of `LocalCommandLineCodeExecutorConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.code\_executors.local.LocalCommandLineCodeExecutor'*[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* execute\_code\_blocks(*code\_blocks: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[CodeBlock](#autogen_core.code_executor.CodeBlock "autogen_core.code_executor._base.CodeBlock")]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → CommandLineCodeResult[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor.execute_code_blocks "Link to this definition")
    :   (Experimental) Execute the code blocks and return the result.

        Parameters:
        :   * **code\_blocks** (*List**[*[*CodeBlock*](#autogen_core.code_executor.CodeBlock "autogen_core.code_executor.CodeBlock")*]*) – The code blocks to execute.
            * **cancellation\_token** ([*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken")) – a token to cancel the operation

        Returns:
        :   **CommandLineCodeResult** – The result of the code execution.

    format\_functions\_for\_prompt(*prompt\_template: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = FUNCTION\_PROMPT\_TEMPLATE*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor.format_functions_for_prompt "Link to this definition")
    :   (Experimental) Format the functions for a prompt.

        The template includes two variables:
        - $module\_name: The module name.
        - $functions: The functions formatted as stubs with two newlines between each function.

        Parameters:
        :   **prompt\_template** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The prompt template. Default is the class default.

        Returns:
        :   **str** – The formatted prompt.

    *property* functions*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor.functions "Link to this definition")

    *property* functions\_module*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor.functions_module "Link to this definition")
    :   (Experimental) The module name for the functions.

    *async* restart() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor.restart "Link to this definition")
    :   (Experimental) Restart the code executor.

    *async* start() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor.start "Link to this definition")
    :   (Experimental) Start the code executor.

    *async* stop() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor.stop "Link to this definition")
    :   (Experimental) Stop the code executor.

    *property* timeout*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor.timeout "Link to this definition")
    :   (Experimental) The timeout for code execution.

    *property* work\_dir*: [Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)")*[#](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor.work_dir "Link to this definition")
    :   (Experimental) The working directory for the code execution.

### autogen\_ext.code\_executors.docker[#](#module-autogen_ext.code_executors.docker "Link to this heading")

*class* DockerCommandLineCodeExecutor(*image: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'python:3-slim'*, *container\_name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *\**, *timeout: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 60*, *work\_dir: [Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)") | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = Path('.')*, *bind\_dir: [Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)") | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *auto\_remove: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*, *stop\_container: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*, *functions: [Sequence](https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence "(in Python v3.13)")[[FunctionWithRequirements](#autogen_core.code_executor.FunctionWithRequirements "autogen_core.code_executor._func_with_reqs.FunctionWithRequirements")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), A] | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[...], [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [FunctionWithRequirementsStr](#autogen_core.code_executor.FunctionWithRequirementsStr "autogen_core.code_executor._func_with_reqs.FunctionWithRequirementsStr")] = []*, *functions\_module: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'functions'*, *extra\_volumes: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_hosts: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *init\_command: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor "Link to this definition")
:   Bases: [`CodeExecutor`](#autogen_core.code_executor.CodeExecutor "autogen_core.code_executor._base.CodeExecutor"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`DockerCommandLineCodeExecutorConfig`]

    Executes code through a command line environment in a Docker container.

    Note

    This class requires the `docker` extra for the `autogen-ext` package:

    ```
    pip install "autogen-ext[docker]"

    ```

    The executor first saves each code block in a file in the working
    directory, and then executes the code file in the container.
    The executor executes the code blocks in the order they are received.
    Currently, the executor only supports Python and shell scripts.
    For Python code, use the language “python” for the code block.
    For shell scripts, use the language “bash”, “shell”, or “sh” for the code
    block.

    Parameters:
    :   * **image** (*\_type\_**,* *optional*) – Docker image to use for code execution.
          Defaults to “python:3-slim”.
        * **container\_name** (*Optional**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]**,* *optional*) – Name of the Docker container
          which is created. If None, will autogenerate a name. Defaults to None.
        * **timeout** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*,* *optional*) – The timeout for code execution. Defaults to 60.
        * **work\_dir** (*Union**[**Path**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]**,* *optional*) – The working directory for the code
          execution. Defaults to Path(“.”).
        * **bind\_dir** (*Union**[**Path**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]**,* *optional*) – The directory that will be bound
        * **spawn** (*to the code executor container. Useful for cases where you want to*)
        * **work\_dir.** (*the container from within a container. Defaults to*)
        * **auto\_remove** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – If true, will automatically remove the Docker
          container when it is stopped. Defaults to True.
        * **stop\_container** ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*,* *optional*) – If true, will automatically stop the
          container when stop is called, when the context manager exits or when
          the Python process exits with atext. Defaults to True.
        * **functions** (*List**[**Union**[*[*FunctionWithRequirements*](#autogen_core.code_executor.FunctionWithRequirements "autogen_core.code_executor.FunctionWithRequirements")*[**Any**,* *A**]**,* *Callable**[**...**,* *Any**]**]**]*) – A list of functions that are available to the code executor. Default is an empty list.
        * **functions\_module** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *optional*) – The name of the module that will be created to store the functions. Defaults to “functions”.
        * **extra\_volumes** (*Optional**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]**]**]**,* *optional*) – A dictionary of extra volumes (beyond the work\_dir) to mount to the container;
          key is host source path and value ‘bind’ is the container path. See Defaults to None.
          Example: extra\_volumes = {‘/home/user1/’: {‘bind’: ‘/mnt/vol2’, ‘mode’: ‘rw’}, ‘/var/www’: {‘bind’: ‘/mnt/vol1’, ‘mode’: ‘ro’}}
        * **extra\_hosts** (*Optional**[**Dict**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]**]**,* *optional*) – A dictionary of host mappings to add to the container. (See Docker docs on extra\_hosts) Defaults to None.
          Example: extra\_hosts = {“kubernetes.docker.internal”: “host-gateway”}
        * **init\_command** (*Optional**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]**,* *optional*) – A shell command to run before each shell operation execution. Defaults to None.
          Example: init\_command=”kubectl config use-context docker-hub”

    FUNCTION\_PROMPT\_TEMPLATE*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]* *= 'You have access to the following user defined functions. They can be accessed from the module called `$module\_name` by their function names.\n\nFor example, if there was a function called `foo` you could import it by writing `from $module\_name import foo`\n\n$functions'*[#](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor.FUNCTION_PROMPT_TEMPLATE "Link to this definition")

    SUPPORTED\_LANGUAGES*: ClassVar[List[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]]* *= ['bash', 'shell', 'sh', 'pwsh', 'powershell', 'ps1', 'python']*[#](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor.SUPPORTED_LANGUAGES "Link to this definition")

    *property* bind\_dir*: [Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)")*[#](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor.bind_dir "Link to this definition")
    :   (Experimental) The binding directory for the code execution container.

    component\_config\_schema[#](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor.component_config_schema "Link to this definition")
    :   alias of `DockerCommandLineCodeExecutorConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.code\_executors.docker.DockerCommandLineCodeExecutor'*[#](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* execute\_code\_blocks(*code\_blocks: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[CodeBlock](#autogen_core.code_executor.CodeBlock "autogen_core.code_executor._base.CodeBlock")]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → CommandLineCodeResult[#](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor.execute_code_blocks "Link to this definition")
    :   (Experimental) Execute the code blocks and return the result.

        Parameters:
        :   **code\_blocks** (*List**[*[*CodeBlock*](#autogen_core.code_executor.CodeBlock "autogen_core.code_executor.CodeBlock")*]*) – The code blocks to execute.

        Returns:
        :   **CommandlineCodeResult** – The result of the code execution.

    *async* restart() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor.restart "Link to this definition")
    :   Restart the code executor.

        This method should be implemented by the code executor.

        This method is called when the agent is reset.

    *async* start() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor.start "Link to this definition")
    :   Start the code executor.

    *async* stop() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor.stop "Link to this definition")
    :   (Experimental) Stop the code executor.

    *property* timeout*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*[#](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor.timeout "Link to this definition")
    :   (Experimental) The timeout for code execution.

    *property* work\_dir*: [Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)")*[#](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor.work_dir "Link to this definition")
    :   (Experimental) The working directory for the code execution.

### autogen\_ext.code\_executors.jupyter[#](#module-autogen_ext.code_executors.jupyter "Link to this heading")

*class* JupyterCodeExecutor(*kernel\_name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'python3'*, *timeout: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 60*, *output\_dir: [Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)") = Path('.')*)[#](#autogen_ext.code_executors.jupyter.JupyterCodeExecutor "Link to this definition")
:   Bases: [`CodeExecutor`](#autogen_core.code_executor.CodeExecutor "autogen_core.code_executor._base.CodeExecutor"), [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[`JupyterCodeExecutorConfig`]

    A code executor class that executes code statefully using [nbclient]([jupyter/nbclient](https://github.com/jupyter/nbclient)).

    Danger

    This will execute code on the local machine. If being used with LLM generated code, caution should be used.

    Example of using it directly:

    ```
    import asyncio
    from autogen_core import CancellationToken
    from autogen_core.code_executor import CodeBlock
    from autogen_ext.code_executors.jupyter import JupyterCodeExecutor

    async def main() -> None:
        async with JupyterCodeExecutor() as executor:
            cancel_token = CancellationToken()
            code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
            code_result = await executor.execute_code_blocks(code_blocks, cancel_token)
            print(code_result)

    asyncio.run(main())

    ```

    Example of using it with [`PythonCodeExecutionTool`](#autogen_ext.tools.code_execution.PythonCodeExecutionTool "autogen_ext.tools.code_execution.PythonCodeExecutionTool"):

    ```
    import asyncio
    from autogen_agentchat.agents import AssistantAgent
    from autogen_ext.code_executors.jupyter import JupyterCodeExecutor
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_ext.tools.code_execution import PythonCodeExecutionTool

    async def main() -> None:
        async with JupyterCodeExecutor() as executor:
            tool = PythonCodeExecutionTool(executor)
            model_client = OpenAIChatCompletionClient(model="gpt-4o")
            agent = AssistantAgent("assistant", model_client=model_client, tools=[tool])
            result = await agent.run(task="What is the 10th Fibonacci number? Use Python to calculate it.")
            print(result)

    asyncio.run(main())

    ```

    Example of using it inside a [`CodeExecutorAgent`](#autogen_agentchat.agents.CodeExecutorAgent "autogen_agentchat.agents._code_executor_agent.CodeExecutorAgent"):

    ```
    import asyncio
    from autogen_agentchat.agents import CodeExecutorAgent
    from autogen_agentchat.messages import TextMessage
    from autogen_ext.code_executors.jupyter import JupyterCodeExecutor
    from autogen_core import CancellationToken

    async def main() -> None:
        async with JupyterCodeExecutor() as executor:
            code_executor_agent = CodeExecutorAgent("code_executor", code_executor=executor)
            task = TextMessage(
                content='''Here is some code
        ```python
        print('Hello world')
        ```
        ''',
                source="user",
            )
            response = await code_executor_agent.on_messages([task], CancellationToken())
            print(response.chat_message)

    asyncio.run(main())

    ```

    Parameters:
    :   * **kernel\_name** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The kernel name to use. By default, “python3”.
        * **timeout** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The timeout for code execution, by default 60.
        * **output\_dir** (*Path*) – The directory to save output files, by default “.”.

    component\_config\_schema[#](#autogen_ext.code_executors.jupyter.JupyterCodeExecutor.component_config_schema "Link to this definition")
    :   alias of `JupyterCodeExecutorConfig`

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.code\_executors.jupyter.JupyterCodeExecutor'*[#](#autogen_ext.code_executors.jupyter.JupyterCodeExecutor.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    *async* execute\_code\_blocks(*code\_blocks: [list](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.13)")[[CodeBlock](#autogen_core.code_executor.CodeBlock "autogen_core.code_executor._base.CodeBlock")]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [JupyterCodeResult](#autogen_ext.code_executors.jupyter.JupyterCodeResult "autogen_ext.code_executors.jupyter._jupyter_code_executor.JupyterCodeResult")[#](#autogen_ext.code_executors.jupyter.JupyterCodeExecutor.execute_code_blocks "Link to this definition")
    :   Execute code blocks and return the result.

        Parameters:
        :   **code\_blocks** ([*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.13)")*[*[*CodeBlock*](#autogen_core.code_executor.CodeBlock "autogen_core.code_executor.CodeBlock")*]*) – The code blocks to execute.

        Returns:
        :   **JupyterCodeResult** – The result of the code execution.

    *async* restart() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.code_executors.jupyter.JupyterCodeExecutor.restart "Link to this definition")
    :   Restart the code executor.

    *async* start() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.code_executors.jupyter.JupyterCodeExecutor.start "Link to this definition")
    :   Start the code executor.

    *async* stop() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.code_executors.jupyter.JupyterCodeExecutor.stop "Link to this definition")
    :   Stop the kernel.

*class* JupyterCodeResult(*exit\_code: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*, *output: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *output\_files: [list](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.13)")[[Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)")]*)[#](#autogen_ext.code_executors.jupyter.JupyterCodeResult "Link to this definition")
:   Bases: [`CodeResult`](#autogen_core.code_executor.CodeResult "autogen_core.code_executor._base.CodeResult")

    A code result class for Jupyter code executor.

    output\_files*: [list](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.13)")[[Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)")]*[#](#autogen_ext.code_executors.jupyter.JupyterCodeResult.output_files "Link to this definition")

### autogen\_ext.code\_executors.azure[#](#module-autogen_ext.code_executors.azure "Link to this heading")

*class* ACADynamicSessionsCodeExecutor(*pool\_management\_endpoint: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *credential: [TokenProvider](#autogen_ext.code_executors.azure.TokenProvider "autogen_ext.code_executors.azure._azure_container_code_executor.TokenProvider")*, *timeout: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 60*, *work\_dir: [Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)") | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = Path('.')*, *functions: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[FunctionWithRequirements](#autogen_core.code_executor.FunctionWithRequirements "autogen_core.code_executor._func_with_reqs.FunctionWithRequirements")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)"), A] | [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[...], [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [FunctionWithRequirementsStr](#autogen_core.code_executor.FunctionWithRequirementsStr "autogen_core.code_executor._func_with_reqs.FunctionWithRequirementsStr")] = []*, *functions\_module: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'functions'*)[#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor "Link to this definition")
:   Bases: [`CodeExecutor`](#autogen_core.code_executor.CodeExecutor "autogen_core.code_executor._base.CodeExecutor")

    (Experimental) A code executor class that executes code through a an Azure
    Container Apps Dynamic Sessions instance.

    Note

    This class requires the `azure` extra for the `autogen-ext` package:

    ```
    pip install "autogen-ext[azure]"

    ```

    Caution

    **This will execute LLM generated code on an Azure dynamic code container.**

    The execution environment is similar to that of a jupyter notebook which allows for incremental code execution. The parameter functions are executed in order once at the beginning of each session. Each code block is then executed serially and in the order they are received. Each environment has a statically defined set of available packages which cannot be changed.
    Currently, attempting to use packages beyond what is available on the environment will result in an error. To get the list of supported packages, call the get\_available\_packages function.
    Currently the only supported language is Python.
    For Python code, use the language “python” for the code block.

    Parameters:
    :   * **pool\_management\_endpoint** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The azure container apps dynamic sessions endpoint.
        * **credential** ([*TokenProvider*](#autogen_ext.code_executors.azure.TokenProvider "autogen_ext.code_executors.azure.TokenProvider")) – An object that implements the get\_token function.
        * **timeout** ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The timeout for the execution of any single code block. Default is 60.
        * **work\_dir** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The working directory for the code execution. If None,
          a default working directory will be used. The default working
          directory is the current directory “.”.
        * **functions** (*List**[**Union**[*[*FunctionWithRequirements*](#autogen_core.code_executor.FunctionWithRequirements "autogen_core.code_executor.FunctionWithRequirements")*[**Any**,* *A**]**,* *Callable**[**...**,* *Any**]**]**]*) – A list of functions that are available to the code executor. Default is an empty list.

    FUNCTION\_PROMPT\_TEMPLATE*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]* *= 'You have access to the following user defined functions.\n\n$functions'*[#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.FUNCTION_PROMPT_TEMPLATE "Link to this definition")

    SUPPORTED\_LANGUAGES*: ClassVar[List[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]]* *= ['python']*[#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.SUPPORTED_LANGUAGES "Link to this definition")

    *async* download\_files(*files: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)") | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")][#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.download_files "Link to this definition")

    *async* execute\_code\_blocks(*code\_blocks: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[CodeBlock](#autogen_core.code_executor.CodeBlock "autogen_core.code_executor._base.CodeBlock")]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [CodeResult](#autogen_core.code_executor.CodeResult "autogen_core.code_executor._base.CodeResult")[#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.execute_code_blocks "Link to this definition")
    :   (Experimental) Execute the code blocks and return the result.

        Parameters:
        :   * **code\_blocks** (*List**[*[*CodeBlock*](#autogen_core.code_executor.CodeBlock "autogen_core.code_executor.CodeBlock")*]*) – The code blocks to execute.
            * **cancellation\_token** ([*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken")) – a token to cancel the operation
            * **input\_files** (*Optional**[**Union**[**Path**,* [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*]**]*) – Any files the code blocks will need to access

        Returns:
        :   **CodeResult** – The result of the code execution.

    format\_functions\_for\_prompt(*prompt\_template: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = FUNCTION\_PROMPT\_TEMPLATE*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.format_functions_for_prompt "Link to this definition")
    :   (Experimental) Format the functions for a prompt.

        The template includes one variable:
        - $functions: The functions formatted as stubs with two newlines between each function.

        Parameters:
        :   **prompt\_template** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The prompt template. Default is the class default.

        Returns:
        :   **str** – The formatted prompt.

    *property* functions*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*[#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.functions "Link to this definition")

    *property* functions\_module*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.functions_module "Link to this definition")
    :   (Experimental) The module name for the functions.

    *async* get\_available\_packages(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [set](https://docs.python.org/3/library/stdtypes.html#set "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")][#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.get_available_packages "Link to this definition")

    *async* get\_file\_list(*cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")][#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.get_file_list "Link to this definition")

    *async* restart() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.restart "Link to this definition")
    :   (Experimental) Restart the code executor.

    *async* start() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.start "Link to this definition")
    :   (Experimental) Start the code executor.

    *async* stop() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.stop "Link to this definition")
    :   (Experimental) Stop the code executor.

    *property* timeout*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*[#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.timeout "Link to this definition")
    :   (Experimental) The timeout for code execution.

    *async* upload\_files(*files: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)") | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.upload_files "Link to this definition")

    *property* work\_dir*: [Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.13)")*[#](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor.work_dir "Link to this definition")
    :   (Experimental) The working directory for the code execution.

*class* TokenProvider(*\*args*, *\*\*kwargs*)[#](#autogen_ext.code_executors.azure.TokenProvider "Link to this definition")
:   Bases: [`Protocol`](https://docs.python.org/3/library/typing.html#typing.Protocol "(in Python v3.13)")

    get\_token(*\*scopes: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *claims: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *tenant\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *\*\*kwargs: Any*) → AccessToken[#](#autogen_ext.code_executors.azure.TokenProvider.get_token "Link to this definition")

### autogen\_ext.cache\_store.diskcache[#](#module-autogen_ext.cache_store.diskcache "Link to this heading")

*class* DiskCacheStore(*cache\_instance: Cache*)[#](#autogen_ext.cache_store.diskcache.DiskCacheStore "Link to this definition")
:   Bases: [`CacheStore`](#autogen_core.CacheStore "autogen_core._cache_store.CacheStore")[`T`], [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[[`DiskCacheStoreConfig`](#autogen_ext.cache_store.diskcache.DiskCacheStoreConfig "autogen_ext.cache_store.diskcache.DiskCacheStoreConfig")]

    A typed CacheStore implementation that uses diskcache as the underlying storage.
    See [`ChatCompletionCache`](#autogen_ext.models.cache.ChatCompletionCache "autogen_ext.models.cache.ChatCompletionCache") for an example of usage.

    Parameters:
    :   **cache\_instance** – An instance of diskcache.Cache.
        The user is responsible for managing the DiskCache instance’s lifetime.

    *classmethod* \_from\_config(*config: [DiskCacheStoreConfig](#autogen_ext.cache_store.diskcache.DiskCacheStoreConfig "autogen_ext.cache_store.diskcache.DiskCacheStoreConfig")*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_ext.cache_store.diskcache.DiskCacheStore._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → [DiskCacheStoreConfig](#autogen_ext.cache_store.diskcache.DiskCacheStoreConfig "autogen_ext.cache_store.diskcache.DiskCacheStoreConfig")[#](#autogen_ext.cache_store.diskcache.DiskCacheStore._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_ext.cache_store.diskcache.DiskCacheStore.component_config_schema "Link to this definition")
    :   alias of [`DiskCacheStoreConfig`](#autogen_ext.cache_store.diskcache.DiskCacheStoreConfig "autogen_ext.cache_store.diskcache.DiskCacheStoreConfig")

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.cache\_store.diskcache.DiskCacheStore'*[#](#autogen_ext.cache_store.diskcache.DiskCacheStore.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    get(*key: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *default: T | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → T | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.cache_store.diskcache.DiskCacheStore.get "Link to this definition")
    :   Retrieve an item from the store.

        Parameters:
        :   * **key** – The key identifying the item in the store.
            * **default** (*optional*) – The default value to return if the key is not found.
              Defaults to None.

        Returns:
        :   **The value associated with the key if found, else the default value.**

    set(*key: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *value: T*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.cache_store.diskcache.DiskCacheStore.set "Link to this definition")
    :   Set an item in the store.

        Parameters:
        :   * **key** – The key under which the item is to be stored.
            * **value** – The value to be stored in the store.

*pydantic model* DiskCacheStoreConfig[#](#autogen_ext.cache_store.diskcache.DiskCacheStoreConfig "Link to this definition")
:   Bases: `BaseModel`

    Configuration for DiskCacheStore

    Show JSON schema

    ```
    {
       "title": "DiskCacheStoreConfig",
       "description": "Configuration for DiskCacheStore",
       "type": "object",
       "properties": {
          "directory": {
             "title": "Directory",
             "type": "string"
          }
       },
       "required": [
          "directory"
       ]
    }

    ```

    Fields:
    :   * [`directory (str)`](#autogen_ext.cache_store.diskcache.DiskCacheStoreConfig.directory "autogen_ext.cache_store.diskcache.DiskCacheStoreConfig.directory")

    *field* directory*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.cache_store.diskcache.DiskCacheStoreConfig.directory "Link to this definition")

### autogen\_ext.cache\_store.redis[#](#module-autogen_ext.cache_store.redis "Link to this heading")

*class* RedisStore(*redis\_instance: Redis*)[#](#autogen_ext.cache_store.redis.RedisStore "Link to this definition")
:   Bases: [`CacheStore`](#autogen_core.CacheStore "autogen_core._cache_store.CacheStore")[`T`], [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[[`RedisStoreConfig`](#autogen_ext.cache_store.redis.RedisStoreConfig "autogen_ext.cache_store.redis.RedisStoreConfig")]

    A typed CacheStore implementation that uses redis as the underlying storage.
    See [`ChatCompletionCache`](#autogen_ext.models.cache.ChatCompletionCache "autogen_ext.models.cache.ChatCompletionCache") for an example of usage.

    Parameters:
    :   **cache\_instance** – An instance of redis.Redis.
        The user is responsible for managing the Redis instance’s lifetime.

    *classmethod* \_from\_config(*config: [RedisStoreConfig](#autogen_ext.cache_store.redis.RedisStoreConfig "autogen_ext.cache_store.redis.RedisStoreConfig")*) → [Self](https://docs.python.org/3/library/typing.html#typing.Self "(in Python v3.13)")[#](#autogen_ext.cache_store.redis.RedisStore._from_config "Link to this definition")
    :   Create a new instance of the component from a configuration object.

        Parameters:
        :   **config** (*T*) – The configuration object.

        Returns:
        :   **Self** – The new instance of the component.

    \_to\_config() → [RedisStoreConfig](#autogen_ext.cache_store.redis.RedisStoreConfig "autogen_ext.cache_store.redis.RedisStoreConfig")[#](#autogen_ext.cache_store.redis.RedisStore._to_config "Link to this definition")
    :   Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
        :   **T** – The configuration of the component.

    component\_config\_schema[#](#autogen_ext.cache_store.redis.RedisStore.component_config_schema "Link to this definition")
    :   alias of [`RedisStoreConfig`](#autogen_ext.cache_store.redis.RedisStoreConfig "autogen_ext.cache_store.redis.RedisStoreConfig")

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.cache\_store.redis.RedisStore'*[#](#autogen_ext.cache_store.redis.RedisStore.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    get(*key: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *default: T | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → T | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.cache_store.redis.RedisStore.get "Link to this definition")
    :   Retrieve an item from the store.

        Parameters:
        :   * **key** – The key identifying the item in the store.
            * **default** (*optional*) – The default value to return if the key is not found.
              Defaults to None.

        Returns:
        :   **The value associated with the key if found, else the default value.**

    set(*key: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *value: T*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.cache_store.redis.RedisStore.set "Link to this definition")
    :   Set an item in the store.

        Parameters:
        :   * **key** – The key under which the item is to be stored.
            * **value** – The value to be stored in the store.

*pydantic model* RedisStoreConfig[#](#autogen_ext.cache_store.redis.RedisStoreConfig "Link to this definition")
:   Bases: `BaseModel`

    Configuration for RedisStore

    Show JSON schema

    ```
    {
       "title": "RedisStoreConfig",
       "description": "Configuration for RedisStore",
       "type": "object",
       "properties": {
          "host": {
             "default": "localhost",
             "title": "Host",
             "type": "string"
          },
          "port": {
             "default": 6379,
             "title": "Port",
             "type": "integer"
          },
          "db": {
             "default": 0,
             "title": "Db",
             "type": "integer"
          },
          "username": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Username"
          },
          "password": {
             "anyOf": [
                {
                   "type": "string"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Password"
          },
          "ssl": {
             "default": false,
             "title": "Ssl",
             "type": "boolean"
          },
          "socket_timeout": {
             "anyOf": [
                {
                   "type": "number"
                },
                {
                   "type": "null"
                }
             ],
             "default": null,
             "title": "Socket Timeout"
          }
       }
    }

    ```

    Fields:
    :   * [`db (int)`](#autogen_ext.cache_store.redis.RedisStoreConfig.db "autogen_ext.cache_store.redis.RedisStoreConfig.db")
        * [`host (str)`](#autogen_ext.cache_store.redis.RedisStoreConfig.host "autogen_ext.cache_store.redis.RedisStoreConfig.host")
        * [`password (str | None)`](#autogen_ext.cache_store.redis.RedisStoreConfig.password "autogen_ext.cache_store.redis.RedisStoreConfig.password")
        * [`port (int)`](#autogen_ext.cache_store.redis.RedisStoreConfig.port "autogen_ext.cache_store.redis.RedisStoreConfig.port")
        * [`socket_timeout (float | None)`](#autogen_ext.cache_store.redis.RedisStoreConfig.socket_timeout "autogen_ext.cache_store.redis.RedisStoreConfig.socket_timeout")
        * [`ssl (bool)`](#autogen_ext.cache_store.redis.RedisStoreConfig.ssl "autogen_ext.cache_store.redis.RedisStoreConfig.ssl")
        * [`username (str | None)`](#autogen_ext.cache_store.redis.RedisStoreConfig.username "autogen_ext.cache_store.redis.RedisStoreConfig.username")

    *field* db*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")* *= 0*[#](#autogen_ext.cache_store.redis.RedisStoreConfig.db "Link to this definition")

    *field* host*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *= 'localhost'*[#](#autogen_ext.cache_store.redis.RedisStoreConfig.host "Link to this definition")

    *field* password*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.cache_store.redis.RedisStoreConfig.password "Link to this definition")

    *field* port*: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")* *= 6379*[#](#autogen_ext.cache_store.redis.RedisStoreConfig.port "Link to this definition")

    *field* socket\_timeout*: [float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.cache_store.redis.RedisStoreConfig.socket_timeout "Link to this definition")

    *field* ssl*: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")* *= False*[#](#autogen_ext.cache_store.redis.RedisStoreConfig.ssl "Link to this definition")

    *field* username*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")* *= None*[#](#autogen_ext.cache_store.redis.RedisStoreConfig.username "Link to this definition")

### autogen\_ext.runtimes.grpc[#](#module-autogen_ext.runtimes.grpc "Link to this heading")

*class* GrpcWorkerAgentRuntime(*host\_address: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *tracer\_provider: TracerProvider | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_grpc\_config: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tuple](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *payload\_serialization\_format: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = JSON\_DATA\_CONTENT\_TYPE*)[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime "Link to this definition")
:   Bases: [`AgentRuntime`](#autogen_core.AgentRuntime "autogen_core._agent_runtime.AgentRuntime")

    An agent runtime for running remote or cross-language agents.

    Agent messaging uses protobufs from [agent\_worker.proto](https://github.com/microsoft/autogen/blob/main/protos/agent_worker.proto) and `CloudEvent` from [cloudevent.proto](https://github.com/microsoft/autogen/blob/main/protos/cloudevent.proto).

    Cross-language agents will additionally require all agents use shared protobuf schemas for any message types that are sent between agents.

    add\_message\_serializer(*serializer: [MessageSerializer](#autogen_core.MessageSerializer "autogen_core._serialization.MessageSerializer")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] | [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[MessageSerializer](#autogen_core.MessageSerializer "autogen_core._serialization.MessageSerializer")[[Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.add_message_serializer "Link to this definition")
    :   Add a new message serialization serializer to the runtime

        Note: This will deduplicate serializers based on the type\_name and data\_content\_type properties

        Parameters:
        :   **serializer** ([*MessageSerializer*](#autogen_core.MessageSerializer "autogen_core.MessageSerializer")*[**Any**]* *|* *Sequence**[*[*MessageSerializer*](#autogen_core.MessageSerializer "autogen_core.MessageSerializer")*[**Any**]**]*) – The serializer/s to add

    *async* add\_subscription(*subscription: [Subscription](#autogen_core.Subscription "autogen_core._subscription.Subscription")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.add_subscription "Link to this definition")
    :   Add a new subscription that the runtime should fulfill when processing published messages

        Parameters:
        :   **subscription** ([*Subscription*](#autogen_core.Subscription "autogen_core.Subscription")) – The subscription to add

    *async* agent\_load\_state(*agent: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.agent_load_state "Link to this definition")
    :   Load the state of a single agent.

        Parameters:
        :   * **agent** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent id.
            * **state** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]*) – The saved state.

    *async* agent\_metadata(*agent: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*) → [AgentMetadata](#autogen_core.AgentMetadata "autogen_core._agent_metadata.AgentMetadata")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.agent_metadata "Link to this definition")
    :   Get the metadata for an agent.

        Parameters:
        :   **agent** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent id.

        Returns:
        :   **AgentMetadata** – The agent metadata.

    *async* agent\_save\_state(*agent: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*) → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.agent_save_state "Link to this definition")
    :   Save the state of a single agent.

        The structure of the state is implementation defined and can be any JSON serializable object.

        Parameters:
        :   **agent** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent id.

        Returns:
        :   **Mapping[str, Any]** – The saved state.

    *async* get(*id\_or\_type: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType") | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, */*, *key: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'default'*, *\**, *lazy: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*) → [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.get "Link to this definition")

    *async* load\_state(*state: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.load_state "Link to this definition")
    :   Load the state of the entire runtime, including all hosted agents. The state should be the same as the one returned by [`save_state()`](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.save_state "autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.save_state").

        Parameters:
        :   **state** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]*) – The saved state.

    *async* publish\_message(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *topic\_id: [TopicId](#autogen_core.TopicId "autogen_core._topic.TopicId")*, *\**, *sender: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *message\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.publish_message "Link to this definition")
    :   Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.

        No responses are expected from publishing.

        Parameters:
        :   * **message** (*Any*) – The message to publish.
            * **topic** ([*TopicId*](#autogen_core.TopicId "autogen_core.TopicId")) – The topic to publish the message to.
            * **sender** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId") *|* *None**,* *optional*) – The agent which sent the message. Defaults to None.
            * **cancellation\_token** ([*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken") *|* *None**,* *optional*) – Token used to cancel an in progress. Defaults to None.
            * **message\_id** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *|* *None**,* *optional*) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.

        Raises:
        :   [**UndeliverableException**](#autogen_core.exceptions.UndeliverableException "autogen_core.exceptions.UndeliverableException") – If the message cannot be delivered.

    *async* register\_factory(*type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType")*, *agent\_factory: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[], T | [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[T]]*, *\**, *expected\_class: [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[T] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [AgentType](#autogen_core.AgentType "autogen_core._agent_type.AgentType")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.register_factory "Link to this definition")
    :   Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

        Note

        This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

        Example:

        ```
        from dataclasses import dataclass

        from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
        from autogen_core.models import UserMessage

        @dataclass
        class MyMessage:
            content: str

        class MyAgent(RoutedAgent):
            def __init__(self) -> None:
                super().__init__("My core agent")

            @event
            async def handler(self, message: UserMessage, context: MessageContext) -> None:
                print("Event received: ", message.content)

        async def my_agent_factory():
            return MyAgent()

        async def main() -> None:
            runtime: AgentRuntime = ...  # type: ignore
            await runtime.register_factory("my_agent", lambda: MyAgent())

        import asyncio

        asyncio.run(main())

        ```

        Parameters:
        :   * **type** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
            * **agent\_factory** (*Callable**[**[**]**,* *T**]*) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen\_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
            * **expected\_class** ([*type*](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")*[**T**]* *|* *None**,* *optional*) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.

    *async* remove\_subscription(*id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.remove_subscription "Link to this definition")
    :   Remove a subscription from the runtime

        Parameters:
        :   **id** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")) – id of the subscription to remove

        Raises:
        :   [**LookupError**](https://docs.python.org/3/library/exceptions.html#LookupError "(in Python v3.13)") – If the subscription does not exist

    *async* save\_state() → [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")][#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.save_state "Link to this definition")
    :   Save the state of the entire runtime, including all hosted agents. The only way to restore the state is to pass it to [`load_state()`](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.load_state "autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.load_state").

        The structure of the state is implementation defined and can be any JSON serializable object.

        Returns:
        :   **Mapping[str, Any]** – The saved state.

    *async* send\_message(*message: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*, *recipient: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *\**, *sender: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *message\_id: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.send_message "Link to this definition")
    :   Send a message to an agent and get a response.

        Parameters:
        :   * **message** (*Any*) – The message to send.
            * **recipient** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent to send the message to.
            * **sender** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId") *|* *None**,* *optional*) – Agent which sent the message. Should **only** be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
            * **cancellation\_token** ([*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken") *|* *None**,* *optional*) – Token used to cancel an in progress . Defaults to None.

        Raises:
        :   * [**CantHandleException**](#autogen_core.exceptions.CantHandleException "autogen_core.exceptions.CantHandleException") – If the recipient cannot handle the message.
            * [**UndeliverableException**](#autogen_core.exceptions.UndeliverableException "autogen_core.exceptions.UndeliverableException") – If the message cannot be delivered.
            * **Other** – Any other exception raised by the recipient.

        Returns:
        :   **Any** – The response from the agent.

    *async* start() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.start "Link to this definition")
    :   Start the runtime in a background task.

    *async* stop() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.stop "Link to this definition")
    :   Stop the runtime immediately.

    *async* stop\_when\_signal(*signals: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Signals](https://docs.python.org/3/library/signal.html#signal.Signals "(in Python v3.13)")] = (signal.SIGTERM, signal.SIGINT)*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.stop_when_signal "Link to this definition")
    :   Stop the runtime when a signal is received.

    *async* try\_get\_underlying\_agent\_instance(*id: [AgentId](#autogen_core.AgentId "autogen_core._agent_id.AgentId")*, *type: [Type](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.13)")[T] = Agent*) → T[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime.try_get_underlying_agent_instance "Link to this definition")
    :   Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.

        If the underlying agent is not accessible, this will raise an exception.

        Parameters:
        :   * **id** ([*AgentId*](#autogen_core.AgentId "autogen_core.AgentId")) – The agent id.
            * **type** (*Type**[**T**]**,* *optional*) – The expected type of the agent. Defaults to Agent.

        Returns:
        :   **T** – The concrete agent instance.

        Raises:
        :   * [**LookupError**](https://docs.python.org/3/library/exceptions.html#LookupError "(in Python v3.13)") – If the agent is not found.
            * [**NotAccessibleError**](#autogen_core.exceptions.NotAccessibleError "autogen_core.exceptions.NotAccessibleError") – If the agent is not accessible, for example if it is located remotely.
            * [**TypeError**](https://docs.python.org/3/library/exceptions.html#TypeError "(in Python v3.13)") – If the agent is not of the expected type.

*class* GrpcWorkerAgentRuntimeHost(*address: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *extra\_grpc\_config: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tuple](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHost "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    start() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHost.start "Link to this definition")
    :   Start the server in a background task.

    *async* stop(*grace: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 5*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHost.stop "Link to this definition")
    :   Stop the server.

    *async* stop\_when\_signal(*grace: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 5*, *signals: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Signals](https://docs.python.org/3/library/signal.html#signal.Signals "(in Python v3.13)")] = (signal.SIGTERM, signal.SIGINT)*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHost.stop_when_signal "Link to this definition")
    :   Stop the server when a signal is received.

*class* GrpcWorkerAgentRuntimeHostServicer[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHostServicer "Link to this definition")
:   Bases: `AgentRpcServicer`

    A gRPC servicer that hosts message delivery service for agents.

    *async* AddSubscription(*request: AddSubscriptionRequest*, *context: ServicerContext[AddSubscriptionRequest, AddSubscriptionResponse]*) → AddSubscriptionResponse[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHostServicer.AddSubscription "Link to this definition")
    :   Missing associated documentation comment in .proto file.

    *async* GetSubscriptions(*request: GetSubscriptionsRequest*, *context: ServicerContext[GetSubscriptionsRequest, GetSubscriptionsResponse]*) → GetSubscriptionsResponse[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHostServicer.GetSubscriptions "Link to this definition")
    :   Missing associated documentation comment in .proto file.

    *async* OpenChannel(*request\_iterator: [AsyncIterator](https://docs.python.org/3/library/typing.html#typing.AsyncIterator "(in Python v3.13)")[Message]*, *context: ServicerContext[Message, Message]*) → [AsyncIterator](https://docs.python.org/3/library/typing.html#typing.AsyncIterator "(in Python v3.13)")[Message][#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHostServicer.OpenChannel "Link to this definition")
    :   Missing associated documentation comment in .proto file.

    *async* OpenControlChannel(*request\_iterator: [AsyncIterator](https://docs.python.org/3/library/typing.html#typing.AsyncIterator "(in Python v3.13)")[ControlMessage]*, *context: ServicerContext[ControlMessage, ControlMessage]*) → [AsyncIterator](https://docs.python.org/3/library/typing.html#typing.AsyncIterator "(in Python v3.13)")[ControlMessage][#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHostServicer.OpenControlChannel "Link to this definition")
    :   Missing associated documentation comment in .proto file.

    *async* RegisterAgent(*request: RegisterAgentTypeRequest*, *context: ServicerContext[RegisterAgentTypeRequest, RegisterAgentTypeResponse]*) → RegisterAgentTypeResponse[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHostServicer.RegisterAgent "Link to this definition")
    :   Missing associated documentation comment in .proto file.

    *async* RemoveSubscription(*request: RemoveSubscriptionRequest*, *context: ServicerContext[RemoveSubscriptionRequest, RemoveSubscriptionResponse]*) → RemoveSubscriptionResponse[#](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHostServicer.RemoveSubscription "Link to this definition")
    :   Missing associated documentation comment in .proto file.

### autogen\_ext.auth.azure[#](#module-autogen_ext.auth.azure "Link to this heading")

*class* AzureTokenProvider(*credential: TokenCredential | SupportsTokenInfo*, *\*scopes: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*)[#](#autogen_ext.auth.azure.AzureTokenProvider "Link to this definition")
:   Bases: [`ComponentBase`](#autogen_core.ComponentBase "autogen_core._component_config.ComponentBase")[[`TokenProviderConfig`](#autogen_ext.auth.azure.TokenProviderConfig "autogen_ext.auth.azure.TokenProviderConfig")], [`Component`](#autogen_core.Component "autogen_core._component_config.Component")[[`TokenProviderConfig`](#autogen_ext.auth.azure.TokenProviderConfig "autogen_ext.auth.azure.TokenProviderConfig")]

    component\_config\_schema[#](#autogen_ext.auth.azure.AzureTokenProvider.component_config_schema "Link to this definition")
    :   alias of [`TokenProviderConfig`](#autogen_ext.auth.azure.TokenProviderConfig "autogen_ext.auth.azure.TokenProviderConfig")

    component\_provider\_override*: ClassVar[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")]* *= 'autogen\_ext.auth.azure.AzureTokenProvider'*[#](#autogen_ext.auth.azure.AzureTokenProvider.component_provider_override "Link to this definition")
    :   Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

    component\_type*: ClassVar[ComponentType]* *= 'token\_provider'*[#](#autogen_ext.auth.azure.AzureTokenProvider.component_type "Link to this definition")
    :   The logical type of the component.

*pydantic model* TokenProviderConfig[#](#autogen_ext.auth.azure.TokenProviderConfig "Link to this definition")
:   Bases: `BaseModel`

    Show JSON schema

    ```
    {
       "title": "TokenProviderConfig",
       "type": "object",
       "properties": {
          "provider_kind": {
             "title": "Provider Kind",
             "type": "string"
          },
          "scopes": {
             "items": {
                "type": "string"
             },
             "title": "Scopes",
             "type": "array"
          }
       },
       "required": [
          "provider_kind",
          "scopes"
       ]
    }

    ```

    Fields:
    :   * [`provider_kind (str)`](#autogen_ext.auth.azure.TokenProviderConfig.provider_kind "autogen_ext.auth.azure.TokenProviderConfig.provider_kind")
        * [`scopes (List[str])`](#autogen_ext.auth.azure.TokenProviderConfig.scopes "autogen_ext.auth.azure.TokenProviderConfig.scopes")

    *field* provider\_kind*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")* *[Required]*[#](#autogen_ext.auth.azure.TokenProviderConfig.provider_kind "Link to this definition")

    *field* scopes*: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]* *[Required]*[#](#autogen_ext.auth.azure.TokenProviderConfig.scopes "Link to this definition")

### autogen\_ext.experimental.task\_centric\_memory[#](#module-autogen_ext.experimental.task_centric_memory "Link to this heading")

*class* MemoryController(*reset: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*, *client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *task\_assignment\_callback: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")], [Awaitable](https://docs.python.org/3/library/typing.html#typing.Awaitable "(in Python v3.13)")[[Tuple](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]]] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *config: MemoryControllerConfig | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *logger: [PageLogger](#autogen_ext.experimental.task_centric_memory.utils.PageLogger "autogen_ext.experimental.task_centric_memory.utils.page_logger.PageLogger") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.experimental.task_centric_memory.MemoryController "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    (EXPERIMENTAL, RESEARCH IN PROGRESS)

    Implements fast, memory-based learning, and manages the flow of information to and from a memory bank.

    Parameters:
    :   * **reset** – True to empty the memory bank before starting.
        * **client** – The model client to use internally.
        * **task\_assignment\_callback** – An optional callback used to assign a task to any agent managed by the caller.
        * **config** –

          An optional dict that can be used to override the following values:

          + max\_train\_trials: The maximum number of learning iterations to attempt when training on a task.
          + max\_test\_trials: The total number of attempts made when testing for failure on a task.
          + MemoryBank: A config dict passed to MemoryBank.
        * **logger** – An optional logger. If None, a default logger will be created.

    Example

    The task-centric-memory extra first needs to be installed:

    ```
    pip install "autogen-ext[task-centric-memory]"

    ```

    The following code snippet shows how to use this class for the most basic storage and retrieval of memories.:

    ```
    import asyncio
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    from autogen_ext.experimental.task_centric_memory import MemoryController
    from autogen_ext.experimental.task_centric_memory.utils import PageLogger

    async def main() -> None:
        client = OpenAIChatCompletionClient(model="gpt-4o")
        logger = PageLogger(config={"level": "DEBUG", "path": "./pagelogs/quickstart"})  # Optional, but very useful.
        memory_controller = MemoryController(reset=True, client=client, logger=logger)

        # Add a few task-insight pairs as memories, where an insight can be any string that may help solve the task.
        await memory_controller.add_memo(task="What color do I like?", insight="Deep blue is my favorite color")
        await memory_controller.add_memo(task="What's another color I like?", insight="I really like cyan")
        await memory_controller.add_memo(task="What's my favorite food?", insight="Halibut is my favorite")

        # Retrieve memories for a new task that's related to only two of the stored memories.
        memos = await memory_controller.retrieve_relevant_memos(task="What colors do I like most?")
        print("{} memories retrieved".format(len(memos)))
        for memo in memos:
            print("- " + memo.insight)

    asyncio.run(main())

    ```

    *async* add\_memo(*insight: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *task: [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") | [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = None*, *index\_on\_both: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.MemoryController.add_memo "Link to this definition")
    :   Adds one insight to the memory bank, using the task (if provided) as context.

    *async* add\_task\_solution\_pair\_to\_memory(*task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *solution: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.MemoryController.add_task_solution_pair_to_memory "Link to this definition")
    :   Adds a task-solution pair to the memory bank, to be retrieved together later as a combined insight.
        This is useful when the task-solution pair is an exemplar of solving a task related to some other task.

    *async* assign\_task(*task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *use\_memory: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*, *should\_await: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.MemoryController.assign_task "Link to this definition")
    :   Assigns a task to some agent through the task\_assignment\_callback, along with any relevant memories.

    *async* consider\_memo\_storage(*text: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.MemoryController.consider_memo_storage "Link to this definition")
    :   Tries to extract any advice from the given text and add it to memory.

    *async* handle\_user\_message(*text: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *should\_await: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.MemoryController.handle_user_message "Link to this definition")
    :   Handles a user message by extracting any advice as an insight to be stored in memory, and then calling assign\_task().

    reset\_memory() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.MemoryController.reset_memory "Link to this definition")
    :   Empties the memory bank in RAM and on disk.

    *async* retrieve\_relevant\_memos(*task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[Memo][#](#autogen_ext.experimental.task_centric_memory.MemoryController.retrieve_relevant_memos "Link to this definition")
    :   Retrieves any memos from memory that seem relevant to the task.

    *async* test\_on\_task(*task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *expected\_answer: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *num\_trials: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 1*) → [Tuple](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)"), [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")][#](#autogen_ext.experimental.task_centric_memory.MemoryController.test_on_task "Link to this definition")
    :   Assigns a task to the agent, along with any relevant memos retrieved from memory.

    *async* train\_on\_task(*task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *expected\_answer: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.MemoryController.train_on_task "Link to this definition")
    :   Repeatedly assigns a task to the agent, and tries to learn from failures by creating useful insights as memories.

### autogen\_ext.experimental.task\_centric\_memory.utils[#](#module-autogen_ext.experimental.task_centric_memory.utils "Link to this heading")

*class* Apprentice(*client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *config: ApprenticeConfig | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *logger: [PageLogger](#autogen_ext.experimental.task_centric_memory.utils.PageLogger "autogen_ext.experimental.task_centric_memory.utils.page_logger.PageLogger") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.experimental.task_centric_memory.utils.Apprentice "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    A minimal wrapper combining task-centric memory with an agent or team.
    Applications may use the Apprentice class, or they may directly instantiate
    and call the Memory Controller using this class as an example.

    Parameters:
    :   * **client** – The client to call the model.
        * **config** –

          An optional dict that can be used to override the following values:

          + name\_of\_agent\_or\_team: The name of the target agent or team for assigning tasks to.
          + disable\_prefix\_caching: True to disable prefix caching by prepending random ints to the first message.
          + MemoryController: A config dict passed to MemoryController.
        * **logger** – An optional logger. If None, a default logger will be created.

    *async* add\_task\_solution\_pair\_to\_memory(*task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *solution: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.Apprentice.add_task_solution_pair_to_memory "Link to this definition")
    :   Adds a task-solution pair to the memory bank, to be retrieved together later as a combined insight.
        This is useful when the insight is a demonstration of how to solve a given type of task.

    *async* assign\_task(*task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *use\_memory: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*, *should\_await: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.Apprentice.assign_task "Link to this definition")
    :   Assigns a task to the agent, along with any relevant insights/memories.

    *async* assign\_task\_to\_agent\_or\_team(*task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [Tuple](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")][#](#autogen_ext.experimental.task_centric_memory.utils.Apprentice.assign_task_to_agent_or_team "Link to this definition")
    :   Passes the given task to the target agent or team.

    *async* handle\_user\_message(*text: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *should\_await: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.Apprentice.handle_user_message "Link to this definition")
    :   Handles a user message, extracting any advice and assigning a task to the agent.

    reset\_memory() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.Apprentice.reset_memory "Link to this definition")
    :   Resets the memory bank.

    *async* train\_on\_task(*task: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *expected\_answer: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.Apprentice.train_on_task "Link to this definition")
    :   Repeatedly assigns a task to the completion agent, and tries to learn from failures by creating useful insights as memories.

*class* ChatCompletionClientRecorder(*client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *mode: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")['record', 'replay']*, *session\_file\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *logger: [PageLogger](#autogen_ext.experimental.task_centric_memory.utils.PageLogger "autogen_ext.experimental.task_centric_memory.utils.page_logger.PageLogger") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.experimental.task_centric_memory.utils.ChatCompletionClientRecorder "Link to this definition")
:   Bases: [`ChatCompletionClient`](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")

    A chat completion client that supports fast, large-scale tests of code calling LLM clients.

    Two modes are supported:

    > 1. “record”: delegates to the underlying client while also recording the input messages and responses,
    >    which are saved to disk when finalize() is called.
    > 2. “replay”: loads previously recorded message and responses from disk, then on each call
    >    checks that its message matches the recorded message, and returns the recorded response.

    The recorded data is stored as a JSON list of records. Each record is a dictionary with a “mode”
    field (either “create” or “create\_stream”), a serialized list of messages, and either a “response” (for
    create calls) or a “stream” (a list of streamed outputs for create\_stream calls).

    ReplayChatCompletionClient and ChatCompletionCache do similar things, but with significant differences:
    - ReplayChatCompletionClient replays pre-defined responses in a specified order
    without recording anything or checking the messages sent to the client.
    - ChatCompletionCache caches responses and replays them for messages that have been seen before,
    regardless of order, and calls the base client for any uncached messages.

    actual\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.experimental.task_centric_memory.utils.ChatCompletionClientRecorder.actual_usage "Link to this definition")

    *property* capabilities*: [ModelCapabilities](#autogen_core.models.ModelCapabilities "autogen_core.models._model_client.ModelCapabilities")*[#](#autogen_ext.experimental.task_centric_memory.utils.ChatCompletionClientRecorder.capabilities "Link to this definition")

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.ChatCompletionClientRecorder.close "Link to this definition")

    count\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.ChatCompletionClientRecorder.count_tokens "Link to this definition")

    *async* create(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult")[#](#autogen_ext.experimental.task_centric_memory.utils.ChatCompletionClientRecorder.create "Link to this definition")
    :   Creates a single response from the model.

        Parameters:
        :   * **messages** (*Sequence**[**LLMMessage**]*) – The messages to send to the model.
            * **tools** (*Sequence**[*[*Tool*](#autogen_core.tools.Tool "autogen_core.tools.Tool") *|* [*ToolSchema*](#autogen_core.tools.ToolSchema "autogen_core.tools.ToolSchema")*]**,* *optional*) – The tools to use with the model. Defaults to [].
            * **json\_output** (*Optional**[*[*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") *|* [*type*](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")*[**BaseModel**]**]**,* *optional*) – Whether to use JSON mode, structured output, or neither.
              Defaults to None. If set to a [Pydantic BaseModel](https://docs.pydantic.dev/latest/usage/models/#model) type,
              it will be used as the output type for structured output.
              If set to a boolean, it will be used to determine whether to use JSON mode or not.
              If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
            * **extra\_create\_args** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]**,* *optional*) – Extra arguments to pass to the underlying client. Defaults to {}.
            * **cancellation\_token** (*Optional**[*[*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken")*]**,* *optional*) – A token for cancellation. Defaults to None.

        Returns:
        :   **CreateResult** – The result of the model call.

    create\_stream(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*, *json\_output: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [type](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")[BaseModel] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *extra\_create\_args: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")] = {}*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [AsyncGenerator](https://docs.python.org/3/library/typing.html#typing.AsyncGenerator "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")][#](#autogen_ext.experimental.task_centric_memory.utils.ChatCompletionClientRecorder.create_stream "Link to this definition")
    :   Creates a stream of string chunks from the model ending with a CreateResult.

        Parameters:
        :   * **messages** (*Sequence**[**LLMMessage**]*) – The messages to send to the model.
            * **tools** (*Sequence**[*[*Tool*](#autogen_core.tools.Tool "autogen_core.tools.Tool") *|* [*ToolSchema*](#autogen_core.tools.ToolSchema "autogen_core.tools.ToolSchema")*]**,* *optional*) – The tools to use with the model. Defaults to [].
            * **json\_output** (*Optional**[*[*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") *|* [*type*](https://docs.python.org/3/library/functions.html#type "(in Python v3.13)")*[**BaseModel**]**]**,* *optional*) –

              Whether to use JSON mode, structured output, or neither.
              Defaults to None. If set to a [Pydantic BaseModel](https://docs.pydantic.dev/latest/usage/models/#model) type,
              it will be used as the output type for structured output.
              If set to a boolean, it will be used to determine whether to use JSON mode or not.
              If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
            * **extra\_create\_args** (*Mapping**[*[*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*,* *Any**]**,* *optional*) – Extra arguments to pass to the underlying client. Defaults to {}.
            * **cancellation\_token** (*Optional**[*[*CancellationToken*](#autogen_core.CancellationToken "autogen_core.CancellationToken")*]**,* *optional*) – A token for cancellation. Defaults to None.

        Returns:
        :   **AsyncGenerator[Union[str, CreateResult], None]** – A generator that yields string chunks and ends with a `CreateResult`.

    finalize() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.ChatCompletionClientRecorder.finalize "Link to this definition")
    :   In record mode, saves the accumulated records to disk.
        In replay mode, makes sure all the records were checked.

    *property* model\_info*: [ModelInfo](#autogen_core.models.ModelInfo "autogen_core.models._model_client.ModelInfo")*[#](#autogen_ext.experimental.task_centric_memory.utils.ChatCompletionClientRecorder.model_info "Link to this definition")

    remaining\_tokens(*messages: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *\**, *tools: [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[Tool](#autogen_core.tools.Tool "autogen_core.tools._base.Tool") | [ToolSchema](#autogen_core.tools.ToolSchema "autogen_core.tools._base.ToolSchema")] = []*) → [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.ChatCompletionClientRecorder.remaining_tokens "Link to this definition")

    total\_usage() → [RequestUsage](#autogen_core.models.RequestUsage "autogen_core.models._types.RequestUsage")[#](#autogen_ext.experimental.task_centric_memory.utils.ChatCompletionClientRecorder.total_usage "Link to this definition")

*class* Grader(*client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models._model_client.ChatCompletionClient")*, *logger: [PageLogger](#autogen_ext.experimental.task_centric_memory.utils.PageLogger "autogen_ext.experimental.task_centric_memory.utils.page_logger.PageLogger") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.experimental.task_centric_memory.utils.Grader "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    Runs basic tests, and determines task success without limitation to string matches.

    Parameters:
    :   * **client** – The client to call the model.
        * **logger** – An optional logger. If None, no logging will be performed.

    *async* call\_model(*summary: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *user\_content: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Image](#autogen_core.Image "autogen_core._image.Image")]*, *system\_message\_content: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *keep\_these\_messages: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.Grader.call_model "Link to this definition")
    :   Calls the model client with the given input and returns the response.

    *async* is\_response\_correct(*task\_description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *response\_to\_be\_graded: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *correct\_answer: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [Tuple](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")[[bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")][#](#autogen_ext.experimental.task_centric_memory.utils.Grader.is_response_correct "Link to this definition")
    :   Determines whether the response is equivalent to the task’s correct answer.

    *async* test\_apprentice(*apprentice: [Apprentice](#autogen_ext.experimental.task_centric_memory.utils.Apprentice "autogen_ext.experimental.task_centric_memory.utils.Apprentice")*, *task\_description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *expected\_answer: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *num\_trials: [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")*, *use\_memory: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")*, *client: [ChatCompletionClient](#autogen_core.models.ChatCompletionClient "autogen_core.models.ChatCompletionClient")*) → Tuple[[int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)"), [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")][#](#autogen_ext.experimental.task_centric_memory.utils.Grader.test_apprentice "Link to this definition")

*class* PageLogger(*config: PageLoggerConfig | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger "Link to this definition")
:   Bases: [`object`](https://docs.python.org/3/library/functions.html#object "(in Python v3.13)")

    Logs text and images to a set of HTML pages, one per function/method, linked to each other in a call tree.

    Parameters:
    :   **config** –

        An optional dict that can be used to override the following values:

        * level: The logging level, one of DEBUG, INFO, WARNING, ERROR, CRITICAL, or NONE.
        * path: The path to the directory where the log files will be written.

    add\_link\_to\_image(*description: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *source\_image\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.add_link_to_image "Link to this definition")
    :   Inserts a thumbnail link to an image to the page.

    critical(*line: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.critical "Link to this definition")
    :   Adds CRITICAL text to the current page if debugging level <= CRITICAL.

    debug(*line: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.debug "Link to this definition")
    :   Adds DEBUG text to the current page if debugging level <= DEBUG.

    enter\_function() → Page | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.enter_function "Link to this definition")
    :   Adds a new page corresponding to the current function call.

    error(*line: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.error "Link to this definition")
    :   Adds ERROR text to the current page if debugging level <= ERROR.

    finalize() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.finalize "Link to this definition")

    flush(*finished: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.flush "Link to this definition")
    :   Writes the current state of the log to disk.

    info(*line: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.info "Link to this definition")
    :   Adds INFO text to the current page if debugging level <= INFO.

    leave\_function() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.leave_function "Link to this definition")
    :   Finishes the page corresponding to the current function call.

    log\_dict\_list(*content: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")]]*, *summary: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.log_dict_list "Link to this definition")
    :   Adds a page containing a list of dicts.

    log\_link\_to\_local\_file(*file\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.log_link_to_local_file "Link to this definition")
    :   Returns a link to a local file in the log.

    log\_message\_content(*message\_content: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [Image](#autogen_core.Image "autogen_core._image.Image")] | [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[FunctionCall](#autogen_core.FunctionCall "autogen_core._types.FunctionCall")] | [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[FunctionExecutionResult](#autogen_core.models.FunctionExecutionResult "autogen_core.models._types.FunctionExecutionResult")]*, *summary: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.log_message_content "Link to this definition")
    :   Adds a page containing the message’s content, including any images.

    log\_model\_call(*summary: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *input\_messages: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *response: [CreateResult](#autogen_core.models.CreateResult "autogen_core.models._types.CreateResult")*) → Page | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.log_model_call "Link to this definition")
    :   Logs messages sent to a model and the TaskResult response to a new page.

    log\_model\_task(*summary: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *input\_messages: [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[Annotated](https://docs.python.org/3/library/typing.html#typing.Annotated "(in Python v3.13)")[[SystemMessage](#autogen_core.models.SystemMessage "autogen_core.models._types.SystemMessage") | [UserMessage](#autogen_core.models.UserMessage "autogen_core.models._types.UserMessage") | [AssistantMessage](#autogen_core.models.AssistantMessage "autogen_core.models._types.AssistantMessage") | [FunctionExecutionResultMessage](#autogen_core.models.FunctionExecutionResultMessage "autogen_core.models._types.FunctionExecutionResultMessage"), FieldInfo(annotation=NoneType, required=True, discriminator='type')]]*, *task\_result: [TaskResult](#autogen_agentchat.base.TaskResult "autogen_agentchat.base._task.TaskResult")*) → Page | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.log_model_task "Link to this definition")
    :   Logs messages sent to a model and the TaskResult response to a new page.

    warning(*line: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.PageLogger.warning "Link to this definition")
    :   Adds WARNING text to the current page if debugging level <= WARNING.

*class* Teachability(*memory\_controller: [MemoryController](#autogen_ext.experimental.task_centric_memory.MemoryController "autogen_ext.experimental.task_centric_memory.MemoryController")*, *name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*)[#](#autogen_ext.experimental.task_centric_memory.utils.Teachability "Link to this definition")
:   Bases: [`Memory`](#autogen_core.memory.Memory "autogen_core.memory._base_memory.Memory")

    Gives an AssistantAgent the ability to learn quickly from user teachings, hints, and advice.

    Steps for usage:
    1. Instantiate MemoryController.
    2. Instantiate Teachability, passing the memory controller as a parameter.
    3. Instantiate an AssistantAgent, passing the teachability instance (wrapped in a list) as the memory parameter.
    4. Use the AssistantAgent as usual, such as for chatting with the user.

    *async* add(*content: [MemoryContent](#autogen_core.memory.MemoryContent "autogen_core.memory._base_memory.MemoryContent")*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.Teachability.add "Link to this definition")
    :   Tries to extract any advice from the passed content and add it to memory.

    *async* clear() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.Teachability.clear "Link to this definition")
    :   Clear all entries from memory.

    *async* close() → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[#](#autogen_ext.experimental.task_centric_memory.utils.Teachability.close "Link to this definition")
    :   Clean up memory resources.

    *property* name*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[#](#autogen_ext.experimental.task_centric_memory.utils.Teachability.name "Link to this definition")
    :   Get the memory instance identifier.

    *async* query(*query: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [MemoryContent](#autogen_core.memory.MemoryContent "autogen_core.memory._base_memory.MemoryContent")*, *cancellation\_token: [CancellationToken](#autogen_core.CancellationToken "autogen_core._cancellation_token.CancellationToken") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *\*\*kwargs: [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")*) → [MemoryQueryResult](#autogen_core.memory.MemoryQueryResult "autogen_core.memory._base_memory.MemoryQueryResult")[#](#autogen_ext.experimental.task_centric_memory.utils.Teachability.query "Link to this definition")
    :   Returns any memories that seem relevant to the query.

    *async* update\_context(*model\_context: [ChatCompletionContext](#autogen_core.model_context.ChatCompletionContext "autogen_core.model_context._chat_completion_context.ChatCompletionContext")*) → [UpdateContextResult](#autogen_core.memory.UpdateContextResult "autogen_core.memory._base_memory.UpdateContextResult")[#](#autogen_ext.experimental.task_centric_memory.utils.Teachability.update_context "Link to this definition")
    :   Extracts any advice from the last user turn to be stored in memory,
        and adds any relevant memories to the model context.

On this page

* [AgentChat](#document-user-guide/agentchat-user-guide/index)
  + [Installation](#document-user-guide/agentchat-user-guide/installation)
    - [Create a Virtual Environment (optional)](#create-a-virtual-environment-optional)
    - [Install Using pip](#install-using-pip)
    - [Install OpenAI for Model Client](#install-openai-for-model-client)
  + [Quickstart](#document-user-guide/agentchat-user-guide/quickstart)
    - [What’s Next?](#what-s-next)
  + [Migration Guide for v0.2 to v0.4](#document-user-guide/agentchat-user-guide/migration-guide)
    - [What is `v0.4`?](#what-is-v0-4)
    - [New to AutoGen?](#new-to-autogen)
    - [What’s in this guide?](#what-s-in-this-guide)
    - [Model Client](#model-client)
    - [Model Client for OpenAI-Compatible APIs](#model-client-for-openai-compatible-apis)
    - [Model Client Cache](#model-client-cache)
    - [Assistant Agent](#assistant-agent)
    - [Multi-Modal Agent](#multi-modal-agent)
    - [User Proxy](#user-proxy)
    - [RAG Agent](#rag-agent)
    - [Conversable Agent and Register Reply](#conversable-agent-and-register-reply)
    - [Save and Load Agent State](#save-and-load-agent-state)
    - [Two-Agent Chat](#two-agent-chat)
    - [Tool Use](#tool-use)
    - [Chat Result](#chat-result)
    - [Conversion between v0.2 and v0.4 Messages](#conversion-between-v0-2-and-v0-4-messages)
    - [Group Chat](#group-chat)
    - [Group Chat with Resume](#group-chat-with-resume)
    - [Save and Load Group Chat State](#save-and-load-group-chat-state)
    - [Group Chat with Tool Use](#group-chat-with-tool-use)
    - [Group Chat with Custom Selector (Stateflow)](#group-chat-with-custom-selector-stateflow)
    - [Nested Chat](#nested-chat)
    - [Sequential Chat](#sequential-chat)
    - [GPTAssistantAgent](#gptassistantagent)
    - [Long Context Handling](#long-context-handling)
    - [Observability and Control](#observability-and-control)
    - [Code Executors](#code-executors)
  + [Introduction](#document-user-guide/agentchat-user-guide/tutorial/index)
  + [Models](#document-user-guide/agentchat-user-guide/tutorial/models)
    - [Log Model Calls](#log-model-calls)
    - [OpenAI](#openai)
    - [Azure OpenAI](#azure-openai)
    - [Azure AI Foundry](#azure-ai-foundry)
    - [Anthropic (experimental)](#anthropic-experimental)
    - [Ollama (experimental)](#ollama-experimental)
    - [Gemini (experimental)](#gemini-experimental)
    - [Semantic Kernel Adapter](#semantic-kernel-adapter)
  + [Messages](#document-user-guide/agentchat-user-guide/tutorial/messages)
    - [Types of Messages](#types-of-messages)
  + [Agents](#document-user-guide/agentchat-user-guide/tutorial/agents)
    - [Assistant Agent](#assistant-agent)
    - [Getting Responses](#getting-responses)
    - [Multi-Modal Input](#multi-modal-input)
    - [Streaming Messages](#streaming-messages)
    - [Using Tools](#using-tools)
    - [Running an Agent in a Loop](#running-an-agent-in-a-loop)
    - [Structured Output](#structured-output)
    - [Streaming Tokens](#streaming-tokens)
    - [Using Model Context](#using-model-context)
    - [Other Preset Agents](#other-preset-agents)
    - [Next Step](#next-step)
  + [Teams](#document-user-guide/agentchat-user-guide/tutorial/teams)
    - [Creating a Team](#creating-a-team)
    - [Running a Team](#running-a-team)
    - [Observing a Team](#observing-a-team)
    - [Resetting a Team](#resetting-a-team)
    - [Stopping a Team](#stopping-a-team)
    - [Resuming a Team](#resuming-a-team)
    - [Aborting a Team](#aborting-a-team)
    - [Single-Agent Team](#single-agent-team)
  + [Human-in-the-Loop](#document-user-guide/agentchat-user-guide/tutorial/human-in-the-loop)
    - [Providing Feedback During a Run](#providing-feedback-during-a-run)
    - [Providing Feedback to the Next Run](#providing-feedback-to-the-next-run)
  + [Termination](#document-user-guide/agentchat-user-guide/tutorial/termination)
    - [Basic Usage](#basic-usage)
    - [Combining Termination Conditions](#combining-termination-conditions)
    - [Custom Termination Condition](#custom-termination-condition)
  + [Managing State](#document-user-guide/agentchat-user-guide/tutorial/state)
    - [Saving and Loading Agents](#saving-and-loading-agents)
    - [Saving and Loading Teams](#saving-and-loading-teams)
    - [Persisting State (File or Database)](#persisting-state-file-or-database)
  + [Custom Agents](#document-user-guide/agentchat-user-guide/custom-agents)
    - [CountDownAgent](#countdownagent)
    - [ArithmeticAgent](#arithmeticagent)
    - [Using Custom Model Clients in Custom Agents](#using-custom-model-clients-in-custom-agents)
    - [Making the Custom Agent Declarative](#making-the-custom-agent-declarative)
    - [Next Steps](#next-steps)
  + [Selector Group Chat](#document-user-guide/agentchat-user-guide/selector-group-chat)
    - [How Does it Work?](#how-does-it-work)
    - [Example: Web Search/Analysis](#example-web-search-analysis)
    - [Custom Selector Function](#custom-selector-function)
    - [Custom Candidate Function](#custom-candidate-function)
    - [User Feedback](#user-feedback)
    - [Using Reasoning Models](#using-reasoning-models)
  + [Swarm](#document-user-guide/agentchat-user-guide/swarm)
    - [How Does It Work?](#how-does-it-work)
    - [Customer Support Example](#customer-support-example)
    - [Stock Research Example](#stock-research-example)
  + [Magentic-One](#document-user-guide/agentchat-user-guide/magentic-one)
    - [Getting started](#getting-started)
    - [Architecture](#architecture)
    - [Citation](#citation)
  + [Memory and RAG](#document-user-guide/agentchat-user-guide/memory)
  + [ListMemory Example](#listmemory-example)
  + [Custom Memory Stores (Vector DBs, etc.)](#custom-memory-stores-vector-dbs-etc)
  + [RAG Agent: Putting It All Together](#rag-agent-putting-it-all-together)
    - [Building a Simple RAG Agent](#building-a-simple-rag-agent)
  + [Logging](#document-user-guide/agentchat-user-guide/logging)
  + [Serializing Components](#document-user-guide/agentchat-user-guide/serialize-components)
    - [Termination Condition Example](#termination-condition-example)
    - [Agent Example](#agent-example)
    - [Team Example](#team-example)
  + [Examples](#document-user-guide/agentchat-user-guide/examples/index)
    - [Travel Planning](#document-user-guide/agentchat-user-guide/examples/travel-planning)
    - [Company Research](#document-user-guide/agentchat-user-guide/examples/company-research)
    - [Literature Review](#document-user-guide/agentchat-user-guide/examples/literature-review)
* [Core](#document-user-guide/core-user-guide/index)
  + [Installation](#document-user-guide/core-user-guide/installation)
    - [Create a Virtual Environment (optional)](#create-a-virtual-environment-optional)
    - [Install using pip](#install-using-pip)
    - [Install OpenAI for Model Client](#install-openai-for-model-client)
    - [Install Docker for Code Execution (Optional)](#install-docker-for-code-execution-optional)
  + [Quick Start](#document-user-guide/core-user-guide/quickstart)
  + [Agent and Multi-Agent Applications](#document-user-guide/core-user-guide/core-concepts/agent-and-multi-agent-application)
    - [Characteristics of Multi-Agent Applications](#characteristics-of-multi-agent-applications)
  + [Agent Runtime Environments](#document-user-guide/core-user-guide/core-concepts/architecture)
    - [Standalone Agent Runtime](#standalone-agent-runtime)
    - [Distributed Agent Runtime](#distributed-agent-runtime)
  + [Application Stack](#document-user-guide/core-user-guide/core-concepts/application-stack)
    - [An Example Application](#an-example-application)
  + [Agent Identity and Lifecycle](#document-user-guide/core-user-guide/core-concepts/agent-identity-and-lifecycle)
    - [Agent ID](#agent-id)
    - [Agent Lifecycle](#agent-lifecycle)
  + [Topic and Subscription](#document-user-guide/core-user-guide/core-concepts/topic-and-subscription)
    - [Topic](#topic)
    - [Subscription](#subscription)
    - [Type-based Subscription](#type-based-subscription)
  + [Agent and Agent Runtime](#document-user-guide/core-user-guide/framework/agent-and-agent-runtime)
    - [Implementing an Agent](#implementing-an-agent)
    - [Using an AgentChat Agent](#using-an-agentchat-agent)
    - [Registering Agent Type](#registering-agent-type)
    - [Running the Single-Threaded Agent Runtime](#running-the-single-threaded-agent-runtime)
  + [Message and Communication](#document-user-guide/core-user-guide/framework/message-and-communication)
    - [Messages](#messages)
    - [Message Handlers](#message-handlers)
    - [Direct Messaging](#direct-messaging)
    - [Broadcast](#broadcast)
  + [Logging](#document-user-guide/core-user-guide/framework/logging)
    - [Enabling logging output](#enabling-logging-output)
    - [Emitting logs](#emitting-logs)
  + [Open Telemetry](#document-user-guide/core-user-guide/framework/telemetry)
    - [Instrumenting your application](#instrumenting-your-application)
    - [Clean instrumentation](#clean-instrumentation)
  + [Distributed Agent Runtime](#document-user-guide/core-user-guide/framework/distributed-agent-runtime)
  + [Cross-Language Runtimes](#cross-language-runtimes)
  + [Next Steps](#next-steps)
  + [Component config](#document-user-guide/core-user-guide/framework/component-config)
    - [How does this differ from state?](#how-does-this-differ-from-state)
    - [Usage](#usage)
    - [Creating a component class](#creating-a-component-class)
    - [Secrets](#secrets)
  + [Model Clients](#document-user-guide/core-user-guide/components/model-clients)
    - [Log Model Calls](#log-model-calls)
    - [Call Model Client](#call-model-client)
    - [Streaming Tokens](#streaming-tokens)
    - [Structured Output](#structured-output)
    - [Caching Model Responses](#caching-model-responses)
    - [Build an Agent with a Model Client](#build-an-agent-with-a-model-client)
    - [API Keys From Environment Variables](#api-keys-from-environment-variables)
  + [Model Context](#document-user-guide/core-user-guide/components/model-context)
  + [Tools](#document-user-guide/core-user-guide/components/tools)
    - [Built-in Tools](#built-in-tools)
    - [Custom Function Tools](#custom-function-tools)
    - [Calling Tools with Model Clients](#calling-tools-with-model-clients)
    - [Tool-Equipped Agent](#tool-equipped-agent)
  + [Command Line Code Executors](#document-user-guide/core-user-guide/components/command-line-code-executors)
    - [Docker](#docker)
    - [Local](#local)
    - [Local within a Virtual Environment](#local-within-a-virtual-environment)
  + [Intro](#document-user-guide/core-user-guide/design-patterns/intro)
  + [Concurrent Agents](#document-user-guide/core-user-guide/design-patterns/concurrent-agents)
    - [Single Message & Multiple Processors](#single-message-multiple-processors)
    - [Multiple messages & Multiple Processors](#multiple-messages-multiple-processors)
    - [Direct Messages](#direct-messages)
    - [Additional Resources](#additional-resources)
  + [Sequential Workflow](#document-user-guide/core-user-guide/design-patterns/sequential-workflow)
    - [Message Protocol](#message-protocol)
    - [Topics](#topics)
    - [Agents](#agents)
    - [Workflow](#workflow)
    - [Run the Workflow](#run-the-workflow)
  + [Group Chat](#document-user-guide/core-user-guide/design-patterns/group-chat)
    - [Message Protocol](#message-protocol)
    - [Base Group Chat Agent](#base-group-chat-agent)
    - [Writer and Editor Agents](#writer-and-editor-agents)
    - [Illustrator Agent with Image Generation](#illustrator-agent-with-image-generation)
    - [User Agent](#user-agent)
    - [Group Chat Manager](#group-chat-manager)
    - [Creating the Group Chat](#creating-the-group-chat)
    - [Running the Group Chat](#running-the-group-chat)
    - [Next Steps](#next-steps)
  + [Handoffs](#document-user-guide/core-user-guide/design-patterns/handoffs)
    - [Scenario](#scenario)
    - [Message Protocol](#message-protocol)
    - [AI Agent](#ai-agent)
    - [Human Agent](#human-agent)
    - [User Agent](#user-agent)
    - [Tools for the AI agents](#tools-for-the-ai-agents)
    - [Topic types for the agents](#topic-types-for-the-agents)
    - [Delegate tools for the AI agents](#delegate-tools-for-the-ai-agents)
    - [Creating the team](#creating-the-team)
    - [Running the team](#running-the-team)
    - [Next steps](#next-steps)
  + [Mixture of Agents](#document-user-guide/core-user-guide/design-patterns/mixture-of-agents)
    - [Message Protocol](#message-protocol)
    - [Worker Agent](#worker-agent)
    - [Orchestrator Agent](#orchestrator-agent)
    - [Running Mixture of Agents](#running-mixture-of-agents)
  + [Multi-Agent Debate](#document-user-guide/core-user-guide/design-patterns/multi-agent-debate)
    - [Message Protocol](#message-protocol)
    - [Solver Agent](#solver-agent)
    - [Aggregator Agent](#aggregator-agent)
    - [Setting Up a Debate](#setting-up-a-debate)
    - [Solving Math Problems](#solving-math-problems)
  + [Reflection](#document-user-guide/core-user-guide/design-patterns/reflection)
    - [Message Protocol](#message-protocol)
    - [Agents](#agents)
    - [Logging](#logging)
    - [Running the Design Pattern](#running-the-design-pattern)
  + [Code Execution](#document-user-guide/core-user-guide/design-patterns/code-execution-groupchat)
  + [Cookbook](#document-user-guide/core-user-guide/cookbook/index)
    - [List of recipes](#list-of-recipes)
  + [FAQs](#document-user-guide/core-user-guide/faqs)
    - [How do I get the underlying agent instance?](#how-do-i-get-the-underlying-agent-instance)
    - [How do I call call a function on an agent?](#how-do-i-call-call-a-function-on-an-agent)
    - [Why do I need to use a factory to register an agent?](#why-do-i-need-to-use-a-factory-to-register-an-agent)
    - [How do I increase the GRPC message size?](#how-do-i-increase-the-grpc-message-size)
    - [What are model capabilities and how do I specify them?](#what-are-model-capabilities-and-how-do-i-specify-them)
* [Extensions](#document-user-guide/extensions-user-guide/index)
  + [Installation](#document-user-guide/extensions-user-guide/installation)
  + [Discover community projects](#document-user-guide/extensions-user-guide/discover)
    - [List of community projects](#list-of-community-projects)
  + [Creating your own extension](#document-user-guide/extensions-user-guide/create-your-own)
    - [Best practices](#best-practices)
    - [Discovery](#discovery)
    - [Changes from 0.2](#changes-from-0-2)
  + [ACA Dynamic Sessions Code Executor](#document-user-guide/extensions-user-guide/azure-container-code-executor)
    - [Create a Container Apps Session Pool](#create-a-container-apps-session-pool)
    - [ACADynamicSessionsCodeExecutor](#acadynamicsessionscodeexecutor)
* [Studio](#document-user-guide/autogenstudio-user-guide/index)
  + [Capabilities - What Can You Do with AutoGen Studio?](#capabilities-what-can-you-do-with-autogen-studio)
    - [Roadmap](#roadmap)
  + [Contribution Guide](#contribution-guide)
  + [A Note on Security](#a-note-on-security)
  + [Acknowledgements and Citation](#acknowledgements-and-citation)
  + [Next Steps](#next-steps)
    - [Installation](#document-user-guide/autogenstudio-user-guide/installation)
    - [Usage](#document-user-guide/autogenstudio-user-guide/usage)
    - [Experimental Features](#document-user-guide/autogenstudio-user-guide/experimental)
    - [FAQ](#document-user-guide/autogenstudio-user-guide/faq)
* [API Reference](#document-reference/index)
  + [autogen\_agentchat](#document-reference/python/autogen_agentchat)
    - [`EVENT_LOGGER_NAME`](#autogen_agentchat.EVENT_LOGGER_NAME)
    - [`TRACE_LOGGER_NAME`](#autogen_agentchat.TRACE_LOGGER_NAME)
  + [autogen\_agentchat.messages](#document-reference/python/autogen_agentchat.messages)
    - [`AgentEvent`](#autogen_agentchat.messages.AgentEvent)
    - [`BaseMessage`](#autogen_agentchat.messages.BaseMessage)
    - [`ChatMessage`](#autogen_agentchat.messages.ChatMessage)
    - [`HandoffMessage`](#autogen_agentchat.messages.HandoffMessage)
    - [`MemoryQueryEvent`](#autogen_agentchat.messages.MemoryQueryEvent)
    - [`ModelClientStreamingChunkEvent`](#autogen_agentchat.messages.ModelClientStreamingChunkEvent)
    - [`MultiModalMessage`](#autogen_agentchat.messages.MultiModalMessage)
    - [`StopMessage`](#autogen_agentchat.messages.StopMessage)
    - [`TextMessage`](#autogen_agentchat.messages.TextMessage)
    - [`ThoughtEvent`](#autogen_agentchat.messages.ThoughtEvent)
    - [`ToolCallExecutionEvent`](#autogen_agentchat.messages.ToolCallExecutionEvent)
    - [`ToolCallRequestEvent`](#autogen_agentchat.messages.ToolCallRequestEvent)
    - [`ToolCallSummaryMessage`](#autogen_agentchat.messages.ToolCallSummaryMessage)
    - [`UserInputRequestedEvent`](#autogen_agentchat.messages.UserInputRequestedEvent)
  + [autogen\_agentchat.agents](#document-reference/python/autogen_agentchat.agents)
    - [`AssistantAgent`](#autogen_agentchat.agents.AssistantAgent)
    - [`BaseChatAgent`](#autogen_agentchat.agents.BaseChatAgent)
    - [`CodeExecutorAgent`](#autogen_agentchat.agents.CodeExecutorAgent)
    - [`SocietyOfMindAgent`](#autogen_agentchat.agents.SocietyOfMindAgent)
    - [`UserProxyAgent`](#autogen_agentchat.agents.UserProxyAgent)
  + [autogen\_agentchat.teams](#document-reference/python/autogen_agentchat.teams)
    - [`BaseGroupChat`](#autogen_agentchat.teams.BaseGroupChat)
    - [`MagenticOneGroupChat`](#autogen_agentchat.teams.MagenticOneGroupChat)
    - [`RoundRobinGroupChat`](#autogen_agentchat.teams.RoundRobinGroupChat)
    - [`SelectorGroupChat`](#autogen_agentchat.teams.SelectorGroupChat)
    - [`Swarm`](#autogen_agentchat.teams.Swarm)
  + [autogen\_agentchat.base](#document-reference/python/autogen_agentchat.base)
    - [`AndTerminationCondition`](#autogen_agentchat.base.AndTerminationCondition)
    - [`ChatAgent`](#autogen_agentchat.base.ChatAgent)
    - [`Handoff`](#autogen_agentchat.base.Handoff)
    - [`OrTerminationCondition`](#autogen_agentchat.base.OrTerminationCondition)
    - [`Response`](#autogen_agentchat.base.Response)
    - [`TaskResult`](#autogen_agentchat.base.TaskResult)
    - [`TaskRunner`](#autogen_agentchat.base.TaskRunner)
    - [`Team`](#autogen_agentchat.base.Team)
    - [`TerminatedException`](#autogen_agentchat.base.TerminatedException)
    - [`TerminationCondition`](#autogen_agentchat.base.TerminationCondition)
  + [autogen\_agentchat.conditions](#document-reference/python/autogen_agentchat.conditions)
    - [`ExternalTermination`](#autogen_agentchat.conditions.ExternalTermination)
    - [`FunctionCallTermination`](#autogen_agentchat.conditions.FunctionCallTermination)
    - [`HandoffTermination`](#autogen_agentchat.conditions.HandoffTermination)
    - [`MaxMessageTermination`](#autogen_agentchat.conditions.MaxMessageTermination)
    - [`SourceMatchTermination`](#autogen_agentchat.conditions.SourceMatchTermination)
    - [`StopMessageTermination`](#autogen_agentchat.conditions.StopMessageTermination)
    - [`TextMentionTermination`](#autogen_agentchat.conditions.TextMentionTermination)
    - [`TextMessageTermination`](#autogen_agentchat.conditions.TextMessageTermination)
    - [`TimeoutTermination`](#autogen_agentchat.conditions.TimeoutTermination)
    - [`TokenUsageTermination`](#autogen_agentchat.conditions.TokenUsageTermination)
  + [autogen\_agentchat.ui](#document-reference/python/autogen_agentchat.ui)
    - [`Console()`](#autogen_agentchat.ui.Console)
    - [`UserInputManager`](#autogen_agentchat.ui.UserInputManager)
  + [autogen\_agentchat.state](#document-reference/python/autogen_agentchat.state)
    - [`AssistantAgentState`](#autogen_agentchat.state.AssistantAgentState)
    - [`BaseGroupChatManagerState`](#autogen_agentchat.state.BaseGroupChatManagerState)
    - [`BaseState`](#autogen_agentchat.state.BaseState)
    - [`ChatAgentContainerState`](#autogen_agentchat.state.ChatAgentContainerState)
    - [`MagenticOneOrchestratorState`](#autogen_agentchat.state.MagenticOneOrchestratorState)
    - [`RoundRobinManagerState`](#autogen_agentchat.state.RoundRobinManagerState)
    - [`SelectorManagerState`](#autogen_agentchat.state.SelectorManagerState)
    - [`SocietyOfMindAgentState`](#autogen_agentchat.state.SocietyOfMindAgentState)
    - [`SwarmManagerState`](#autogen_agentchat.state.SwarmManagerState)
    - [`TeamState`](#autogen_agentchat.state.TeamState)
  + [autogen\_core](#document-reference/python/autogen_core)
    - [`Agent`](#autogen_core.Agent)
    - [`AgentId`](#autogen_core.AgentId)
    - [`AgentProxy`](#autogen_core.AgentProxy)
    - [`AgentMetadata`](#autogen_core.AgentMetadata)
    - [`AgentRuntime`](#autogen_core.AgentRuntime)
    - [`BaseAgent`](#autogen_core.BaseAgent)
    - [`CacheStore`](#autogen_core.CacheStore)
    - [`InMemoryStore`](#autogen_core.InMemoryStore)
    - [`CancellationToken`](#autogen_core.CancellationToken)
    - [`AgentInstantiationContext`](#autogen_core.AgentInstantiationContext)
    - [`TopicId`](#autogen_core.TopicId)
    - [`Subscription`](#autogen_core.Subscription)
    - [`MessageContext`](#autogen_core.MessageContext)
    - [`AgentType`](#autogen_core.AgentType)
    - [`SubscriptionInstantiationContext`](#autogen_core.SubscriptionInstantiationContext)
    - [`MessageHandlerContext`](#autogen_core.MessageHandlerContext)
    - [`MessageSerializer`](#autogen_core.MessageSerializer)
    - [`UnknownPayload`](#autogen_core.UnknownPayload)
    - [`Image`](#autogen_core.Image)
    - [`RoutedAgent`](#autogen_core.RoutedAgent)
    - [`ClosureAgent`](#autogen_core.ClosureAgent)
    - [`ClosureContext`](#autogen_core.ClosureContext)
    - [`message_handler()`](#autogen_core.message_handler)
    - [`event()`](#autogen_core.event)
    - [`rpc()`](#autogen_core.rpc)
    - [`FunctionCall`](#autogen_core.FunctionCall)
    - [`TypeSubscription`](#autogen_core.TypeSubscription)
    - [`DefaultSubscription`](#autogen_core.DefaultSubscription)
    - [`DefaultTopicId`](#autogen_core.DefaultTopicId)
    - [`default_subscription()`](#autogen_core.default_subscription)
    - [`type_subscription()`](#autogen_core.type_subscription)
    - [`TypePrefixSubscription`](#autogen_core.TypePrefixSubscription)
    - [`JSON_DATA_CONTENT_TYPE`](#autogen_core.JSON_DATA_CONTENT_TYPE)
    - [`PROTOBUF_DATA_CONTENT_TYPE`](#autogen_core.PROTOBUF_DATA_CONTENT_TYPE)
    - [`SingleThreadedAgentRuntime`](#autogen_core.SingleThreadedAgentRuntime)
    - [`ROOT_LOGGER_NAME`](#autogen_core.ROOT_LOGGER_NAME)
    - [`EVENT_LOGGER_NAME`](#autogen_core.EVENT_LOGGER_NAME)
    - [`TRACE_LOGGER_NAME`](#autogen_core.TRACE_LOGGER_NAME)
    - [`Component`](#autogen_core.Component)
    - [`ComponentBase`](#autogen_core.ComponentBase)
    - [`ComponentFromConfig`](#autogen_core.ComponentFromConfig)
    - [`ComponentLoader`](#autogen_core.ComponentLoader)
    - [`ComponentModel`](#autogen_core.ComponentModel)
    - [`ComponentSchemaType`](#autogen_core.ComponentSchemaType)
    - [`ComponentToConfig`](#autogen_core.ComponentToConfig)
    - [`is_component_class()`](#autogen_core.is_component_class)
    - [`is_component_instance()`](#autogen_core.is_component_instance)
    - [`DropMessage`](#autogen_core.DropMessage)
    - [`InterventionHandler`](#autogen_core.InterventionHandler)
    - [`DefaultInterventionHandler`](#autogen_core.DefaultInterventionHandler)
    - [`ComponentType`](#autogen_core.ComponentType)
  + [autogen\_core.code\_executor](#document-reference/python/autogen_core.code_executor)
    - [`Alias`](#autogen_core.code_executor.Alias)
    - [`CodeBlock`](#autogen_core.code_executor.CodeBlock)
    - [`CodeExecutor`](#autogen_core.code_executor.CodeExecutor)
    - [`CodeResult`](#autogen_core.code_executor.CodeResult)
    - [`FunctionWithRequirements`](#autogen_core.code_executor.FunctionWithRequirements)
    - [`FunctionWithRequirementsStr`](#autogen_core.code_executor.FunctionWithRequirementsStr)
    - [`ImportFromModule`](#autogen_core.code_executor.ImportFromModule)
    - [`with_requirements()`](#autogen_core.code_executor.with_requirements)
  + [autogen\_core.models](#document-reference/python/autogen_core.models)
    - [`AssistantMessage`](#autogen_core.models.AssistantMessage)
    - [`ChatCompletionClient`](#autogen_core.models.ChatCompletionClient)
    - [`ChatCompletionTokenLogprob`](#autogen_core.models.ChatCompletionTokenLogprob)
    - [`CreateResult`](#autogen_core.models.CreateResult)
    - [`FunctionExecutionResult`](#autogen_core.models.FunctionExecutionResult)
    - [`FunctionExecutionResultMessage`](#autogen_core.models.FunctionExecutionResultMessage)
    - [`ModelCapabilities`](#autogen_core.models.ModelCapabilities)
    - [`ModelFamily`](#autogen_core.models.ModelFamily)
    - [`ModelInfo`](#autogen_core.models.ModelInfo)
    - [`RequestUsage`](#autogen_core.models.RequestUsage)
    - [`SystemMessage`](#autogen_core.models.SystemMessage)
    - [`TopLogprob`](#autogen_core.models.TopLogprob)
    - [`UserMessage`](#autogen_core.models.UserMessage)
    - [`validate_model_info()`](#autogen_core.models.validate_model_info)
  + [autogen\_core.model\_context](#document-reference/python/autogen_core.model_context)
    - [`BufferedChatCompletionContext`](#autogen_core.model_context.BufferedChatCompletionContext)
    - [`ChatCompletionContext`](#autogen_core.model_context.ChatCompletionContext)
    - [`ChatCompletionContextState`](#autogen_core.model_context.ChatCompletionContextState)
    - [`HeadAndTailChatCompletionContext`](#autogen_core.model_context.HeadAndTailChatCompletionContext)
    - [`UnboundedChatCompletionContext`](#autogen_core.model_context.UnboundedChatCompletionContext)
  + [autogen\_core.tools](#document-reference/python/autogen_core.tools)
    - [`BaseTool`](#autogen_core.tools.BaseTool)
    - [`BaseToolWithState`](#autogen_core.tools.BaseToolWithState)
    - [`FunctionTool`](#autogen_core.tools.FunctionTool)
    - [`ParametersSchema`](#autogen_core.tools.ParametersSchema)
    - [`Tool`](#autogen_core.tools.Tool)
    - [`ToolSchema`](#autogen_core.tools.ToolSchema)
  + [autogen\_core.tool\_agent](#document-reference/python/autogen_core.tool_agent)
    - [`InvalidToolArgumentsException`](#autogen_core.tool_agent.InvalidToolArgumentsException)
    - [`ToolAgent`](#autogen_core.tool_agent.ToolAgent)
    - [`ToolException`](#autogen_core.tool_agent.ToolException)
    - [`ToolExecutionException`](#autogen_core.tool_agent.ToolExecutionException)
    - [`ToolNotFoundException`](#autogen_core.tool_agent.ToolNotFoundException)
    - [`tool_agent_caller_loop()`](#autogen_core.tool_agent.tool_agent_caller_loop)
  + [autogen\_core.memory](#document-reference/python/autogen_core.memory)
    - [`ListMemory`](#autogen_core.memory.ListMemory)
    - [`Memory`](#autogen_core.memory.Memory)
    - [`MemoryContent`](#autogen_core.memory.MemoryContent)
    - [`MemoryMimeType`](#autogen_core.memory.MemoryMimeType)
    - [`MemoryQueryResult`](#autogen_core.memory.MemoryQueryResult)
    - [`UpdateContextResult`](#autogen_core.memory.UpdateContextResult)
  + [autogen\_core.exceptions](#document-reference/python/autogen_core.exceptions)
    - [`CantHandleException`](#autogen_core.exceptions.CantHandleException)
    - [`MessageDroppedException`](#autogen_core.exceptions.MessageDroppedException)
    - [`NotAccessibleError`](#autogen_core.exceptions.NotAccessibleError)
    - [`UndeliverableException`](#autogen_core.exceptions.UndeliverableException)
  + [autogen\_core.logging](#document-reference/python/autogen_core.logging)
    - [`AgentConstructionExceptionEvent`](#autogen_core.logging.AgentConstructionExceptionEvent)
    - [`DeliveryStage`](#autogen_core.logging.DeliveryStage)
    - [`LLMCallEvent`](#autogen_core.logging.LLMCallEvent)
    - [`LLMStreamEndEvent`](#autogen_core.logging.LLMStreamEndEvent)
    - [`LLMStreamStartEvent`](#autogen_core.logging.LLMStreamStartEvent)
    - [`MessageDroppedEvent`](#autogen_core.logging.MessageDroppedEvent)
    - [`MessageEvent`](#autogen_core.logging.MessageEvent)
    - [`MessageHandlerExceptionEvent`](#autogen_core.logging.MessageHandlerExceptionEvent)
    - [`MessageKind`](#autogen_core.logging.MessageKind)
    - [`ToolCallEvent`](#autogen_core.logging.ToolCallEvent)
  + [autogen\_ext.agents.magentic\_one](#document-reference/python/autogen_ext.agents.magentic_one)
    - [`MagenticOneCoderAgent`](#autogen_ext.agents.magentic_one.MagenticOneCoderAgent)
  + [autogen\_ext.agents.openai](#document-reference/python/autogen_ext.agents.openai)
    - [`OpenAIAssistantAgent`](#autogen_ext.agents.openai.OpenAIAssistantAgent)
  + [autogen\_ext.agents.web\_surfer](#document-reference/python/autogen_ext.agents.web_surfer)
    - [`MultimodalWebSurfer`](#autogen_ext.agents.web_surfer.MultimodalWebSurfer)
    - [`PlaywrightController`](#autogen_ext.agents.web_surfer.PlaywrightController)
  + [autogen\_ext.agents.file\_surfer](#document-reference/python/autogen_ext.agents.file_surfer)
    - [`FileSurfer`](#autogen_ext.agents.file_surfer.FileSurfer)
  + [autogen\_ext.agents.video\_surfer](#document-reference/python/autogen_ext.agents.video_surfer)
    - [`VideoSurfer`](#autogen_ext.agents.video_surfer.VideoSurfer)
  + [autogen\_ext.agents.video\_surfer.tools](#document-reference/python/autogen_ext.agents.video_surfer.tools)
    - [`extract_audio()`](#autogen_ext.agents.video_surfer.tools.extract_audio)
    - [`get_screenshot_at()`](#autogen_ext.agents.video_surfer.tools.get_screenshot_at)
    - [`get_video_length()`](#autogen_ext.agents.video_surfer.tools.get_video_length)
    - [`save_screenshot()`](#autogen_ext.agents.video_surfer.tools.save_screenshot)
    - [`transcribe_audio_with_timestamps()`](#autogen_ext.agents.video_surfer.tools.transcribe_audio_with_timestamps)
    - [`transcribe_video_screenshot()`](#autogen_ext.agents.video_surfer.tools.transcribe_video_screenshot)
  + [autogen\_ext.teams.magentic\_one](#document-reference/python/autogen_ext.teams.magentic_one)
    - [`MagenticOne`](#autogen_ext.teams.magentic_one.MagenticOne)
  + [autogen\_ext.models.cache](#document-reference/python/autogen_ext.models.cache)
    - [`ChatCompletionCache`](#autogen_ext.models.cache.ChatCompletionCache)
  + [autogen\_ext.models.openai](#document-reference/python/autogen_ext.models.openai)
    - [`OpenAIChatCompletionClient`](#autogen_ext.models.openai.OpenAIChatCompletionClient)
    - [`AzureOpenAIChatCompletionClient`](#autogen_ext.models.openai.AzureOpenAIChatCompletionClient)
    - [`BaseOpenAIChatCompletionClient`](#autogen_ext.models.openai.BaseOpenAIChatCompletionClient)
    - [`AzureOpenAIClientConfigurationConfigModel`](#autogen_ext.models.openai.AzureOpenAIClientConfigurationConfigModel)
    - [`OpenAIClientConfigurationConfigModel`](#autogen_ext.models.openai.OpenAIClientConfigurationConfigModel)
    - [`BaseOpenAIClientConfigurationConfigModel`](#autogen_ext.models.openai.BaseOpenAIClientConfigurationConfigModel)
    - [`CreateArgumentsConfigModel`](#autogen_ext.models.openai.CreateArgumentsConfigModel)
  + [autogen\_ext.models.replay](#document-reference/python/autogen_ext.models.replay)
    - [`ReplayChatCompletionClient`](#autogen_ext.models.replay.ReplayChatCompletionClient)
  + [autogen\_ext.models.azure](#document-reference/python/autogen_ext.models.azure)
    - [`AzureAIChatCompletionClient`](#autogen_ext.models.azure.AzureAIChatCompletionClient)
    - [`AzureAIChatCompletionClientConfig`](#autogen_ext.models.azure.AzureAIChatCompletionClientConfig)
  + [autogen\_ext.models.anthropic](#document-reference/python/autogen_ext.models.anthropic)
    - [`AnthropicChatCompletionClient`](#autogen_ext.models.anthropic.AnthropicChatCompletionClient)
    - [`AnthropicClientConfiguration`](#autogen_ext.models.anthropic.AnthropicClientConfiguration)
    - [`AnthropicClientConfigurationConfigModel`](#autogen_ext.models.anthropic.AnthropicClientConfigurationConfigModel)
    - [`BaseAnthropicChatCompletionClient`](#autogen_ext.models.anthropic.BaseAnthropicChatCompletionClient)
    - [`CreateArgumentsConfigModel`](#autogen_ext.models.anthropic.CreateArgumentsConfigModel)
  + [autogen\_ext.models.semantic\_kernel](#document-reference/python/autogen_ext.models.semantic_kernel)
    - [`SKChatCompletionAdapter`](#autogen_ext.models.semantic_kernel.SKChatCompletionAdapter)
  + [autogen\_ext.models.ollama](#document-reference/python/autogen_ext.models.ollama)
    - [`BaseOllamaClientConfigurationConfigModel`](#autogen_ext.models.ollama.BaseOllamaClientConfigurationConfigModel)
    - [`CreateArgumentsConfigModel`](#autogen_ext.models.ollama.CreateArgumentsConfigModel)
    - [`OllamaChatCompletionClient`](#autogen_ext.models.ollama.OllamaChatCompletionClient)
  + [autogen\_ext.models.llama\_cpp](#document-reference/python/autogen_ext.models.llama_cpp)
    - [`LlamaCppChatCompletionClient`](#autogen_ext.models.llama_cpp.LlamaCppChatCompletionClient)
  + [autogen\_ext.tools.code\_execution](#document-reference/python/autogen_ext.tools.code_execution)
    - [`CodeExecutionInput`](#autogen_ext.tools.code_execution.CodeExecutionInput)
    - [`CodeExecutionResult`](#autogen_ext.tools.code_execution.CodeExecutionResult)
    - [`PythonCodeExecutionTool`](#autogen_ext.tools.code_execution.PythonCodeExecutionTool)
  + [autogen\_ext.tools.graphrag](#document-reference/python/autogen_ext.tools.graphrag)
    - [`GlobalContextConfig`](#autogen_ext.tools.graphrag.GlobalContextConfig)
    - [`GlobalDataConfig`](#autogen_ext.tools.graphrag.GlobalDataConfig)
    - [`GlobalSearchTool`](#autogen_ext.tools.graphrag.GlobalSearchTool)
    - [`GlobalSearchToolArgs`](#autogen_ext.tools.graphrag.GlobalSearchToolArgs)
    - [`GlobalSearchToolReturn`](#autogen_ext.tools.graphrag.GlobalSearchToolReturn)
    - [`LocalContextConfig`](#autogen_ext.tools.graphrag.LocalContextConfig)
    - [`LocalDataConfig`](#autogen_ext.tools.graphrag.LocalDataConfig)
    - [`LocalSearchTool`](#autogen_ext.tools.graphrag.LocalSearchTool)
    - [`LocalSearchToolArgs`](#autogen_ext.tools.graphrag.LocalSearchToolArgs)
    - [`LocalSearchToolReturn`](#autogen_ext.tools.graphrag.LocalSearchToolReturn)
    - [`MapReduceConfig`](#autogen_ext.tools.graphrag.MapReduceConfig)
    - [`SearchConfig`](#autogen_ext.tools.graphrag.SearchConfig)
  + [autogen\_ext.tools.http](#document-reference/python/autogen_ext.tools.http)
    - [`HttpTool`](#autogen_ext.tools.http.HttpTool)
  + [autogen\_ext.tools.langchain](#document-reference/python/autogen_ext.tools.langchain)
    - [`LangChainToolAdapter`](#autogen_ext.tools.langchain.LangChainToolAdapter)
  + [autogen\_ext.tools.mcp](#document-reference/python/autogen_ext.tools.mcp)
    - [`SseMcpToolAdapter`](#autogen_ext.tools.mcp.SseMcpToolAdapter)
    - [`SseServerParams`](#autogen_ext.tools.mcp.SseServerParams)
    - [`StdioMcpToolAdapter`](#autogen_ext.tools.mcp.StdioMcpToolAdapter)
    - [`StdioServerParams`](#autogen_ext.tools.mcp.StdioServerParams)
    - [`mcp_server_tools()`](#autogen_ext.tools.mcp.mcp_server_tools)
  + [autogen\_ext.tools.semantic\_kernel](#document-reference/python/autogen_ext.tools.semantic_kernel)
    - [`KernelFunctionFromTool`](#autogen_ext.tools.semantic_kernel.KernelFunctionFromTool)
  + [autogen\_ext.code\_executors.local](#document-reference/python/autogen_ext.code_executors.local)
    - [`LocalCommandLineCodeExecutor`](#autogen_ext.code_executors.local.LocalCommandLineCodeExecutor)
  + [autogen\_ext.code\_executors.docker](#document-reference/python/autogen_ext.code_executors.docker)
    - [`DockerCommandLineCodeExecutor`](#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor)
  + [autogen\_ext.code\_executors.jupyter](#document-reference/python/autogen_ext.code_executors.jupyter)
    - [`JupyterCodeExecutor`](#autogen_ext.code_executors.jupyter.JupyterCodeExecutor)
    - [`JupyterCodeResult`](#autogen_ext.code_executors.jupyter.JupyterCodeResult)
  + [autogen\_ext.code\_executors.azure](#document-reference/python/autogen_ext.code_executors.azure)
    - [`ACADynamicSessionsCodeExecutor`](#autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor)
    - [`TokenProvider`](#autogen_ext.code_executors.azure.TokenProvider)
  + [autogen\_ext.cache\_store.diskcache](#document-reference/python/autogen_ext.cache_store.diskcache)
    - [`DiskCacheStore`](#autogen_ext.cache_store.diskcache.DiskCacheStore)
    - [`DiskCacheStoreConfig`](#autogen_ext.cache_store.diskcache.DiskCacheStoreConfig)
  + [autogen\_ext.cache\_store.redis](#document-reference/python/autogen_ext.cache_store.redis)
    - [`RedisStore`](#autogen_ext.cache_store.redis.RedisStore)
    - [`RedisStoreConfig`](#autogen_ext.cache_store.redis.RedisStoreConfig)
  + [autogen\_ext.runtimes.grpc](#document-reference/python/autogen_ext.runtimes.grpc)
    - [`GrpcWorkerAgentRuntime`](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntime)
    - [`GrpcWorkerAgentRuntimeHost`](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHost)
    - [`GrpcWorkerAgentRuntimeHostServicer`](#autogen_ext.runtimes.grpc.GrpcWorkerAgentRuntimeHostServicer)
  + [autogen\_ext.auth.azure](#document-reference/python/autogen_ext.auth.azure)
    - [`AzureTokenProvider`](#autogen_ext.auth.azure.AzureTokenProvider)
    - [`TokenProviderConfig`](#autogen_ext.auth.azure.TokenProviderConfig)
  + [autogen\_ext.experimental.task\_centric\_memory](#document-reference/python/autogen_ext.experimental.task_centric_memory)
    - [`MemoryController`](#autogen_ext.experimental.task_centric_memory.MemoryController)
  + [autogen\_ext.experimental.task\_centric\_memory.utils](#document-reference/python/autogen_ext.experimental.task_centric_memory.utils)
    - [`Apprentice`](#autogen_ext.experimental.task_centric_memory.utils.Apprentice)
    - [`ChatCompletionClientRecorder`](#autogen_ext.experimental.task_centric_memory.utils.ChatCompletionClientRecorder)
    - [`Grader`](#autogen_ext.experimental.task_centric_memory.utils.Grader)
    - [`PageLogger`](#autogen_ext.experimental.task_centric_memory.utils.PageLogger)
    - [`Teachability`](#autogen_ext.experimental.task_centric_memory.utils.Teachability)

© Copyright 2024, Microsoft.

[Privacy Policy](https://go.microsoft.com/fwlink/?LinkId=521839) | [Consumer Health Privacy](https://go.microsoft.com/fwlink/?linkid=2259814)

Built with the [PyData Sphinx Theme](https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html) 0.16.0.