"""Vulnerability research workflow for the Skwaq vulnerability assessment copilot.

This module implements the comprehensive vulnerability research workflow, integrating
with code analysis, knowledge retrieval, and tool invocation to provide a complete
vulnerability assessment.
"""

from typing import Dict, List, Any, Optional, AsyncGenerator, Set, TypeVar, Union
import asyncio
import json
import os
import uuid
import tempfile
from datetime import datetime
from pathlib import Path

import autogen_core as autogen
from skwaq.agents.vulnerability_research_agent import VulnerabilityResearchAgent
from skwaq.agents.skwaq_agent import SkwaqAgent
from skwaq.agents.vulnerability_events import VulnerabilityDiscoveryEvent
from skwaq.code_analysis.analyzer import CodeAnalyzer
from skwaq.shared.finding import Finding

from .base import Workflow
from ..db.neo4j_connector import get_connector
from ..utils.logging import get_logger

logger = get_logger(__name__)

# Default security focus areas for vulnerability research
DEFAULT_FOCUS_AREAS = [
    "Injection",
    "Authentication",
    "Authorization",
    "Sensitive Data Exposure",
    "Security Misconfiguration",
    "Input Validation",
    "Error Handling",
    "Cryptographic Issues",
]

T = TypeVar('T')


class VulnerabilityResearchAgent(SkwaqAgent):
    """Enhanced agent for vulnerability research workflow.
    
    This agent extends the SkwaqAgent to provide specialized capabilities
    for vulnerability research, including code analysis, knowledge retrieval,
    and vulnerability verification.
    """
    
    def __init__(
        self, 
        name: str = "VulnerabilityResearchAgent",
        system_message: Optional[str] = None,
        **kwargs
    ):
        """Initialize the vulnerability research agent.
        
        Args:
            name: The name of the agent
            system_message: Optional system message for the agent
            **kwargs: Additional keyword arguments
        """
        if system_message is None:
            system_message = """You are a specialized agent for vulnerability research.
Your goal is to identify security vulnerabilities in code by analyzing code patterns,
semantics, and context. You should focus on identifying common vulnerability types 
including injection flaws, authentication issues, authorization problems, data exposure,
insecure configurations, and cryptographic weaknesses.

For each piece of code you analyze:
1. Look for specific vulnerability patterns
2. Consider the context and programming language
3. Assess the severity and confidence of your findings
4. Provide detailed remediation guidance
5. Reference CWE IDs when applicable"""

        super().__init__(
            name=name,
            system_message=system_message,
            description="Specialized agent for vulnerability research and assessment",
            **kwargs
        )
        
        # Set up the standard vulnerability research agent
        from ..agents.vulnerability_research_agent import VulnerabilityResearchAgent as BasicVRAgent
        self.basic_agent = BasicVRAgent()
        
    async def analyze_code(
        self,
        code: str,
        context: Dict[str, Any],
        vulnerability_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """Analyze code for potential vulnerabilities.
        
        Args:
            code: The code to analyze
            context: Additional context for the analysis
            vulnerability_type: Optional specific vulnerability type to look for
            
        Returns:
            Dictionary with analysis results
        """
        # First, use the basic pattern-based agent for quick checks
        basic_results = await self.basic_agent.research_vulnerability(
            vulnerability_type=vulnerability_type or "General",
            evidence=code,
            context=context
        )
        
        # For higher confidence analysis, use LLM reasoning
        if not basic_results.get("confirmed", False) or basic_results.get("confidence", 0) < 0.7:
            # Build a prompt for more detailed analysis
            language = context.get("language", "Unknown")
            prompt = f"""Analyze the following {language} code for potential security vulnerabilities:

```{language}
{code[:2000]}  # Limit code size for LLM context
```

Focus on {vulnerability_type if vulnerability_type else "all types of"} vulnerabilities.

File context: {context.get('file_path', 'Unknown file')}

Provide your analysis in the following JSON format:
{{
  "vulnerability_found": true/false,
  "vulnerability_type": "Specific vulnerability type",
  "description": "Detailed description of the vulnerability",
  "severity": "Critical/High/Medium/Low",
  "confidence": 0.0-1.0,
  "line_numbers": [list of relevant line numbers],
  "cwe_id": "CWE-XXX",
  "remediation": "Detailed remediation advice"
}}
"""
            
            # Generate a response from the LLM
            response = await self.generate_response(prompt, temperature=0.3)
            
            # Try to parse the JSON response
            try:
                enhanced_results = json.loads(response)
                
                # Merge with basic results, prioritizing enhanced results
                if enhanced_results.get("vulnerability_found", False):
                    return {
                        "confirmed": enhanced_results["vulnerability_found"],
                        "vulnerability_type": enhanced_results.get("vulnerability_type"),
                        "description": enhanced_results.get("description"),
                        "severity": enhanced_results.get("severity", "Medium"),
                        "confidence": enhanced_results.get("confidence", 0.7),
                        "line_numbers": enhanced_results.get("line_numbers", []),
                        "cwe_id": enhanced_results.get("cwe_id"),
                        "remediation": enhanced_results.get("remediation"),
                        "context": context,
                        "evidence": code[:200] + "..." if len(code) > 200 else code,
                    }
            except json.JSONDecodeError:
                # Fall back to basic results if JSON parsing fails
                logger.warning("Failed to parse LLM response as JSON, falling back to basic analysis")
                pass
                
        return basic_results
        
    async def verify_vulnerability(
        self,
        finding: Dict[str, Any],
        additional_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Verify a potential vulnerability finding.
        
        Args:
            finding: The vulnerability finding to verify
            additional_context: Optional additional context
            
        Returns:
            Verification results
        """
        # Build a prompt for verification
        evidence = finding.get("evidence", "")
        vulnerability_type = finding.get("vulnerability_type", "Unknown")
        description = finding.get("description", "")
        
        prompt = f"""Verify the following potential security vulnerability:

Vulnerability Type: {vulnerability_type}
Description: {description}

Code Evidence:
```
{evidence}
```

Additional Context: {additional_context or {}}

Is this a real vulnerability or a false positive? Explain your reasoning in detail.
Provide your verification in the following JSON format:
{{
  "verified": true/false,
  "confidence": 0.0-1.0,
  "explanation": "Detailed explanation of your verification",
  "additional_concerns": "Any additional security concerns identified",
  "refined_remediation": "Any refined remediation guidance"
}}
"""
        
        # Generate a response from the LLM
        response = await self.generate_response(prompt, temperature=0.2)
        
        # Try to parse the JSON response
        try:
            verification = json.loads(response)
            return verification
        except json.JSONDecodeError:
            # Return a default verification if parsing fails
            logger.warning("Failed to parse verification response as JSON")
            return {
                "verified": finding.get("confirmed", False),
                "confidence": finding.get("confidence", 0.5),
                "explanation": "Automatic verification failed, using original assessment",
                "additional_concerns": None,
                "refined_remediation": finding.get("remediation", "")
            }


class InvestigationState:
    """State management class for persistent investigations.
    
    This class manages the serialization and deserialization of investigation
    state, allowing workflows to be paused and resumed across sessions.
    """
    
    def __init__(self, workflow_id: str, repository_id: str):
        """Initialize the investigation state.
        
        Args:
            workflow_id: The ID of the workflow
            repository_id: The ID of the repository being investigated
        """
        self.workflow_id = workflow_id
        self.repository_id = repository_id
        self.state_data: Dict[str, Any] = {
            "workflow_id": workflow_id,
            "repository_id": repository_id,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "current_phase": 0,
            "current_focus_area_index": 0,
            "findings": [],
            "completed_steps": set(),
            "metadata": {},
        }
        
    def update(self, **kwargs) -> None:
        """Update the investigation state.
        
        Args:
            **kwargs: Key-value pairs to update in the state
        """
        for key, value in kwargs.items():
            if key in self.state_data:
                self.state_data[key] = value
            elif key == "add_finding":
                self.state_data["findings"].append(value)
            elif key == "add_completed_step":
                self.state_data["completed_steps"].add(value)
        
        self.state_data["updated_at"] = datetime.now().isoformat()
        
    def save(self) -> bool:
        """Save the investigation state to the database.
        
        Returns:
            True if successful, False otherwise
        """
        connector = get_connector()
        if not connector:
            logger.error("Cannot save investigation state: database connection failed")
            return False
            
        # Convert sets to lists for JSON serialization
        serializable_state = self.state_data.copy()
        serializable_state["completed_steps"] = list(serializable_state["completed_steps"])
        
        # Check if an investigation state record already exists
        query = """
        MATCH (i:Investigation {workflow_id: $workflow_id})
        RETURN id(i) as id
        """
        
        result = connector.run_query(query, {"workflow_id": self.workflow_id})
        
        if result:
            # Update existing state
            node_id = result[0]["id"]
            connector.update_node(
                node_id,
                {"state": json.dumps(serializable_state), "updated_at": serializable_state["updated_at"]}
            )
            logger.debug(f"Updated investigation state for workflow {self.workflow_id}")
        else:
            # Create new state
            props = {
                "workflow_id": self.workflow_id,
                "repository_id": self.repository_id,
                "state": json.dumps(serializable_state),
                "created_at": serializable_state["created_at"],
                "updated_at": serializable_state["updated_at"],
            }
            
            connector.create_node("Investigation", props)
            logger.info(f"Created new investigation state for workflow {self.workflow_id}")
            
        return True
        
    @classmethod
    def load(cls, workflow_id: str) -> Optional['InvestigationState']:
        """Load an investigation state from the database.
        
        Args:
            workflow_id: The ID of the workflow
            
        Returns:
            The loaded investigation state, or None if not found
        """
        connector = get_connector()
        if not connector:
            logger.error("Cannot load investigation state: database connection failed")
            return None
            
        query = """
        MATCH (i:Investigation {workflow_id: $workflow_id})
        RETURN i.state as state, i.repository_id as repository_id
        """
        
        result = connector.run_query(query, {"workflow_id": workflow_id})
        
        if not result:
            logger.warning(f"No investigation state found for workflow {workflow_id}")
            return None
            
        # Create a new instance
        repository_id = result[0]["repository_id"]
        instance = cls(workflow_id, repository_id)
        
        # Load the state data
        try:
            state_data = json.loads(result[0]["state"])
            instance.state_data = state_data
            
            # Convert lists back to sets
            if "completed_steps" in state_data:
                instance.state_data["completed_steps"] = set(state_data["completed_steps"])
                
            logger.info(f"Loaded investigation state for workflow {workflow_id}")
            return instance
        except json.JSONDecodeError:
            logger.error(f"Failed to parse investigation state for workflow {workflow_id}")
            return None


class MarkdownReportGenerator:
    """Generates markdown reports from vulnerability findings.
    
    This class provides functionality to generate comprehensive, structured
    markdown reports for vulnerability assessments.
    """
    
    def __init__(self, working_dir: Optional[str] = None):
        """Initialize the markdown report generator.
        
        Args:
            working_dir: Optional working directory for report files
        """
        self.working_dir = working_dir or tempfile.mkdtemp(prefix="skwaq_report_")
        
    def cleanup(self) -> None:
        """Clean up resources."""
        if os.path.exists(self.working_dir):
            import shutil
            try:
                shutil.rmtree(self.working_dir)
            except Exception as e:
                logger.warning(f"Failed to clean up working directory: {e}")
    
    def _severity_emoji(self, severity: str) -> str:
        """Get emoji for a severity level.
        
        Args:
            severity: The severity level
            
        Returns:
            Emoji representing the severity
        """
        return {
            "critical": "🔥",
            "high": "⚠️",
            "medium": "⚡",
            "low": "ℹ️",
        }.get(severity.lower(), "ℹ️")
    
    async def generate_report(
        self, 
        report_data: Dict[str, Any], 
        findings: List[Dict[str, Any]],
        output_path: Optional[str] = None
    ) -> str:
        """Generate a markdown report.
        
        Args:
            report_data: Report data including summary and metadata
            findings: List of vulnerability findings
            output_path: Optional output path
            
        Returns:
            Path to the generated report
        """
        # Create report filename if not provided
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            repo_name = report_data.get("repository", {}).get("name", "repo")
            filename = f"vulnerability_assessment_{repo_name}_{timestamp}.md"
            output_path = os.path.join(self.working_dir, filename)
        
        # Build the report content
        lines = []
        
        # Title section
        lines.append(f"# {report_data.get('title', 'Vulnerability Assessment Report')}")
        lines.append(f"**Generated:** {report_data.get('date', datetime.now().isoformat())}")
        lines.append("")
        
        # Repository information
        repo = report_data.get("repository", {})
        lines.append("## Repository Information")
        lines.append(f"- **Name:** {repo.get('name', 'Unknown')}")
        if repo.get("url"):
            lines.append(f"- **URL:** {repo.get('url')}")
        if repo.get("description"):
            lines.append(f"- **Description:** {repo.get('description')}")
        lines.append("")
        
        # Executive summary
        lines.append("## Executive Summary")
        summary = report_data.get("summary", {})
        lines.append(f"A vulnerability assessment was conducted on the {repo.get('name', 'subject')} repository. ")
        lines.append(f"The assessment identified **{summary.get('total_vulnerabilities', 0)}** potential vulnerabilities ")
        lines.append(f"with an overall risk score of **{summary.get('risk_score', 'N/A')}/100**.")
        lines.append("")
        
        # Severity distribution
        lines.append("### Severity Distribution")
        lines.append("")
        for severity, count in summary.get("severity_distribution", {}).items():
            if count > 0:
                lines.append(f"- {self._severity_emoji(severity)} **{severity}:** {count}")
        lines.append("")
        
        # Key findings
        lines.append("## Key Findings")
        lines.append("")
        
        key_findings = report_data.get("key_findings", [])
        if not key_findings:
            lines.append("No significant vulnerabilities were identified.")
        else:
            for i, finding in enumerate(key_findings):
                severity = finding.get("severity", "Medium")
                lines.append(f"### {i+1}. {self._severity_emoji(severity)} {finding.get('vulnerability_type', 'Issue')}")
                lines.append("")
                lines.append(f"**Severity:** {severity}")
                lines.append(f"**Location:** `{finding.get('file_path', 'Unknown')}`")
                lines.append("")
                lines.append(finding.get('description', 'No description provided'))
                lines.append("")
        
        # Recommendations
        lines.append("## Recommendations")
        lines.append("")
        
        recommendations = report_data.get("recommendations", [])
        if not recommendations:
            lines.append("No specific recommendations available.")
        else:
            for i, rec in enumerate(recommendations):
                priority = rec.get("priority", "Medium")
                priority_marker = "🔴" if priority.lower() == "high" else "🟡" if priority.lower() == "medium" else "🟢"
                
                lines.append(f"### {priority_marker} {rec.get('category', f'Recommendation {i+1}')}")
                lines.append("")
                lines.append(f"**Priority:** {priority}")
                lines.append("")
                lines.append(rec.get('recommendation', 'No specific recommendation provided'))
                lines.append("")
        
        # Detailed findings
        lines.append("## Detailed Findings")
        lines.append("")
        
        if not findings:
            lines.append("No vulnerabilities were identified.")
        else:
            for i, finding in enumerate(findings):
                severity = finding.get("severity", "Medium")
                lines.append(f"### {i+1}. {self._severity_emoji(severity)} {finding.get('vulnerability_type', 'Issue')}")
                lines.append("")
                lines.append(f"**Severity:** {severity}")
                lines.append(f"**Confidence:** {finding.get('confidence', 'N/A')}")
                if finding.get("cwe_id"):
                    lines.append(f"**CWE:** [{finding.get('cwe_id')}](https://cwe.mitre.org/data/definitions/{finding.get('cwe_id', '').replace('CWE-', '')}.html)")
                lines.append(f"**Location:** `{finding.get('file_path', 'Unknown')}`")
                lines.append("")
                lines.append("#### Description")
                lines.append("")
                lines.append(finding.get('description', 'No description provided'))
                lines.append("")
                
                if finding.get("remediation"):
                    lines.append("#### Remediation")
                    lines.append("")
                    lines.append(finding.get('remediation', 'No remediation guidance provided'))
                    lines.append("")
                
                lines.append("---")
                lines.append("")
        
        # Methodology
        lines.append("## Assessment Methodology")
        lines.append("")
        lines.append("The vulnerability assessment was conducted using the following methodology:")
        lines.append("")
        lines.append("1. **Initial repository assessment** - Analysis of the repository structure and code composition")
        lines.append("2. **Focus area analysis** - Targeted analysis of specific security areas:")
        for area in report_data.get("focus_areas", DEFAULT_FOCUS_AREAS):
            lines.append(f"   - {area}")
        lines.append("3. **Verification and validation** - Review and validation of findings")
        lines.append("4. **Report generation** - Compilation of findings and recommendations")
        lines.append("")
        
        # Footer
        lines.append("---")
        lines.append("")
        lines.append("*This report was generated by Skwaq Vulnerability Assessment Copilot*")
        lines.append(f"*Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")
        
        # Write the report to the file
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w") as f:
            f.write("\n".join(lines))
        
        return output_path


class GitHubIssueGenerator:
    """Generates GitHub issues from vulnerability findings.
    
    This class provides functionality to prepare GitHub issues for vulnerability
    findings, with appropriate formatting and details.
    """
    
    def __init__(self):
        """Initialize the GitHub issue generator."""
        pass
    
    async def prepare_issues(
        self,
        report_data: Dict[str, Any],
        findings: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Prepare GitHub issues from vulnerability findings.
        
        Args:
            report_data: Report data including summary and metadata
            findings: List of vulnerability findings
            
        Returns:
            List of prepared GitHub issues (not yet created)
        """
        issues = []
        
        # Group findings by vulnerability type
        grouped_findings = {}
        for finding in findings:
            vuln_type = finding.get("vulnerability_type", "Unknown Issue")
            if vuln_type not in grouped_findings:
                grouped_findings[vuln_type] = []
            grouped_findings[vuln_type].append(finding)
        
        # Create an issue for each vulnerability type
        for vuln_type, type_findings in grouped_findings.items():
            # Get highest severity
            severity = "low"
            for f in type_findings:
                f_severity = f.get("severity", "low").lower()
                if f_severity in ["critical", "high"]:
                    severity = f_severity
                    break
                elif f_severity == "medium" and severity == "low":
                    severity = "medium"
            
            # Create issue body
            body = f"""## {vuln_type} Vulnerability

**Severity:** {severity.capitalize()}
**Number of occurrences:** {len(type_findings)}

### Description

{type_findings[0].get('description', 'No description provided')}

### Affected Locations

"""
            
            # Add affected locations
            for f in type_findings:
                body += f"- `{f.get('file_path', 'Unknown')}:{f.get('line_number', 'N/A')}`\n"
            
            body += """
### Remediation

"""
            
            # Add remediation guidance
            body += type_findings[0].get('remediation', 'No remediation guidance provided')
            
            # Add metadata
            body += """

---

*This issue was automatically generated by Skwaq Vulnerability Assessment Copilot*
"""
            
            # Create the issue object
            issue = {
                "title": f"Security: {vuln_type} Vulnerability",
                "body": body,
                "labels": ["security", f"severity:{severity}"]
            }
            
            issues.append(issue)
        
        return issues
    
    async def create_issues(
        self,
        issues: List[Dict[str, Any]],
        repository_url: str,
        github_token: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """Create GitHub issues for vulnerability findings.
        
        Args:
            issues: List of prepared GitHub issues
            repository_url: URL of the GitHub repository
            github_token: Optional GitHub token for authentication
            
        Returns:
            List of created GitHub issues with their URLs
        """
        # Parse repository owner and name from URL
        import re
        match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repository_url)
        if not match:
            logger.error(f"Invalid GitHub repository URL: {repository_url}")
            return []
            
        owner, repo = match.groups()
        
        # We'll just prepare the shell commands for now
        created_issues = []
        
        for issue in issues:
            # Create a temp file for the issue body
            with tempfile.NamedTemporaryFile(mode="w", suffix=".md", delete=False) as f:
                f.write(issue["body"])
                issue_file = f.name
            
            # Prepare the shell command
            labels = ",".join(issue["labels"])
            command = [
                "gh", "issue", "create",
                "--title", issue["title"],
                "--body-file", issue_file,
                "--repo", f"{owner}/{repo}",
                "--label", labels
            ]
            
            if github_token:
                # In a real implementation, we'd use the token
                pass
            
            # In a real implementation, we'd execute the command here
            # For now, just return the command and issue data
            created_issues.append({
                "title": issue["title"],
                "command": " ".join(command),
                "issue_file": issue_file,
                "repository": f"{owner}/{repo}",
                "status": "prepared"  # Not actually created yet
            })
            
        return created_issues


class VulnerabilityResearchWorkflow(Workflow):
    """Workflow for conducting vulnerability research on a codebase.
    
    This workflow implements a comprehensive vulnerability assessment process,
    integrating code analysis, knowledge retrieval, and tool invocation to
    provide detailed findings and actionable recommendations.
    """

    def __init__(
        self, 
        repository_id: str, 
        focus_areas: Optional[List[str]] = None,
        workflow_id: Optional[str] = None,
        enable_persistence: bool = True
    ):
        """Initialize the vulnerability research workflow.
        
        Args:
            repository_id: The ID of the repository to analyze
            focus_areas: Optional list of security focus areas to analyze
            workflow_id: Optional workflow ID for persistence
            enable_persistence: Whether to enable investigation persistence
        """
        super().__init__(
            name="VulnerabilityResearch",
            description="Comprehensive vulnerability research workflow",
            repository_id=repository_id
        )
        
        self.focus_areas = focus_areas or DEFAULT_FOCUS_AREAS
        self.workflow_id = workflow_id or f"vr-{uuid.uuid4().hex[:8]}"
        self.enable_persistence = enable_persistence
        
        # Track findings and state
        self._findings: List[Dict[str, Any]] = []
        self._verified_findings: List[Dict[str, Any]] = []
        self._current_phase = 0
        self._current_focus_area_index = 0
        
        # State management
        if enable_persistence:
            self._state = InvestigationState(self.workflow_id, repository_id)
        else:
            self._state = None
            
        # Working directory for outputs
        self._working_dir = None
            
    async def setup(self) -> None:
        """Set up the vulnerability research workflow."""
        await super().setup()
        
        # Create the research agent
        self.agents["research"] = VulnerabilityResearchAgent()
        
        # Create the code analyzer
        self.analyzer = CodeAnalyzer()
        
        # Try to load previous state if workflow_id was provided
        if self.enable_persistence and self.workflow_id:
            loaded_state = InvestigationState.load(self.workflow_id)
            if loaded_state:
                self._state = loaded_state
                # Restore workflow state from the loaded state
                self._current_phase = self._state.state_data.get("current_phase", 0)
                self._current_focus_area_index = self._state.state_data.get("current_focus_area_index", 0)
                self._findings = self._state.state_data.get("findings", [])
                logger.info(f"Restored workflow state from previous session: phase {self._current_phase}")
                
        # Create working directory for outputs
        self._working_dir = tempfile.mkdtemp(prefix="skwaq_vr_")
        logger.debug(f"Created working directory: {self._working_dir}")
            
    def cleanup(self) -> None:
        """Clean up resources when the workflow is done."""
        # Save final state if persistence is enabled
        if self.enable_persistence and self._state:
            self._state.save()
            
        # Clean up working directory if it exists
        if self._working_dir and os.path.exists(self._working_dir):
            import shutil
            try:
                shutil.rmtree(self._working_dir)
                logger.debug(f"Cleaned up working directory: {self._working_dir}")
            except Exception as e:
                logger.warning(f"Failed to clean up working directory: {e}")
                
    def pause(self) -> None:
        """Pause the workflow iteration."""
        super().pause()
        
        # Save current state if persistence is enabled
        if self.enable_persistence and self._state:
            self._state.update(
                current_phase=self._current_phase,
                current_focus_area_index=self._current_focus_area_index
            )
            self._state.save()
            
        logger.info(
            f"Workflow paused at phase {self._current_phase}, focus area {self._current_focus_area_index}"
        )

    def resume(self) -> None:
        """Resume the workflow iteration."""
        super().resume()
        
        logger.info(
            f"Workflow resumed from phase {self._current_phase}, focus area {self._current_focus_area_index}"
        )

    def should_continue(self) -> bool:
        """Check if the workflow should continue iterating.

        Returns:
            bool: True if the workflow should continue, False if it should stop
        """
        # Check if the workflow has been paused
        if not super().should_continue():
            return False

        # Check if all phases are complete
        if self._current_phase >= 3:  # All phases complete
            return False

        # Check if current phase is complete
        if self._current_phase == 1 and self._current_focus_area_index >= len(
            self.focus_areas
        ):
            return False

        return True

    async def run(self) -> AsyncGenerator[Dict[str, Any], None]:
        """Run the vulnerability research workflow.
        
        Yields:
            Dict[str, Any]: Progress updates and findings
        """
        try:
            # Reset state if starting from the beginning
            if self._current_phase == 0:
                self._current_phase = 0
                self._current_focus_area_index = 0
                
                # Save initial state if persistence is enabled
                if self.enable_persistence and self._state:
                    self._state.update(
                        current_phase=self._current_phase,
                        current_focus_area_index=self._current_focus_area_index
                    )
                    self._state.save()

            # Phase 1: Initial repository assessment
            if self._current_phase == 0:
                yield {
                    "phase": 1,
                    "status": "starting",
                    "message": "Beginning initial repository assessment",
                }
                
                repo_info = await self._get_repository_info()
                
                yield {
                    "phase": 1,
                    "status": "complete",
                    "message": "Initial assessment complete",
                    "data": repo_info,
                }
                
                # Update state and save
                self._current_phase = 1
                if self.enable_persistence and self._state:
                    self._state.update(
                        current_phase=self._current_phase,
                        add_completed_step="repository_assessment"
                    )
                    self._state.save()

            # Phase 2: Focus area analysis
            if self._current_phase == 1:
                # Start from the current focus area index (supports resuming)
                while (
                    self._current_focus_area_index < len(self.focus_areas)
                    and self.should_continue()
                ):
                    focus_area = self.focus_areas[self._current_focus_area_index]
                    
                    yield {
                        "phase": 2,
                        "status": "in_progress",
                        "message": f"Analyzing focus area: {focus_area}",
                        "focus_area": focus_area,
                        "progress": self._current_focus_area_index / len(self.focus_areas),
                    }

                    # Get findings for this focus area
                    findings = await self._analyze_focus_area(focus_area)
                    
                    # Add findings to our collection
                    self._findings.extend(findings)
                    
                    # Save findings if persistence is enabled
                    if self.enable_persistence and self._state:
                        for finding in findings:
                            self._state.update(add_finding=finding)
                        self._state.save()

                    yield {
                        "phase": 2,
                        "status": "focus_area_complete",
                        "message": f"Completed analysis of {focus_area}",
                        "findings": findings,
                    }

                    # Move to the next focus area
                    self._current_focus_area_index += 1
                    
                    # Update state
                    if self.enable_persistence and self._state:
                        self._state.update(
                            current_focus_area_index=self._current_focus_area_index,
                            add_completed_step=f"focus_area_{focus_area}"
                        )
                        self._state.save()
                    
                    # Check if we should pause (e.g., user requested pause)
                    if not self.should_continue():
                        return
                
                # We've completed all focus areas, move to the next phase
                if self._current_focus_area_index >= len(self.focus_areas):
                    self._current_phase = 2
                    
                    # Update state
                    if self.enable_persistence and self._state:
                        self._state.update(
                            current_phase=self._current_phase,
                            add_completed_step="all_focus_areas"
                        )
                        self._state.save()

            # Phase 3: Generate Markdown Report and GitHub Issues
            if self._current_phase == 2:
                yield {
                    "phase": 3,
                    "status": "starting",
                    "message": "Generating comprehensive report and GitHub issues",
                }
                
                # Generate the final report
                report = await self._generate_final_report()
                
                # Generate markdown report
                markdown_path = await self._generate_markdown_report(report)
                
                # Generate GitHub issues if requested
                github_issues = await self._prepare_github_issues(report)
                
                # Update state
                if self.enable_persistence and self._state:
                    self._state.update(
                        current_phase=3,  # Mark as completed
                        add_completed_step="report_generation"
                    )
                    self._state.save()

                yield {
                    "phase": 3,
                    "status": "complete",
                    "message": "Vulnerability research complete",
                    "report": report,
                    "findings": self._findings,
                    "markdown_path": markdown_path,
                    "github_issues": github_issues,
                }
                
                # Mark as completed
                self._current_phase = 3
            
            # Cleanup at the end
            self.cleanup()
            
        except Exception as e:
            logger.error(f"Error in vulnerability research workflow: {e}")
            # Save state even if there's an error
            if self.enable_persistence and self._state:
                self._state.update(error=str(e))
                self._state.save()
            
            yield {
                "phase": self._current_phase,
                "status": "error",
                "message": f"Error in vulnerability research workflow: {e}",
                "error": str(e),
            }

    async def _get_repository_info(self) -> Dict[str, Any]:
        """Get basic information about the repository.

        Retrieves metadata about the repository from the database and performs
        an initial assessment of the codebase.

        Returns:
            Dictionary containing repository information
        """
        connector = get_connector()
        if not connector.connect():
            logger.error(
                f"Failed to connect to database for repository {self.repository_id}"
            )
            return {"error": "Database connection failed"}

        # Fetch repository information from the database
        query = """
        MATCH (r:Repository {id: $repo_id})
        OPTIONAL MATCH (r)-[:CONTAINS]->(f:File)
        OPTIONAL MATCH (f)-[:USES_LANGUAGE]->(l:Language)
        WITH r, count(f) as file_count, collect(distinct l.name) as languages
        RETURN r.name as name, r.url as url, r.description as description,
               file_count, languages, r.last_updated as last_updated
        """

        result = connector.run_query(query, {"repo_id": self.repository_id})
        if not result:
            logger.warning(f"No repository found with ID {self.repository_id}")
            return {"error": f"Repository {self.repository_id} not found"}

        repo_data = result[0]

        # Get file types distribution
        file_types_query = """
        MATCH (r:Repository {id: $repo_id})-[:CONTAINS]->(f:File)
        WITH f.extension as extension, count(*) as count
        WHERE extension IS NOT NULL
        RETURN extension, count
        ORDER BY count DESC
        LIMIT 10
        """

        file_types = connector.run_query(
            file_types_query, {"repo_id": self.repository_id}
        )

        # Calculate complexity metrics
        complexity_query = """
        MATCH (r:Repository {id: $repo_id})-[:CONTAINS]->(f:File)
        WHERE f.complexity IS NOT NULL
        RETURN avg(f.complexity) as avg_complexity,
               max(f.complexity) as max_complexity,
               min(f.complexity) as min_complexity
        """

        complexity = connector.run_query(
            complexity_query, {"repo_id": self.repository_id}
        )

        # Assemble the repository information
        repo_info = {
            "name": repo_data.get("name", "Unknown"),
            "url": repo_data.get("url"),
            "description": repo_data.get("description"),
            "file_count": repo_data.get("file_count", 0),
            "languages": repo_data.get("languages", []),
            "last_updated": repo_data.get("last_updated"),
            "file_types": {item["extension"]: item["count"] for item in file_types},
            "complexity": complexity[0] if complexity else {"avg_complexity": None},
            "assessment_date": datetime.now().isoformat(),
        }

        logger.info(
            f"Retrieved repository info for {repo_info['name']} ({self.repository_id})"
        )
        return repo_info

    async def _analyze_focus_area(self, focus_area: str) -> List[Dict[str, Any]]:
        """Analyze a specific security focus area.

        Uses the vulnerability research agent to analyze code related to
        a specific security focus area.

        Args:
            focus_area: The security focus area to analyze

        Returns:
            List of findings related to the focus area
        """
        logger.info(f"Starting analysis of focus area: {focus_area}")
        findings = []

        # Get relevant files for this focus area
        files = await self._get_files_for_focus_area(focus_area)

        # Process each file
        for file in files:
            file_id = file.get("id")
            file_path = file.get("path")
            language = file.get("language", "Unknown")
            content = file.get("content", "")

            if not content:
                logger.warning(f"Empty content for file {file_path}, skipping")
                continue

            logger.info(f"Analyzing file {file_path} for {focus_area} vulnerabilities")

            # Generate context for the analysis
            context = {
                "focus_area": focus_area,
                "file_path": file_path,
                "language": language,
                "repository_id": self.repository_id,
            }

            # Research potential vulnerabilities in this file related to the focus area
            # Use the enhanced agent for better analysis
            file_findings = await self.agents["research"].analyze_code(
                code=content,
                context=context,
                vulnerability_type=focus_area
            )

            # If vulnerabilities were found, add them to the findings list
            if file_findings.get("confirmed", False):
                finding = {
                    "file_id": file_id,
                    "file_path": file_path,
                    "focus_area": focus_area,
                    "vulnerability_type": file_findings.get("vulnerability_type", focus_area),
                    "description": file_findings.get("description", "Potential vulnerability detected"),
                    "severity": file_findings.get("severity", "Medium"),
                    "confidence": file_findings.get("confidence", 0.5),
                    "cwe_id": file_findings.get("cwe_id", ""),
                    "remediation": file_findings.get("remediation", ""),
                    "line_number": file_findings.get("line_numbers", [0])[0] if file_findings.get("line_numbers") else 0,
                    "timestamp": datetime.now().isoformat(),
                }

                findings.append(finding)

                # Emit an event for the discovered vulnerability
                Event.add(
                    VulnerabilityDiscoveryEvent(
                        sender=self.__class__.__name__,
                        vulnerability_type=finding["vulnerability_type"],
                        file_path=finding["file_path"],
                        line_number=finding["line_number"],
                        severity=finding["severity"],
                        confidence=finding["confidence"],
                        description=finding["description"],
                        remediation=finding["remediation"]
                    )
                )

                # Store the finding in the database
                await self._store_finding(finding)

        logger.info(
            f"Completed focus area analysis: {focus_area}, found {len(findings)} vulnerabilities"
        )
        return findings

    async def _get_files_for_focus_area(self, focus_area: str) -> List[Dict[str, Any]]:
        """Get files that are relevant for a specific security focus area.

        Args:
            focus_area: The security focus area

        Returns:
            List of relevant files with their content
        """
        connector = get_connector()

        # Map focus areas to relevant file patterns and keywords
        focus_area_mapping = {
            "Injection": {
                "keywords": [
                    "sql",
                    "query",
                    "execute",
                    "injection",
                    "sanitize",
                    "escape",
                ],
                "extensions": [".sql", ".py", ".js", ".php", ".java", ".cs"],
            },
            "Authentication": {
                "keywords": [
                    "auth",
                    "login",
                    "password",
                    "credential",
                    "session",
                    "token",
                ],
                "extensions": [".py", ".js", ".php", ".java", ".cs", ".go", ".rb"],
            },
            "Authorization": {
                "keywords": ["role", "permission", "access", "authz", "rbac", "acl"],
                "extensions": [".py", ".js", ".php", ".java", ".cs", ".go", ".rb"],
            },
            "Sensitive Data Exposure": {
                "keywords": [
                    "encrypt",
                    "decrypt",
                    "secret",
                    "key",
                    "password",
                    "token",
                    "pii",
                ],
                "extensions": [
                    ".py",
                    ".js",
                    ".php",
                    ".java",
                    ".cs",
                    ".go",
                    ".rb",
                    ".conf",
                ],
            },
            "Security Misconfiguration": {
                "keywords": [
                    "config",
                    "setting",
                    "env",
                    "environment",
                    "setup",
                    "init",
                ],
                "extensions": [
                    ".json",
                    ".yml",
                    ".yaml",
                    ".xml",
                    ".conf",
                    ".cfg",
                    ".ini",
                ],
            },
            "Input Validation": {
                "keywords": [
                    "input",
                    "validate",
                    "sanitize",
                    "parse",
                    "escape",
                    "filter",
                ],
                "extensions": [".py", ".js", ".php", ".java", ".cs", ".go", ".rb"],
            },
            "Error Handling": {
                "keywords": ["error", "exception", "catch", "try", "finally", "handle"],
                "extensions": [".py", ".js", ".php", ".java", ".cs", ".go", ".rb"],
            },
            "Cryptographic Issues": {
                "keywords": [
                    "crypt",
                    "hash",
                    "salt",
                    "key",
                    "encrypt",
                    "decrypt",
                    "sign",
                ],
                "extensions": [".py", ".js", ".php", ".java", ".cs", ".go", ".rb"],
            },
        }

        # Get the mapping for the current focus area
        mapping = focus_area_mapping.get(
            focus_area, {"keywords": [], "extensions": [".py", ".js", ".java", ".cs"]}
        )

        # Build a Cypher query to find relevant files
        query = """
        MATCH (r:Repository {id: $repo_id})-[:CONTAINS]->(f:File)
        WHERE 
            (ANY(ext IN $extensions WHERE f.path ENDS WITH ext))
            OR
            (ANY(kw IN $keywords WHERE f.content CONTAINS kw))
        WITH f, f.path as path
        ORDER BY f.complexity DESC
        LIMIT 10
        MATCH (f)-[:USES_LANGUAGE]->(l:Language)
        RETURN id(f) as id, f.path as path, l.name as language, f.content as content
        """

        params = {
            "repo_id": self.repository_id,
            "extensions": mapping["extensions"],
            "keywords": mapping["keywords"],
        }

        result = connector.run_query(query, params)

        # If no files found with specific criteria, get the top 5 most complex files
        if not result:
            logger.warning(
                f"No specific files found for {focus_area}, falling back to most complex files"
            )
            fallback_query = """
            MATCH (r:Repository {id: $repo_id})-[:CONTAINS]->(f:File)
            WITH f ORDER BY f.complexity DESC
            LIMIT 5
            MATCH (f)-[:USES_LANGUAGE]->(l:Language)
            RETURN id(f) as id, f.path as path, l.name as language, f.content as content
            """
            result = connector.run_query(
                fallback_query, {"repo_id": self.repository_id}
            )

        logger.info(f"Retrieved {len(result)} files for focus area: {focus_area}")
        return result

    async def _store_finding(self, finding: Dict[str, Any]) -> None:
        """Store a vulnerability finding in the database.

        Args:
            finding: The vulnerability finding to store
        """
        connector = get_connector()

        # Create a Finding node
        finding_props = {
            "file_path": finding["file_path"],
            "vulnerability_type": finding["vulnerability_type"],
            "description": finding["description"],
            "severity": finding["severity"],
            "cwe_id": finding.get("cwe_id"),
            "remediation": finding.get("remediation", ""),
            "timestamp": finding["timestamp"],
        }

        finding_id = connector.create_node(["Finding", "Vulnerability"], finding_props)

        if finding_id:
            # Connect the finding to the file
            file_id = finding["file_id"]
            connector.create_relationship(
                finding_id, file_id, "FOUND_IN", {"focus_area": finding["focus_area"]}
            )

            # Connect the finding to the repository
            repo_query = "MATCH (r:Repository {id: $repo_id}) RETURN id(r) as id"
            repo_result = connector.run_query(
                repo_query, {"repo_id": self.repository_id}
            )

            if repo_result:
                repo_id = repo_result[0]["id"]
                connector.create_relationship(
                    finding_id,
                    repo_id,
                    "BELONGS_TO",
                    {"timestamp": finding["timestamp"]},
                )

        logger.debug(f"Stored finding in database with ID {finding_id}")

    async def _generate_final_report(self) -> Dict[str, Any]:
        """Generate the final vulnerability research report.

        Consolidates all findings and generates a comprehensive
        vulnerability assessment report.

        Returns:
            Dictionary containing the final report
        """
        logger.info("Generating final vulnerability report")

        # Get repository info
        repo_info = await self._get_repository_info()

        # Calculate statistics
        severity_counts = {
            "Critical": 0,
            "High": 0,
            "Medium": 0,
            "Low": 0,
            "Unknown": 0,
        }
        focus_area_counts = {area: 0 for area in self.focus_areas}

        # Count findings by severity and focus area
        for finding in self._findings:
            severity = finding.get("severity", "Unknown")
            focus_area = finding.get("focus_area")

            if severity in severity_counts:
                severity_counts[severity] += 1

            if focus_area in focus_area_counts:
                focus_area_counts[focus_area] += 1

        # Get the most severe findings (top 5)
        severity_order = ["Critical", "High", "Medium", "Low", "Unknown"]
        most_severe = sorted(
            self._findings,
            key=lambda x: severity_order.index(x.get("severity", "Unknown")),
        )[:5]

        # Calculate the overall risk score (weighted average of severity counts)
        severity_weights = {
            "Critical": 4,
            "High": 3,
            "Medium": 2,
            "Low": 1,
            "Unknown": 1,
        }
        total_findings = sum(severity_counts.values())

        risk_score = 0
        if total_findings > 0:
            weighted_sum = sum(
                severity_counts[s] * severity_weights[s] for s in severity_counts
            )
            risk_score = round(
                (weighted_sum / total_findings) * 25, 1
            )  # Scale to 0-100

        # Generate recommendations based on findings
        recommendations = await self._generate_recommendations()

        # Compile the final report
        report = {
            "title": f"Security Assessment Report for {repo_info.get('name', 'Repository')}",
            "date": datetime.now().isoformat(),
            "repository": {
                "id": self.repository_id,
                "name": repo_info.get("name"),
                "url": repo_info.get("url"),
                "description": repo_info.get("description"),
            },
            "summary": {
                "total_vulnerabilities": total_findings,
                "risk_score": risk_score,
                "severity_distribution": severity_counts,
                "focus_area_distribution": focus_area_counts,
            },
            "key_findings": [
                {
                    "vulnerability_type": finding.get("vulnerability_type"),
                    "severity": finding.get("severity"),
                    "file_path": finding.get("file_path"),
                    "description": finding.get("description"),
                }
                for finding in most_severe
            ],
            "recommendations": recommendations,
            "focus_areas": self.focus_areas,
        }

        # Store the report in the database
        await self._store_report(report)

        logger.info(f"Completed vulnerability report with {total_findings} findings")
        return report

    async def _generate_recommendations(self) -> List[Dict[str, str]]:
        """Generate security recommendations based on findings.

        Returns:
            List of recommendation dictionaries
        """
        # Group findings by vulnerability type
        vuln_types = {}
        for finding in self._findings:
            vuln_type = finding.get("vulnerability_type")
            if vuln_type not in vuln_types:
                vuln_types[vuln_type] = []
            vuln_types[vuln_type].append(finding)

        recommendations = []

        # Generate recommendations for each vulnerability type
        for vuln_type, findings in vuln_types.items():
            # Get the most common remediation advice
            remediations = [
                f.get("remediation", "") for f in findings if f.get("remediation")
            ]

            if remediations:
                # Use the most detailed remediation as the basis
                recommendation = max(remediations, key=len)
            else:
                # Fallback generic recommendation
                recommendation = f"Address {vuln_type} vulnerabilities by implementing proper security controls."

            recommendations.append(
                {
                    "category": vuln_type,
                    "recommendation": recommendation,
                    "priority": (
                        "High"
                        if any(
                            f.get("severity") in ["Critical", "High"] for f in findings
                        )
                        else "Medium"
                    ),
                }
            )

        return recommendations

    async def _store_report(self, report: Dict[str, Any]) -> None:
        """Store the final report in the database.

        Args:
            report: The report to store
        """
        connector = get_connector()

        # Create a Report node
        report_props = {
            "title": report["title"],
            "date": report["date"],
            "total_vulnerabilities": report["summary"]["total_vulnerabilities"],
            "risk_score": report["summary"]["risk_score"],
            "json_data": json.dumps(report),
        }

        report_id = connector.create_node("Report", report_props)

        if report_id:
            # Connect the report to the repository
            repo_query = "MATCH (r:Repository {id: $repo_id}) RETURN id(r) as id"
            repo_result = connector.run_query(
                repo_query, {"repo_id": self.repository_id}
            )

            if repo_result:
                repo_id = repo_result[0]["id"]
                connector.create_relationship(
                    report_id, repo_id, "ASSESSES", {"timestamp": report["date"]}
                )

        logger.debug(f"Stored final report in database with ID {report_id}")
        
    async def _generate_markdown_report(self, report: Dict[str, Any]) -> str:
        """Generate a markdown report from the vulnerability assessment.
        
        Args:
            report: The vulnerability assessment report
            
        Returns:
            Path to the generated markdown report
        """
        # Create the markdown report generator
        report_generator = MarkdownReportGenerator(self._working_dir)
        
        # Generate the report
        report_path = await report_generator.generate_report(
            report_data=report,
            findings=self._findings
        )
        
        logger.info(f"Generated markdown report: {report_path}")
        return report_path
        
    async def _prepare_github_issues(self, report: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Prepare GitHub issues for the vulnerabilities.
        
        Args:
            report: The vulnerability assessment report
            
        Returns:
            List of prepared GitHub issues
        """
        # Create the GitHub issue generator
        issue_generator = GitHubIssueGenerator()
        
        # Prepare the issues
        issues = await issue_generator.prepare_issues(
            report_data=report,
            findings=self._findings
        )
        
        logger.info(f"Prepared {len(issues)} GitHub issues")
        return issues