# Codebase Ingestion Guide

This guide explains how to use the Skwaq Ingestion module to ingest codebases into the system for vulnerability assessment.

## Overview

The Ingestion module is responsible for importing codebases into the system, parsing their structure, and generating summaries to enhance the vulnerability assessment process. It can ingest code from:

- Local filesystem directories
- Remote Git repositories

The result of ingestion is a graph representation of the codebase in the Neo4j database, including:

- Filesystem structure
- Syntax tree (AST)
- Code summaries generated by LLM
- Documentation (if provided)

## Basic Usage

### Ingesting a Local Codebase

```python
import asyncio
from skwaq.core.openai_client import get_openai_client
from skwaq.ingestion import Ingestion

async def ingest_local_codebase():
    # Get OpenAI client for the LLM processing
    openai_client = get_openai_client(async_mode=True)
    
    # Set path to local codebase
    local_path = "/path/to/local/codebase"
    
    # Create ingestion instance with local path
    ingestion = Ingestion(local_path=local_path, model_client=openai_client)
    
    # Start ingestion and get ID
    ingestion_id = await ingestion.ingest()
    print(f"Started ingestion with ID: {ingestion_id}")
    
    # Poll for status updates until complete
    completed = False
    while not completed:
        status = await ingestion.get_status(ingestion_id)
        print(f"Status: {status.state} - Progress: {status.progress}%")
        
        # Check if ingestion is complete
        if status.state in ["completed", "failed"]:
            completed = True
            print(f"Ingestion {status.state} in {status.time_elapsed:.2f} seconds")
            if status.state == "failed":
                print(f"Error: {status.error}")
        else:
            # Wait before polling again
            await asyncio.sleep(5)
    
    return ingestion_id, status

# Run the async function
asyncio.run(ingest_local_codebase())
```

### Ingesting a Git Repository

```python
import asyncio
from skwaq.core.openai_client import get_openai_client
from skwaq.ingestion import Ingestion

async def ingest_repository():
    # Get OpenAI client for the LLM processing
    openai_client = get_openai_client(async_mode=True)
    
    # Initialize ingestion with repo URL and branch
    repo = "https://github.com/dotnet/eShop"
    branch = "main"
    
    # Create ingestion instance
    ingestion = Ingestion(repo=repo, branch=branch, model_client=openai_client)
    
    # Start ingestion and get ID
    ingestion_id = await ingestion.ingest()
    print(f"Started ingestion with ID: {ingestion_id}")
    
    # Poll for status updates until complete
    completed = False
    while not completed:
        status = await ingestion.get_status(ingestion_id)
        print(f"Status: {status.state} - Progress: {status.progress}%")
        print(f"Files processed: {status.files_processed}/{status.total_files}")
        
        if status.errors:
            print(f"Encountered {len(status.errors)} errors")
        
        # Check if ingestion is complete
        if status.state in ["completed", "failed"]:
            completed = True
            print(f"Ingestion {status.state} in {status.time_elapsed:.2f} seconds")
            if status.state == "failed":
                print(f"Error: {status.error}")
        else:
            # Wait before polling again
            await asyncio.sleep(10)
    
    return ingestion_id, status

# Run the async function
asyncio.run(ingest_repository())
```

## Configuration Options

The `Ingestion` class accepts the following parameters:

- `local_path` (optional): Path to a local codebase directory
- `repo` (optional): Git repository URL to clone
- `branch` (optional): Git branch to checkout (default: repository's default branch)
- `model_client` (optional): OpenAI model client for code summarization
- `max_parallel` (optional): Maximum number of parallel threads for processing (default: 3)
- `doc_path` (optional): Path to additional documentation
- `doc_uri` (optional): URI to additional documentation
- `context_token_limit` (optional): Maximum number of tokens to keep in context (default: 20000)
- `parse_only` (optional): Flag to only parse the codebase without LLM summarization (default: False)

Notes:
- You must provide either `local_path` OR `repo`, but not both
- For LLM summarization to work, you must provide a `model_client`

## Tracking Ingestion Status

The `get_status` method returns an `IngestionStatus` object with the following properties:

- `id`: Unique identifier for the ingestion
- `state`: Current state of the ingestion process (e.g., "initializing", "processing", "completed", "failed")
- `progress`: Progress percentage (0-100)
- `start_time`: Timestamp when ingestion started
- `end_time`: Timestamp when ingestion completed or failed
- `error`: Error message if ingestion failed
- `files_processed`: Number of files processed so far
- `total_files`: Total number of files to process
- `time_elapsed`: Time elapsed in seconds
- `errors`: List of errors encountered during ingestion
- `message`: Current status message
- `parsing_stats`: Statistics about parsing process
- `summarization_stats`: Statistics about summarization process

## Advanced Usage

### Ingesting Additional Documentation

You can include additional documentation related to the codebase by providing either a local path or a URI:

```python
# Include local documentation
ingestion = Ingestion(
    repo="https://github.com/user/repo",
    model_client=openai_client,
    doc_path="/path/to/documentation"
)

# Include remote documentation
ingestion = Ingestion(
    local_path="/path/to/codebase",
    model_client=openai_client,
    doc_uri="https://example.com/documentation.md"
)
```

### Parse Only Mode

If you only want to parse the codebase structure without generating LLM summaries:

```python
ingestion = Ingestion(
    repo="https://github.com/user/repo",
    parse_only=True
)
```

## Error Handling

It's important to properly handle errors during ingestion:

```python
try:
    ingestion_id = await ingestion.ingest()
    
    while True:
        status = await ingestion.get_status(ingestion_id)
        
        if status.state == "completed":
            print("Ingestion completed successfully!")
            break
        elif status.state == "failed":
            print(f"Ingestion failed: {status.error}")
            print(f"Detailed errors: {status.errors}")
            break
        
        await asyncio.sleep(5)
        
except ValueError as e:
    print(f"Validation error: {e}")
except Exception as e:
    print(f"Unexpected error: {e}")
```

## References

- [Example Script](../examples/ingestion_example.py)
- [API Reference](api_reference.md#ingestion)